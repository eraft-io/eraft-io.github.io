1
00:00:03,640 --> 00:00:13,140
[Music]

2
00:00:13,959 --> 00:00:18,350
alright let's get started alright you

3
00:00:18,350 --> 00:00:21,580
need to drop tables as back thank you

4
00:00:21,580 --> 00:00:23,990
did you solve all your problems

5
00:00:23,990 --> 00:00:27,090
[Applause]

6
00:00:31,150 --> 00:00:36,170
yeah okay you know you don't want to be

7
00:00:36,170 --> 00:00:38,960
in that boat it's awful yeah everything

8
00:00:38,960 --> 00:00:41,780
else okay I mean you still gonna have

9
00:00:41,780 --> 00:00:42,890
three girlfriends or you try to come

10
00:00:42,890 --> 00:00:50,870
back one and a half what's the half okay

11
00:00:50,870 --> 00:00:53,510
alright alright alright

12
00:00:53,510 --> 00:00:55,640
so we have a lot I think a lot of things

13
00:00:55,640 --> 00:00:58,129
to talk about today so real quickly

14
00:00:58,129 --> 00:01:00,050
before we get into the course the the

15
00:01:00,050 --> 00:01:02,539
topic for the lecture today so this is

16
00:01:02,539 --> 00:01:05,480
the final docket for everyone this

17
00:01:05,480 --> 00:01:07,280
semester this is everything that that

18
00:01:07,280 --> 00:01:08,840
you have to finish up project 3

19
00:01:08,840 --> 00:01:11,180
obviously was due last night homework 5

20
00:01:11,180 --> 00:01:14,030
should go out today ish and that'll be

21
00:01:14,030 --> 00:01:17,450
due in two weeks on December 3rd perfect

22
00:01:17,450 --> 00:01:19,610
for will go out went out this weekend

23
00:01:19,610 --> 00:01:22,729
and that'll be due on December 10th the

24
00:01:22,729 --> 00:01:24,229
extra credit is due on December 10th as

25
00:01:24,229 --> 00:01:26,329
well and then we'll have the checkpoint

26
00:01:26,329 --> 00:01:27,530
which I'll talk about next slide and

27
00:01:27,530 --> 00:01:30,130
then the final exam is on Monday

28
00:01:30,130 --> 00:01:33,560
December 9th at 5:30 p.m. not in this

29
00:01:33,560 --> 00:01:34,970
room I don't know what room they'll put

30
00:01:34,970 --> 00:01:36,399
us in

31
00:01:36,399 --> 00:01:41,329
but it's sort of a suti time so maybe

32
00:01:41,329 --> 00:01:43,009
we'll do is that new candy maybe a new

33
00:01:43,009 --> 00:01:44,060
pizza or something like that something

34
00:01:44,060 --> 00:01:47,119
better okay so any question about any of

35
00:01:47,119 --> 00:01:51,320
these things all right and then some

36
00:01:51,320 --> 00:01:54,950
other things that are floating around we

37
00:01:54,950 --> 00:01:57,110
have three more sort of I'm Convio

38
00:01:57,110 --> 00:01:59,420
lectures but like course topic lectures

39
00:01:59,420 --> 00:02:03,020
on material that's this week and then

40
00:02:03,020 --> 00:02:05,450
one one class next week and then when we

41
00:02:05,450 --> 00:02:07,579
come back from effort the Thanksgiving

42
00:02:07,579 --> 00:02:10,399
break on Monday December 2nd our friends

43
00:02:10,399 --> 00:02:11,840
at Oracle will be coming giving a talk

44
00:02:11,840 --> 00:02:15,500
about the the stuff that they're working

45
00:02:15,500 --> 00:02:19,700
on and again this is not like a it's not

46
00:02:19,700 --> 00:02:20,810
like a lecture where there

47
00:02:20,810 --> 00:02:22,160
you know instead of me talking about the

48
00:02:22,160 --> 00:02:23,030
material they're gonna talk about the

49
00:02:23,030 --> 00:02:24,470
same material they're actually gonna

50
00:02:24,470 --> 00:02:26,390
talk about what they're been building in

51
00:02:26,390 --> 00:02:28,490
in their group and you'll see how it

52
00:02:28,490 --> 00:02:30,200
ties in together all the things various

53
00:02:30,200 --> 00:02:30,980
things we talked about through the

54
00:02:30,980 --> 00:02:34,130
entire semester the other thing that for

55
00:02:34,130 --> 00:02:36,560
the second class in the last week I do

56
00:02:36,560 --> 00:02:38,660
two things one we'll do a final system

57
00:02:38,660 --> 00:02:41,060
review and the second one will be what

58
00:02:41,060 --> 00:02:44,569
I'll call systems potpourri where if

59
00:02:44,569 --> 00:02:46,190
there's any system you want me to talk

60
00:02:46,190 --> 00:02:48,110
about for like 10 or 15 minutes to teach

61
00:02:48,110 --> 00:02:49,850
you about what it is how it works and

62
00:02:49,850 --> 00:02:51,950
why it's interesting you know and using

63
00:02:51,950 --> 00:02:53,060
the vernacular that we talked about

64
00:02:53,060 --> 00:02:55,340
through the entire semester we'll have a

65
00:02:55,340 --> 00:02:58,010
vote online at this URL it's just a

66
00:02:58,010 --> 00:03:00,080
Google form and you go select whatever

67
00:03:00,080 --> 00:03:02,390
system that you want me to cover for 10

68
00:03:02,390 --> 00:03:04,970
or 15 minutes okay so we usually have

69
00:03:04,970 --> 00:03:07,700
time to do three or four of them so the

70
00:03:07,700 --> 00:03:10,340
list that I'm showing here is the when

71
00:03:10,340 --> 00:03:14,090
you go to the Google Form it's the it's

72
00:03:14,090 --> 00:03:17,810
the systems on the DVD DVD video website

73
00:03:17,810 --> 00:03:19,610
that have had the most views for the

74
00:03:19,610 --> 00:03:23,030
last two months so that they're in that

75
00:03:23,030 --> 00:03:24,440
order but you don't necessarily have to

76
00:03:24,440 --> 00:03:26,239
you know follow that but and also

77
00:03:26,239 --> 00:03:27,829
there's another one that's on DVD that I

78
00:03:27,829 --> 00:03:29,030
oh that we want to cover that that's not

79
00:03:29,030 --> 00:03:30,709
on that list just you know that you can

80
00:03:30,709 --> 00:03:34,130
even type it in okay and you can go back

81
00:03:34,130 --> 00:03:36,290
last year and see what I cover but I

82
00:03:36,290 --> 00:03:37,579
encourage you not to do that before you

83
00:03:37,579 --> 00:03:39,799
vote you don't want to sort of take your

84
00:03:39,799 --> 00:03:42,350
your your bias to you just do whatever

85
00:03:42,350 --> 00:03:44,360
people did last year because I'm always

86
00:03:44,360 --> 00:03:45,500
curious see what you guys are just about

87
00:03:45,500 --> 00:03:46,850
like yeah we've covered post miss a

88
00:03:46,850 --> 00:03:48,260
little bit we covered my single Oracle

89
00:03:48,260 --> 00:03:50,840
and sequel light a little bit easier to

90
00:03:50,840 --> 00:03:51,890
see what you guys are seeing on the

91
00:03:51,890 --> 00:03:53,359
internet or what you want to do it in

92
00:03:53,359 --> 00:03:56,840
your job or on a hobby project we know

93
00:03:56,840 --> 00:03:57,890
what system you've been thing about

94
00:03:57,890 --> 00:03:59,090
maybe using and I can come teach you

95
00:03:59,090 --> 00:04:03,040
about what it is and how it works okay

96
00:04:03,040 --> 00:04:05,630
all right and then the extra credit

97
00:04:05,630 --> 00:04:08,480
feedback again you can submit your extra

98
00:04:08,480 --> 00:04:11,540
credit article next this Sunday on

99
00:04:11,540 --> 00:04:14,180
November 24th and then the myself and

100
00:04:14,180 --> 00:04:15,650
the TAS will give you feedback and say

101
00:04:15,650 --> 00:04:16,970
what you're doing correctly what you're

102
00:04:16,970 --> 00:04:19,130
not doing correctly and that way you can

103
00:04:19,130 --> 00:04:21,019
fix it up in time for the this mission

104
00:04:21,019 --> 00:04:24,560
so they did get full credit okay and the

105
00:04:24,560 --> 00:04:26,630
questions about any of these things so

106
00:04:26,630 --> 00:04:27,710
I'll put a deadline for when you should

107
00:04:27,710 --> 00:04:29,599
go vote right if like you obviously

108
00:04:29,599 --> 00:04:30,890
can't vote the day before the lecture

109
00:04:30,890 --> 00:04:31,909
because then I don't have time that you

110
00:04:31,909 --> 00:04:33,560
prepare it certainly or maybe the the

111
00:04:33,560 --> 00:04:34,689
week of Thanksgiving

112
00:04:34,689 --> 00:04:38,979
we did in London for this okay and the

113
00:04:38,979 --> 00:04:41,439
last thing is that in addition to giving

114
00:04:41,439 --> 00:04:46,599
an in-class lecture on on the ii that

115
00:04:46,599 --> 00:04:49,389
should be December third on the Tuesday

116
00:04:49,389 --> 00:04:51,759
December third book will also be giving

117
00:04:51,759 --> 00:04:54,639
a graduate level research talk over in

118
00:04:54,639 --> 00:04:56,349
the PTO in the CIC building I think

119
00:04:56,349 --> 00:04:59,099
they're also giving an undergrad talk

120
00:04:59,099 --> 00:05:01,509
Monday and Monday afternoon as well so

121
00:05:01,509 --> 00:05:02,949
there's me three Oracle talks in two

122
00:05:02,949 --> 00:05:04,719
days and you know one of them you'll

123
00:05:04,719 --> 00:05:06,339
require to come to or not required but

124
00:05:06,339 --> 00:05:07,869
you'll get extra credit for for the

125
00:05:07,869 --> 00:05:09,489
final if you come to that and then these

126
00:05:09,489 --> 00:05:14,139
tuition ones are optional okay any

127
00:05:14,139 --> 00:05:20,159
questions we're almost done all right so

128
00:05:20,159 --> 00:05:24,069
today's class is now the beginning for

129
00:05:24,069 --> 00:05:26,019
our discussion on distributed databases

130
00:05:26,019 --> 00:05:28,659
and as I said last class we can't

131
00:05:28,659 --> 00:05:30,519
actually you know before we just you

132
00:05:30,519 --> 00:05:31,959
know jump in neatly into distributing

133
00:05:31,959 --> 00:05:34,149
databases we had to spend however weeks

134
00:05:34,149 --> 00:05:36,369
we've gone and so far in this semester

135
00:05:36,369 --> 00:05:38,349
to understand how a single node database

136
00:05:38,349 --> 00:05:40,779
system works because when now we start

137
00:05:40,779 --> 00:05:42,789
going distributed you know just because

138
00:05:42,789 --> 00:05:44,409
we have more machines or more hardware

139
00:05:44,409 --> 00:05:46,749
doesn't magically make our system easier

140
00:05:46,749 --> 00:05:49,989
to build or better are you all the

141
00:05:49,989 --> 00:05:51,429
things that we had to talk about for a

142
00:05:51,429 --> 00:05:53,079
single node system we have to still

143
00:05:53,079 --> 00:05:54,339
solve them in a tribute ax system and

144
00:05:54,339 --> 00:05:55,719
actually they're even harder because now

145
00:05:55,719 --> 00:05:57,879
you have to account for the network so

146
00:05:57,879 --> 00:05:59,619
we've already talked about this before

147
00:05:59,619 --> 00:06:01,419
when we talked about query executes this

148
00:06:01,419 --> 00:06:03,309
this contrast between a parallel

149
00:06:03,309 --> 00:06:05,050
database and a distributed database

150
00:06:05,050 --> 00:06:07,329
system and when I talk about parallel

151
00:06:07,329 --> 00:06:08,919
database system we were just assuming

152
00:06:08,919 --> 00:06:11,050
that the the database system was running

153
00:06:11,050 --> 00:06:13,779
on a single box that could have multiple

154
00:06:13,779 --> 00:06:16,449
cores and multiple CPUs and then we

155
00:06:16,449 --> 00:06:18,219
assumed that the the workers that were

156
00:06:18,219 --> 00:06:20,319
executing the queries could communicate

157
00:06:20,319 --> 00:06:21,669
very quickly with each other and that

158
00:06:21,669 --> 00:06:23,800
communication was reliable because if

159
00:06:23,800 --> 00:06:25,179
you're running on the same physical

160
00:06:25,179 --> 00:06:27,339
machine you're sending things over the

161
00:06:27,339 --> 00:06:29,319
interconnect between CPU sockets that's

162
00:06:29,319 --> 00:06:32,259
super fast but now under strip in a

163
00:06:32,259 --> 00:06:34,300
database system we still have to you

164
00:06:34,300 --> 00:06:35,589
know there's still the things we care

165
00:06:35,589 --> 00:06:37,119
about how to do parallel execution but

166
00:06:37,119 --> 00:06:39,039
now we're doing this potentially across

167
00:06:39,039 --> 00:06:42,610
multiple machines and so now we actually

168
00:06:42,610 --> 00:06:45,279
need to be mindful on what it what the

169
00:06:45,279 --> 00:06:47,939
cost is and the reliability of

170
00:06:47,939 --> 00:06:49,949
one worker communicating with another

171
00:06:49,949 --> 00:06:51,809
worker because it's going over the

172
00:06:51,809 --> 00:06:53,189
network that you know that other worker

173
00:06:53,189 --> 00:06:54,499
might not be in the same data center

174
00:06:54,499 --> 00:06:56,610
right mining meet you on the same

175
00:06:56,610 --> 00:06:59,459
continent and so now we can't assume

176
00:06:59,459 --> 00:07:01,949
that you know we send a message there

177
00:07:01,949 --> 00:07:04,499
guillotine to get that and that's gonna

178
00:07:04,499 --> 00:07:05,999
come from problematic when we start

179
00:07:05,999 --> 00:07:07,110
talking about transactions and other

180
00:07:07,110 --> 00:07:11,759
things so as I said the for the Shiva

181
00:07:11,759 --> 00:07:13,110
DNA stuff we could talk about starting

182
00:07:13,110 --> 00:07:15,569
today it's it's building on all the

183
00:07:15,569 --> 00:07:17,849
things we've already talked about so we

184
00:07:17,849 --> 00:07:19,110
still have to do logging we stuff to do

185
00:07:19,110 --> 00:07:21,059
a concur to toe hoodoo query

186
00:07:21,059 --> 00:07:22,769
optimization we have to do query

187
00:07:22,769 --> 00:07:25,349
execution to do joins potentially all

188
00:07:25,349 --> 00:07:27,869
those things we still have to do in

189
00:07:27,869 --> 00:07:29,550
attribute database and now they're just

190
00:07:29,550 --> 00:07:30,869
everything is more expensive everything

191
00:07:30,869 --> 00:07:35,789
is harder so for today's lecture as I

192
00:07:35,789 --> 00:07:37,199
said today's sort of introduction to

193
00:07:37,199 --> 00:07:40,189
distribute databases just to understand

194
00:07:40,189 --> 00:07:42,360
you know what what they actually look

195
00:07:42,360 --> 00:07:44,309
like the different designs of them one

196
00:07:44,309 --> 00:07:46,699
of the implications of those designs and

197
00:07:46,699 --> 00:07:48,929
then we'll talk about how to do

198
00:07:48,929 --> 00:07:50,519
partitioning which is the the key way

199
00:07:50,519 --> 00:07:53,009
we're gonna divide up our database

200
00:07:53,009 --> 00:07:55,139
across multiple resources to get the

201
00:07:55,139 --> 00:07:56,399
parallelism we want in a distribute

202
00:07:56,399 --> 00:07:58,499
environment and then we'll finish up

203
00:07:58,499 --> 00:08:01,769
briefly touching on how how hard

204
00:08:01,769 --> 00:08:03,740
distribute concurrency control is and

205
00:08:03,740 --> 00:08:06,300
then that'll segue into Wednesday's

206
00:08:06,300 --> 00:08:07,349
class where we'll spend the entire day

207
00:08:07,349 --> 00:08:09,180
talking about how we actually do this

208
00:08:09,180 --> 00:08:11,159
and again like we're still going to do

209
00:08:11,159 --> 00:08:12,360
face blocking potentially we're still

210
00:08:12,360 --> 00:08:14,039
gonna do timestamp ordering all those

211
00:08:14,039 --> 00:08:15,419
things we did on a single row system

212
00:08:15,419 --> 00:08:16,949
still apply here just now it's

213
00:08:16,949 --> 00:08:18,809
distribute it so it's even harder okay

214
00:08:18,809 --> 00:08:21,869
and again stop and ask questions as we

215
00:08:21,869 --> 00:08:25,499
go along so the first thing we need to

216
00:08:25,499 --> 00:08:27,300
discuss is what is the system

217
00:08:27,300 --> 00:08:29,879
architecture of the database system so

218
00:08:29,879 --> 00:08:31,889
as I said before when we talk to

219
00:08:31,889 --> 00:08:33,360
parallel systems we talked about there

220
00:08:33,360 --> 00:08:37,289
being there being these workers that are

221
00:08:37,289 --> 00:08:39,750
typically tied to either a process or a

222
00:08:39,750 --> 00:08:41,639
thread that are running on the CPU and

223
00:08:41,639 --> 00:08:44,519
they're gonna access shared resources

224
00:08:44,519 --> 00:08:49,290
like disk and memory and so the design

225
00:08:49,290 --> 00:08:51,600
of our database system in an urban

226
00:08:51,600 --> 00:08:53,009
environment depending what our

227
00:08:53,009 --> 00:08:55,259
architecture is right the variations of

228
00:08:55,259 --> 00:08:58,139
these architectures are going to differ

229
00:08:58,139 --> 00:09:00,240
in how you actually can coordinate the

230
00:09:00,240 --> 00:09:01,680
CPUs and communicate with each

231
00:09:01,680 --> 00:09:03,360
as you're running queries or

232
00:09:03,360 --> 00:09:05,550
transactions in parallel and where are

233
00:09:05,550 --> 00:09:07,170
where is the memory and where is the

234
00:09:07,170 --> 00:09:11,970
dislocated in contacts to to the CPUs so

235
00:09:11,970 --> 00:09:13,410
what we talked about so far the entire

236
00:09:13,410 --> 00:09:14,820
semester is what is known as a shared

237
00:09:14,820 --> 00:09:17,040
everything system assume this is a

238
00:09:17,040 --> 00:09:20,190
single box a single rack unit that has

239
00:09:20,190 --> 00:09:22,380
CPU and CPU has local memory and memory

240
00:09:22,380 --> 00:09:24,120
and you know there's a local disk that

241
00:09:24,120 --> 00:09:26,130
you can read and write to right now any

242
00:09:26,130 --> 00:09:27,750
time I want to access something in the

243
00:09:27,750 --> 00:09:29,399
binomial architecture that we're based

244
00:09:29,399 --> 00:09:30,899
on anyhow I'm gonna get something from

245
00:09:30,899 --> 00:09:32,640
disk I got to bring it to my buffer pool

246
00:09:32,640 --> 00:09:35,580
into memory and then my worker up above

247
00:09:35,580 --> 00:09:37,350
running on my CPU can can read and write

248
00:09:37,350 --> 00:09:39,270
two things read write two pages and then

249
00:09:39,270 --> 00:09:41,339
I let you write them out the disk again

250
00:09:41,339 --> 00:09:43,950
most database minute systems that

251
00:09:43,950 --> 00:09:45,510
everyday the system that's not

252
00:09:45,510 --> 00:09:47,640
distributed is using this approach as I

253
00:09:47,640 --> 00:09:51,240
shared everything system so an

254
00:09:51,240 --> 00:09:53,070
alternative in a distribute environment

255
00:09:53,070 --> 00:09:54,750
is one alternatives called shared memory

256
00:09:54,750 --> 00:09:57,360
and the idea here is that you'll have

257
00:09:57,360 --> 00:09:59,910
multiple CPU resources that are

258
00:09:59,910 --> 00:10:00,930
potentially running on different

259
00:10:00,930 --> 00:10:03,589
machines but there's there's a

260
00:10:03,589 --> 00:10:06,089
communication layer that allows them to

261
00:10:06,089 --> 00:10:08,940
have a unified memory view across all

262
00:10:08,940 --> 00:10:11,580
those machines all right assume this is

263
00:10:11,580 --> 00:10:13,170
some kind of high speed interconnect

264
00:10:13,170 --> 00:10:15,240
like InfiniBand or tcp/ip it doesn't

265
00:10:15,240 --> 00:10:16,320
matter the high level architecture is

266
00:10:16,320 --> 00:10:18,510
still the same and then there's still

267
00:10:18,510 --> 00:10:20,400
going to be some some local or started

268
00:10:20,400 --> 00:10:21,600
some shared disk that everybody's

269
00:10:21,600 --> 00:10:25,860
reading running to I said typically the

270
00:10:25,860 --> 00:10:27,810
spoiler be this is actually not actually

271
00:10:27,810 --> 00:10:28,800
I don't know of any database of them

272
00:10:28,800 --> 00:10:30,240
actually that's commercially or open

273
00:10:30,240 --> 00:10:32,310
source that actually uses this this kind

274
00:10:32,310 --> 00:10:33,750
of architecture is mostly seen in the

275
00:10:33,750 --> 00:10:35,640
HPC or high-performance computing world

276
00:10:35,640 --> 00:10:36,900
like the people running on

277
00:10:36,900 --> 00:10:39,270
supercomputers at like the big National

278
00:10:39,270 --> 00:10:42,270
Labs they build software and assuming

279
00:10:42,270 --> 00:10:44,370
this model for databases is there isn't

280
00:10:44,370 --> 00:10:47,730
that much another approach is do you

281
00:10:47,730 --> 00:10:51,060
share disk and the idea here is that the

282
00:10:51,060 --> 00:10:53,579
CPU workers are the workers running on

283
00:10:53,579 --> 00:10:57,570
the CPU they have local memory but the

284
00:10:57,570 --> 00:10:59,190
the disk where we have maintained a

285
00:10:59,190 --> 00:11:01,050
persistent state of the database that's

286
00:11:01,050 --> 00:11:03,870
some scrum shared architecture shared

287
00:11:03,870 --> 00:11:06,959
device that all these CPUs can can read

288
00:11:06,959 --> 00:11:09,839
and write in to nice to think of this if

289
00:11:09,839 --> 00:11:11,339
you're running on Amazon this is

290
00:11:11,339 --> 00:11:15,480
something like s3 or BBS or HDFS and

291
00:11:15,480 --> 00:11:17,519
kind of distributed file system so all

292
00:11:17,519 --> 00:11:19,230
the the CPUs are still seeing the same

293
00:11:19,230 --> 00:11:22,680
same disk but in order for them to

294
00:11:22,680 --> 00:11:23,820
communicate with each other they have to

295
00:11:23,820 --> 00:11:24,959
maybe send messages back and forth

296
00:11:24,959 --> 00:11:27,420
between them right because they you know

297
00:11:27,420 --> 00:11:29,040
this CPU can't read the memory of this

298
00:11:29,040 --> 00:11:33,660
CPU the last architecture is what most

299
00:11:33,660 --> 00:11:34,740
people think of when they think of it

300
00:11:34,740 --> 00:11:36,510
attributed database just what is going

301
00:11:36,510 --> 00:11:39,360
to shared-nothing meaning every single

302
00:11:39,360 --> 00:11:42,449
worker is running on on a sort of island

303
00:11:42,449 --> 00:11:44,850
by itself it has its own local memory

304
00:11:44,850 --> 00:11:47,670
has its own local disk and the only wave

305
00:11:47,670 --> 00:11:49,800
to coordinate between different workers

306
00:11:49,800 --> 00:11:52,709
it to go up above and communicate at the

307
00:11:52,709 --> 00:11:54,110
user you know using some kind of

308
00:11:54,110 --> 00:11:58,110
messages fabric on the top so again this

309
00:11:58,110 --> 00:12:00,149
CPU worker here can't read the memory or

310
00:12:00,149 --> 00:12:02,519
disk from and any bits animate its

311
00:12:02,519 --> 00:12:06,870
neighbors or friends in the cluster so

312
00:12:06,870 --> 00:12:07,980
again we'll go through each of these one

313
00:12:07,980 --> 00:12:11,610
by one so again under shared memory as I

314
00:12:11,610 --> 00:12:14,279
said this is not that common in

315
00:12:14,279 --> 00:12:16,290
databases I'm not aware of any system

316
00:12:16,290 --> 00:12:18,630
that's actually people are using that's

317
00:12:18,630 --> 00:12:22,260
based on this and the basic idea is that

318
00:12:22,260 --> 00:12:24,690
the database system service is running

319
00:12:24,690 --> 00:12:27,510
on these different CPUs it's running in

320
00:12:27,510 --> 00:12:29,940
the same operating system instance and

321
00:12:29,940 --> 00:12:33,149
assumes it has a single global address

322
00:12:33,149 --> 00:12:35,279
space that maybe just aggregated across

323
00:12:35,279 --> 00:12:39,060
different machines and then there's some

324
00:12:39,060 --> 00:12:40,260
networking layer that allows them to

325
00:12:40,260 --> 00:12:41,519
pass messages back and forth to make

326
00:12:41,519 --> 00:12:43,019
make this work right so again this could

327
00:12:43,019 --> 00:12:45,630
be InfiniBand this could be tcp/ip this

328
00:12:45,630 --> 00:12:48,209
could be intel's omni path alright some

329
00:12:48,209 --> 00:12:51,899
fast interconnect between them so in

330
00:12:51,899 --> 00:12:54,959
this in in this world the database

331
00:12:54,959 --> 00:12:56,579
instance running on one CPU it like the

332
00:12:56,579 --> 00:12:59,190
worker is aware of other workers and so

333
00:12:59,190 --> 00:13:00,510
if they want to communicate between each

334
00:13:00,510 --> 00:13:02,190
other they can just do what you would

335
00:13:02,190 --> 00:13:03,209
normally do in a shared everything

336
00:13:03,209 --> 00:13:05,310
system you can write something into a

337
00:13:05,310 --> 00:13:08,190
global data structure or send a message

338
00:13:08,190 --> 00:13:11,790
over over an IPC and you know the other

339
00:13:11,790 --> 00:13:13,319
process of the other worker running on

340
00:13:13,319 --> 00:13:14,940
on the other another machine would see

341
00:13:14,940 --> 00:13:17,819
that right again clean in the context of

342
00:13:17,819 --> 00:13:19,800
a shared everything system when we were

343
00:13:19,800 --> 00:13:21,269
doing to base locking with if we wanted

344
00:13:21,269 --> 00:13:23,279
to tell another worker that hey I hold

345
00:13:23,279 --> 00:13:26,459
the lock for this tuple I add an entry

346
00:13:26,459 --> 00:13:28,500
to my lock table that's sitting in

347
00:13:28,500 --> 00:13:29,190
memory

348
00:13:29,190 --> 00:13:31,440
so same thing here if I one worker one

349
00:13:31,440 --> 00:13:33,629
sit-up acquire lock on a tuple just

350
00:13:33,629 --> 00:13:36,209
updates the global lock table and then

351
00:13:36,209 --> 00:13:38,670
the the messaging fabric is guarantee to

352
00:13:38,670 --> 00:13:39,839
make sure everything's coherent across

353
00:13:39,839 --> 00:13:43,949
all those all those workers okay as I

354
00:13:43,949 --> 00:13:46,230
said this is not that common I don't

355
00:13:46,230 --> 00:13:49,259
know if anybody actually does this the

356
00:13:49,259 --> 00:13:51,720
more common one is share disk and again

357
00:13:51,720 --> 00:13:53,189
the idea here is that we have these

358
00:13:53,189 --> 00:13:56,430
compute nodes that have their own local

359
00:13:56,430 --> 00:13:59,129
memory they can have a disk up there as

360
00:13:59,129 --> 00:14:02,519
well but that's not the the the final

361
00:14:02,519 --> 00:14:04,350
storage location of any kind of data in

362
00:14:04,350 --> 00:14:05,790
the database you can just use it for

363
00:14:05,790 --> 00:14:07,259
caching in case you need to spill the

364
00:14:07,259 --> 00:14:09,389
disk on your local machine but the

365
00:14:09,389 --> 00:14:11,670
database the final resting lok resting

366
00:14:11,670 --> 00:14:14,040
location is down here so if I say I

367
00:14:14,040 --> 00:14:16,259
bring a page into my buffer pool in my

368
00:14:16,259 --> 00:14:18,629
local memory say I modify it and then

369
00:14:18,629 --> 00:14:19,560
I'm gonna write it out because it's

370
00:14:19,560 --> 00:14:21,149
dirty I would write it down here to the

371
00:14:21,149 --> 00:14:23,129
shared disk and now potentially any

372
00:14:23,129 --> 00:14:26,399
other worker can can see my change how

373
00:14:26,399 --> 00:14:28,199
you coordinate that will make sure that

374
00:14:28,199 --> 00:14:29,610
they're told about that change will get

375
00:14:29,610 --> 00:14:33,990
will get to later so as I said this is

376
00:14:33,990 --> 00:14:36,089
the this is the present architecture in

377
00:14:36,089 --> 00:14:38,459
today's cloud environment because the

378
00:14:38,459 --> 00:14:40,259
disk is going to be you know something

379
00:14:40,259 --> 00:14:43,050
Amazon provides like s3 EBS so pretty

380
00:14:43,050 --> 00:14:45,420
much every sort of cloud native database

381
00:14:45,420 --> 00:14:47,519
system that you've heard about is is

382
00:14:47,519 --> 00:14:49,370
gonna be running this environment

383
00:14:49,370 --> 00:14:52,139
because one big advantage you can get is

384
00:14:52,139 --> 00:14:55,259
that you're able to scale up the compute

385
00:14:55,259 --> 00:14:57,000
resources and the disk resources

386
00:14:57,000 --> 00:15:00,839
separately because the compute resources

387
00:15:00,839 --> 00:15:02,910
are stateless the state of the database

388
00:15:02,910 --> 00:15:05,639
is down here so if these all these

389
00:15:05,639 --> 00:15:08,189
compute resources crash and go away my

390
00:15:08,189 --> 00:15:10,350
you know assuming I you know of log

391
00:15:10,350 --> 00:15:12,899
things out correctly everything is still

392
00:15:12,899 --> 00:15:14,579
here and then I can bring up another

393
00:15:14,579 --> 00:15:17,430
instance and and pick up where the other

394
00:15:17,430 --> 00:15:20,370
guys left off we'll see in a second

395
00:15:20,370 --> 00:15:23,730
that's not so easy to do in a in a

396
00:15:23,730 --> 00:15:25,649
shared nothing environment because every

397
00:15:25,649 --> 00:15:29,009
node every node hold state so let's look

398
00:15:29,009 --> 00:15:30,540
at a high-level example like this again

399
00:15:30,540 --> 00:15:32,279
so we have our application server it's

400
00:15:32,279 --> 00:15:34,860
gonna send requests to these front-end

401
00:15:34,860 --> 00:15:36,689
compute nodes right this is where we

402
00:15:36,689 --> 00:15:38,699
have the workers running on CP us and

403
00:15:38,699 --> 00:15:40,410
the local memory and then we have some

404
00:15:40,410 --> 00:15:43,170
back-end storage storage device

405
00:15:43,170 --> 00:15:45,029
that everyone can can read right - so

406
00:15:45,029 --> 00:15:46,679
let's say that the application says I

407
00:15:46,679 --> 00:15:49,649
want to get record 101 it goes to this

408
00:15:49,649 --> 00:15:51,899
node however it knows the good how it

409
00:15:51,899 --> 00:15:54,329
how it the application knows to go to

410
00:15:54,329 --> 00:15:56,189
this node will cover in a second but

411
00:15:56,189 --> 00:15:58,889
assume it does so then this says some

412
00:15:58,889 --> 00:16:00,209
kind of book up that says well record

413
00:16:00,209 --> 00:16:02,579
101 if I look at my index I see that

414
00:16:02,579 --> 00:16:04,799
it's in page ABC so I go to my shared

415
00:16:04,799 --> 00:16:07,259
disk storage and I say get me page ABC

416
00:16:07,259 --> 00:16:09,059
and I bring that to my bar full same

417
00:16:09,059 --> 00:16:11,069
thing this guy wants 200 he doesn't have

418
00:16:11,069 --> 00:16:12,660
it in its buffer pool so it goes out the

419
00:16:12,660 --> 00:16:16,769
disk and fetches it in so now if I want

420
00:16:16,769 --> 00:16:20,160
to scale up compute resources because

421
00:16:20,160 --> 00:16:21,869
again the state of the database is

422
00:16:21,869 --> 00:16:25,499
always here on own shared disk I can

423
00:16:25,499 --> 00:16:27,809
just bring him a new guy here I don't

424
00:16:27,809 --> 00:16:29,220
have to copy anything immediately

425
00:16:29,220 --> 00:16:33,209
because if I now request say 101 same

426
00:16:33,209 --> 00:16:34,769
thing I just go to disk and bring it

427
00:16:34,769 --> 00:16:37,619
back to my buffer pool and I could serve

428
00:16:37,619 --> 00:16:44,189
the request tricky sorry yes it's

429
00:16:44,189 --> 00:16:45,119
question is how does the lock manner

430
00:16:45,119 --> 00:16:48,389
work in the setting like this and then

431
00:16:48,389 --> 00:16:49,910
we'll cover this yes not yet

432
00:16:49,910 --> 00:16:52,019
to reduce simplicity we're talking about

433
00:16:52,019 --> 00:16:55,669
how do we ever get things in now yeah

434
00:16:59,920 --> 00:17:01,369
[Applause]

435
00:17:01,369 --> 00:17:04,709
yes so this question is in a shared

436
00:17:04,709 --> 00:17:06,898
memory architecture how is that

437
00:17:06,898 --> 00:17:08,970
different than a Multi multi socket

438
00:17:08,970 --> 00:17:11,699
multiprocessor shared everything system

439
00:17:11,699 --> 00:17:14,159
they're the same but think of like so

440
00:17:14,159 --> 00:17:20,309
there are there are there are there are

441
00:17:20,309 --> 00:17:23,579
distributed systems that have a unified

442
00:17:23,579 --> 00:17:25,459
memory cross multiple physical machines

443
00:17:25,459 --> 00:17:27,720
so each machine has its own motherboard

444
00:17:27,720 --> 00:17:30,029
has its own like physical physical

445
00:17:30,029 --> 00:17:31,950
memory that it can be right to but

446
00:17:31,950 --> 00:17:33,539
there's a there's a layer that it says

447
00:17:33,539 --> 00:17:35,850
all the processors think they have this

448
00:17:35,850 --> 00:17:38,029
one giant block of memory

449
00:17:38,029 --> 00:17:40,110
nobody does that for databases you see

450
00:17:40,110 --> 00:17:41,760
that in HPC world like all those people

451
00:17:41,760 --> 00:17:43,679
are doing like nuclear bomb or particle

452
00:17:43,679 --> 00:17:45,539
physics simulations they're writing

453
00:17:45,539 --> 00:17:47,250
those Fortran programs assuming they had

454
00:17:47,250 --> 00:17:50,340
this like no terabytes of memory across

455
00:17:50,340 --> 00:17:52,110
smoke machines they can do computation

456
00:17:52,110 --> 00:17:55,399
on yes

457
00:17:55,490 --> 00:18:05,490
you come mean that companies like this

458
00:18:05,490 --> 00:18:07,980
question is if you zoom in a shared disk

459
00:18:07,980 --> 00:18:09,870
architecture like this if a database

460
00:18:09,870 --> 00:18:12,600
vendor davison vendor is using a shared

461
00:18:12,600 --> 00:18:15,960
disk architecture does that mean that

462
00:18:15,960 --> 00:18:18,390
they either have to have all the the

463
00:18:18,390 --> 00:18:22,039
data for a database in one location no

464
00:18:22,039 --> 00:18:28,830
well we'll get to that again I mean

465
00:18:28,830 --> 00:18:30,120
think about this there's an abstraction

466
00:18:30,120 --> 00:18:33,120
between the physical and the physical

467
00:18:33,120 --> 00:18:35,309
location and the logical location of the

468
00:18:35,309 --> 00:18:37,320
disk these guys don't know anything

469
00:18:37,320 --> 00:18:40,110
about where these things are right so

470
00:18:40,110 --> 00:18:42,659
like it just says hey here's this file I

471
00:18:42,659 --> 00:18:44,130
can I can read and write to just as you

472
00:18:44,130 --> 00:18:46,500
would as if it was a local disk when in

473
00:18:46,500 --> 00:18:49,860
case Amazon or Azure you get like a like

474
00:18:49,860 --> 00:18:52,440
a block base or object based API give me

475
00:18:52,440 --> 00:18:54,870
this bucket give me that bucket that

476
00:18:54,870 --> 00:18:56,700
Nats sending a restful class to go to

477
00:18:56,700 --> 00:18:58,260
some back-end service you don't know

478
00:18:58,260 --> 00:19:13,350
where that data's actually is yeah yeah

479
00:19:13,350 --> 00:19:16,049
so we'll get to that as well so his

480
00:19:16,049 --> 00:19:18,570
question is in my example here when I'm

481
00:19:18,570 --> 00:19:21,690
sending this request in this in this

482
00:19:21,690 --> 00:19:24,090
example here I this the application says

483
00:19:24,090 --> 00:19:25,440
I'm going to this node to get this

484
00:19:25,440 --> 00:19:28,080
record you could have something in front

485
00:19:28,080 --> 00:19:30,539
of this that could hide that or this

486
00:19:30,539 --> 00:19:31,950
thing can maintain to say where to go

487
00:19:31,950 --> 00:19:34,760
get the thing I need we'll come to that

488
00:19:34,760 --> 00:19:37,020
just the third thing I'm gonna focus on

489
00:19:37,020 --> 00:19:40,409
here is like it's like this guy has no

490
00:19:40,409 --> 00:19:42,030
state of the database other than within

491
00:19:42,030 --> 00:19:43,740
its buffer pool but that's not

492
00:19:43,740 --> 00:19:46,440
considered to be that's good not

493
00:19:46,440 --> 00:19:47,789
consider be you know durable or

494
00:19:47,789 --> 00:19:49,770
persistent its ephemeral so this guy

495
00:19:49,770 --> 00:19:52,650
crashes anything we had in here it goes

496
00:19:52,650 --> 00:19:54,740
away

497
00:19:56,000 --> 00:19:57,960
all right so now the tricky things gonna

498
00:19:57,960 --> 00:20:00,420
be if I do an update right so update

499
00:20:00,420 --> 00:20:04,260
page 101 hour sir an ID record of 101 I

500
00:20:04,260 --> 00:20:07,710
have to update page ABC these guys all

501
00:20:07,710 --> 00:20:10,260
read that same record they have page ABC

502
00:20:10,260 --> 00:20:13,080
and the buffer pool but they're not

503
00:20:13,080 --> 00:20:15,540
going to know about the the chains

504
00:20:15,540 --> 00:20:16,980
because these shared disk architectures

505
00:20:16,980 --> 00:20:19,680
they don't provide a notification say

506
00:20:19,680 --> 00:20:24,210
hey by the way somebody updated this so

507
00:20:24,210 --> 00:20:26,940
I have to have additional messages in my

508
00:20:26,940 --> 00:20:29,370
knees notes to say hey I think you have

509
00:20:29,370 --> 00:20:32,010
page EBC by the way I just modified it

510
00:20:32,010 --> 00:20:33,510
and here's the latest version or if

511
00:20:33,510 --> 00:20:34,260
you're gonna find out what the latest

512
00:20:34,260 --> 00:20:36,440
version is come asking me about it

513
00:20:36,440 --> 00:20:38,790
so that's all the stuff that we have to

514
00:20:38,790 --> 00:20:40,980
build in our database system right this

515
00:20:40,980 --> 00:20:44,190
is just reading writing to some desk and

516
00:20:44,190 --> 00:20:46,230
so related his question that it's all

517
00:20:46,230 --> 00:20:48,300
transparent so right now I'm showing the

518
00:20:48,300 --> 00:20:49,890
database and this in this in this

519
00:20:49,890 --> 00:20:52,650
diagram is on two discs but I can easily

520
00:20:52,650 --> 00:20:56,070
add a bunch more to now split up the

521
00:20:56,070 --> 00:20:57,450
data cross more - I'm getting better

522
00:20:57,450 --> 00:20:59,730
parallelism better better replication

523
00:20:59,730 --> 00:21:02,250
better reliability but none of these

524
00:21:02,250 --> 00:21:04,740
guys in the in the compute layer they

525
00:21:04,740 --> 00:21:05,640
don't know anything about that

526
00:21:05,640 --> 00:21:08,040
because that's all hidden from me so you

527
00:21:08,040 --> 00:21:09,390
had this nice separation where you can

528
00:21:09,390 --> 00:21:11,070
scale things out independently but

529
00:21:11,070 --> 00:21:12,830
you're gonna pay a penalty in terms of

530
00:21:12,830 --> 00:21:16,740
locality of access because I can't for

531
00:21:16,740 --> 00:21:18,090
the most part I can't run queries over

532
00:21:18,090 --> 00:21:18,600
here

533
00:21:18,600 --> 00:21:20,700
s3 allows you do some basic filtering

534
00:21:20,700 --> 00:21:22,740
but about anything like a join I have to

535
00:21:22,740 --> 00:21:24,090
do over here so that means I open pool

536
00:21:24,090 --> 00:21:35,670
the data to my computes yes so yes

537
00:21:35,670 --> 00:21:37,350
question is this is different in

538
00:21:37,350 --> 00:21:39,330
charting or partitioning we have

539
00:21:39,330 --> 00:21:41,130
explicit divisions of who has what data

540
00:21:41,130 --> 00:21:44,010
we'll get to there I'm just showing you

541
00:21:44,010 --> 00:21:46,950
what share disk is and a high/low

542
00:21:46,950 --> 00:21:49,520
you can still partition at this level

543
00:21:49,520 --> 00:21:51,360
there's nothing about share disk

544
00:21:51,360 --> 00:21:52,740
precludes you from doing partitioning at

545
00:21:52,740 --> 00:22:00,390
the compute level correct if if you're

546
00:22:00,390 --> 00:22:02,990
not doing partitioning

547
00:22:15,500 --> 00:22:18,179
any question is in this example here I

548
00:22:18,179 --> 00:22:20,820
said when I update page ABC in this node

549
00:22:20,820 --> 00:22:22,529
it sent a message to the other guys

550
00:22:22,529 --> 00:22:24,179
updated and say hey I haven't make a

551
00:22:24,179 --> 00:22:26,100
change is it always like this or what

552
00:22:26,100 --> 00:22:31,590
all over the alternative yeah yeah so

553
00:22:31,590 --> 00:22:33,510
what I said it in this case here I

554
00:22:33,510 --> 00:22:36,600
update a page ABC it has to the compute

555
00:22:36,600 --> 00:22:37,860
node at the top is update these compute

556
00:22:37,860 --> 00:22:39,659
nodes to the bottom an alternative would

557
00:22:39,659 --> 00:22:42,090
be a way to do a push notification and

558
00:22:42,090 --> 00:22:44,789
say hey I just got an update ABC by the

559
00:22:44,789 --> 00:22:46,559
way everybody needs to update and

560
00:22:46,559 --> 00:22:50,669
refresh themselves I'm not aware of any

561
00:22:50,669 --> 00:22:54,230
shared disk architecture like EBS s3

562
00:22:54,230 --> 00:22:56,370
whatever the address up they don't do

563
00:22:56,370 --> 00:22:57,389
that because that would be super

564
00:22:57,389 --> 00:22:59,340
expensive because if you think about it

565
00:22:59,340 --> 00:23:01,649
it's like a pop sub system I need to

566
00:23:01,649 --> 00:23:04,260
know who needs to know about my change

567
00:23:04,260 --> 00:23:06,240
because otherwise I'm sending messages

568
00:23:06,240 --> 00:23:08,700
that are wasteful so as far as they know

569
00:23:08,700 --> 00:23:10,350
nobody actually does this you have to

570
00:23:10,350 --> 00:23:11,909
coordinate at this layer here and this

571
00:23:11,909 --> 00:23:13,500
is the database system does this the

572
00:23:13,500 --> 00:23:15,450
distributed file system or the object

573
00:23:15,450 --> 00:23:16,610
store doesn't do that

574
00:23:16,610 --> 00:23:24,059
yes his question is are we assuming here

575
00:23:24,059 --> 00:23:26,100
that the notification is reliable or

576
00:23:26,100 --> 00:23:29,309
fast enough no I didn't say what this is

577
00:23:29,309 --> 00:23:30,360
I'm saying how we're doing the same

578
00:23:30,360 --> 00:23:36,510
thing you have to do this his question

579
00:23:36,510 --> 00:23:37,529
is can't you run an issue still raise

580
00:23:37,529 --> 00:23:39,510
absolutely yes that's that's concurrency

581
00:23:39,510 --> 00:23:47,730
tool we'll get there yes okay so the the

582
00:23:47,730 --> 00:23:50,399
again the the pilot most people think

583
00:23:50,399 --> 00:23:51,330
about when they think about distributed

584
00:23:51,330 --> 00:23:53,370
databases is the shared nothing

585
00:23:53,370 --> 00:23:55,470
architecture where you have each node

586
00:23:55,470 --> 00:23:57,809
has its own local disk and old local

587
00:23:57,809 --> 00:23:59,429
memory and the only way for me to

588
00:23:59,429 --> 00:24:01,289
coordinate as I run run queries is to

589
00:24:01,289 --> 00:24:03,570
communicate directly between my nodes so

590
00:24:03,570 --> 00:24:05,490
if I want to get data if I query shows

591
00:24:05,490 --> 00:24:06,750
up and he's accessed data on another

592
00:24:06,750 --> 00:24:10,019
machine I can't go to disk and get the

593
00:24:10,019 --> 00:24:11,100
shared disk and get it cuz that doesn't

594
00:24:11,100 --> 00:24:12,659
exist I can't read the memory from up

595
00:24:12,659 --> 00:24:14,159
from the other guy because I can't do

596
00:24:14,159 --> 00:24:15,090
that I

597
00:24:15,090 --> 00:24:17,010
a message to say hey I think you have

598
00:24:17,010 --> 00:24:19,440
this data either run this query for me

599
00:24:19,440 --> 00:24:21,420
and give me back the result or send me

600
00:24:21,420 --> 00:24:23,670
that datum and then now you get the

601
00:24:23,670 --> 00:24:24,900
issue of like who should have what copy

602
00:24:24,900 --> 00:24:28,820
of what data right we'll get that so

603
00:24:28,820 --> 00:24:30,870
this is gonna be the most hardest

604
00:24:30,870 --> 00:24:32,760
architecture to increased capacity and

605
00:24:32,760 --> 00:24:34,590
ensure consistency that's the stale read

606
00:24:34,590 --> 00:24:38,240
issue that he talked about because I

607
00:24:38,240 --> 00:24:40,770
need to be able to run the system and

608
00:24:40,770 --> 00:24:43,950
move data around in a way to ride I'm

609
00:24:43,950 --> 00:24:45,450
not losing things I'm not having false

610
00:24:45,450 --> 00:24:47,280
negatives or false positive as I execute

611
00:24:47,280 --> 00:24:51,180
queries right otherwise I shut the whole

612
00:24:51,180 --> 00:24:52,680
system down then move data around and

613
00:24:52,680 --> 00:24:55,440
add new capacity but I don't want to do

614
00:24:55,440 --> 00:24:56,490
that because I want my system to always

615
00:24:56,490 --> 00:25:00,150
be online so now you say well that this

616
00:25:00,150 --> 00:25:01,500
sounds hard why would I want to do this

617
00:25:01,500 --> 00:25:03,930
well the advantage you're gonna get over

618
00:25:03,930 --> 00:25:05,670
a shared disk system is that you're

619
00:25:05,670 --> 00:25:06,840
gonna get better performance and better

620
00:25:06,840 --> 00:25:09,720
efficiency if the system is is is

621
00:25:09,720 --> 00:25:13,140
written correctly because I can now be

622
00:25:13,140 --> 00:25:16,470
mindful of the locality of data and try

623
00:25:16,470 --> 00:25:17,730
to move the least amount of data over

624
00:25:17,730 --> 00:25:22,880
the network as much as possible yes

625
00:25:42,530 --> 00:25:47,250
correct so yeah so his statement is if

626
00:25:47,250 --> 00:25:50,700
you assume that your your your Chiba

627
00:25:50,700 --> 00:25:52,170
database is partitioned which we'll get

628
00:25:52,170 --> 00:25:55,500
to in a second if now I need to add a

629
00:25:55,500 --> 00:25:57,990
new partition and I need to reach

630
00:25:57,990 --> 00:26:00,810
potential Erie shuffle data depending on

631
00:26:00,810 --> 00:26:03,120
how I'm doing partitioning I may have to

632
00:26:03,120 --> 00:26:05,400
move the whole database I'm in a segment

633
00:26:05,400 --> 00:26:07,440
of it but again I don't want to have to

634
00:26:07,440 --> 00:26:09,690
stop the world while I move that and so

635
00:26:09,690 --> 00:26:10,890
depending how much data I have a single

636
00:26:10,890 --> 00:26:12,900
node and I'm going with a network to

637
00:26:12,900 --> 00:26:14,130
some other machine where's that machine

638
00:26:14,130 --> 00:26:17,580
how long does that take right it's if

639
00:26:17,580 --> 00:26:19,290
I'm doing this if I don't care about

640
00:26:19,290 --> 00:26:20,730
consistency which we haven't talked

641
00:26:20,730 --> 00:26:23,610
about yet then who cares just move data

642
00:26:23,610 --> 00:26:25,500
around and if you if you if you miss a

643
00:26:25,500 --> 00:26:27,870
read whatever because I do care and I am

644
00:26:27,870 --> 00:26:28,410
running turns

645
00:26:28,410 --> 00:26:30,540
then I'm gonna be very careful how I do

646
00:26:30,540 --> 00:26:33,500
this and people get burned by this a lot

647
00:26:33,500 --> 00:26:37,280
so as I said this is just a brief

648
00:26:37,280 --> 00:26:41,400
smattering of or a very limited subset

649
00:26:41,400 --> 00:26:43,320
of the some of the shared-nothing

650
00:26:43,320 --> 00:26:44,880
distributed database this was a better

651
00:26:44,880 --> 00:26:48,900
out there look most of the time for the

652
00:26:48,900 --> 00:26:50,970
no single systems that came in around

653
00:26:50,970 --> 00:26:52,650
maybe ten years ago they're all they're

654
00:26:52,650 --> 00:26:56,160
all considered shared nothing so let's

655
00:26:56,160 --> 00:26:57,900
look how this works again so no longer

656
00:26:57,900 --> 00:26:59,640
we have a shared disk on every single

657
00:26:59,640 --> 00:27:04,080
node we have a we have the CPU workers

658
00:27:04,080 --> 00:27:05,400
we have our local memory we have our

659
00:27:05,400 --> 00:27:07,410
local disk and then now what I'm showing

660
00:27:07,410 --> 00:27:10,350
is we've partitioned the database or

661
00:27:10,350 --> 00:27:14,040
shard in the database into subsets such

662
00:27:14,040 --> 00:27:16,980
that each node has some portion of of

663
00:27:16,980 --> 00:27:20,460
the database and so now I have explicit

664
00:27:20,460 --> 00:27:22,320
information about what data I'm having a

665
00:27:22,320 --> 00:27:24,660
habit each node so now the application

666
00:27:24,660 --> 00:27:26,880
says well if I want to get ID equal to

667
00:27:26,880 --> 00:27:29,070
hundred it has to know that this node

668
00:27:29,070 --> 00:27:32,700
has has the data that it needs so go

669
00:27:32,700 --> 00:27:34,890
ahead and get that and again now this is

670
00:27:34,890 --> 00:27:36,900
operating as the same single node shared

671
00:27:36,900 --> 00:27:38,730
everything data so we had before like

672
00:27:38,730 --> 00:27:40,440
it's it's not my buffer pool I go to

673
00:27:40,440 --> 00:27:42,480
just bring it in and then do whatever it

674
00:27:42,480 --> 00:27:43,710
is that I want to do to answer the query

675
00:27:43,710 --> 00:27:46,680
and return results so if all your

676
00:27:46,680 --> 00:27:49,950
queries are accessing a single node this

677
00:27:49,950 --> 00:27:52,440
this is super fast because again this is

678
00:27:52,440 --> 00:27:54,870
just a single node database system the

679
00:27:54,870 --> 00:27:56,340
tricky thing is then when you start

680
00:27:56,340 --> 00:27:58,020
touching data that's uncross multiple

681
00:27:58,020 --> 00:28:00,330
machines so let's say I have a

682
00:28:00,330 --> 00:28:02,010
transaction that says one get ID you ten

683
00:28:02,010 --> 00:28:03,720
and get ID two equal to hundred like a

684
00:28:03,720 --> 00:28:05,940
single query wants to do this so now I

685
00:28:05,940 --> 00:28:08,730
need to somehow get that data that this

686
00:28:08,730 --> 00:28:11,610
other guy has up here but what am i

687
00:28:11,610 --> 00:28:13,530
sending am i sending the request to run

688
00:28:13,530 --> 00:28:16,830
the query or am i just asking this guy

689
00:28:16,830 --> 00:28:17,820
hey I know you have this piece of data

690
00:28:17,820 --> 00:28:19,200
send it up to me and I'll run the query

691
00:28:19,200 --> 00:28:23,780
up here alright back back in very

692
00:28:24,100 --> 00:28:26,850
now in terms of the the scale out issue

693
00:28:26,850 --> 00:28:29,590
right on the shared disc architecture I

694
00:28:29,590 --> 00:28:31,270
just bring up a new compute node every

695
00:28:31,270 --> 00:28:34,510
compute node it's stateless so therefore

696
00:28:34,510 --> 00:28:36,490
it comes along and start executing

697
00:28:36,490 --> 00:28:37,990
queries and brings things from the back

698
00:28:37,990 --> 00:28:39,430
end shared disk into a buffer pool as

699
00:28:39,430 --> 00:28:41,380
needed but now and I shared nothing

700
00:28:41,380 --> 00:28:43,420
architecture if I have to say bring up a

701
00:28:43,420 --> 00:28:46,360
new node it now needs to get some

702
00:28:46,360 --> 00:28:48,160
portion of the database from these other

703
00:28:48,160 --> 00:28:50,580
nodes here so that I balance things out

704
00:28:50,580 --> 00:28:52,810
right so let's say that this guy is

705
00:28:52,810 --> 00:28:56,050
gonna send it you know a some some

706
00:28:56,050 --> 00:28:57,430
number tuples from this guy from this

707
00:28:57,430 --> 00:28:59,530
bottom partition the other guy here is

708
00:28:59,530 --> 00:29:00,640
it sent up sometime close with another

709
00:29:00,640 --> 00:29:03,550
partition and then once I've new I've

710
00:29:03,550 --> 00:29:05,470
copied the data now I update some global

711
00:29:05,470 --> 00:29:07,600
state to say all right well this node is

712
00:29:07,600 --> 00:29:09,580
now responsible for the range 101 to 200

713
00:29:09,580 --> 00:29:11,920
this guy's 201 to 300 a guy up about 1

714
00:29:11,920 --> 00:29:14,590
to 100 and I was saying to his point

715
00:29:14,590 --> 00:29:16,930
like this is this would be hard to do if

716
00:29:16,930 --> 00:29:18,670
I care about transactions and I don't

717
00:29:18,670 --> 00:29:19,990
want to lose any data because I don't

718
00:29:19,990 --> 00:29:22,480
want to have a query show up but maybe

719
00:29:22,480 --> 00:29:27,090
that wants to access ID equal 150 and

720
00:29:27,090 --> 00:29:31,480
I'll and here and the the data hasn't

721
00:29:31,480 --> 00:29:32,980
been transferred yet so maybe I can

722
00:29:32,980 --> 00:29:34,330
answer but maybe it has been transferred

723
00:29:34,330 --> 00:29:36,820
yet and I go here and it says I don't

724
00:29:36,820 --> 00:29:38,560
have that data anymore so it returns

725
00:29:38,560 --> 00:29:40,600
back nothing even though it existed this

726
00:29:40,600 --> 00:29:43,210
node down here so how did it how to

727
00:29:43,210 --> 00:29:44,770
actually do this in a transactional e

728
00:29:44,770 --> 00:29:47,350
safe manner is is tricky and not not

729
00:29:47,350 --> 00:29:51,450
easy to do yes in the back

730
00:29:57,279 --> 00:30:00,889
his question is his question is how

731
00:30:00,889 --> 00:30:03,739
often you have to scale capacity can I

732
00:30:03,739 --> 00:30:05,509
shut the database down once a month and

733
00:30:05,509 --> 00:30:07,820
add new nodes but what if I won't go to

734
00:30:07,820 --> 00:30:10,759
the other way but so what so let's say

735
00:30:10,759 --> 00:30:12,320
it's singles day or black or Black

736
00:30:12,320 --> 00:30:14,149
Friday or Cyber Monday it's the one day

737
00:30:14,149 --> 00:30:15,529
of the year where like I have a huge

738
00:30:15,529 --> 00:30:17,600
spike that one I can plan I know it's

739
00:30:17,600 --> 00:30:18,649
coming so I can prepare ahead of time

740
00:30:18,649 --> 00:30:20,409
but let's say I have like a flash mob

741
00:30:20,409 --> 00:30:23,210
right everybody wants deejay drop tables

742
00:30:23,210 --> 00:30:25,100
new album all of a sudden so also we

743
00:30:25,100 --> 00:30:26,059
have a huge spike in traffic that's

744
00:30:26,059 --> 00:30:29,570
unexpected I want to be able to scale up

745
00:30:29,570 --> 00:30:30,529
without having to shut everything down

746
00:30:30,529 --> 00:30:34,879
and scale up gradually the the the older

747
00:30:34,879 --> 00:30:36,139
systems will do exactly you're saying

748
00:30:36,139 --> 00:30:37,700
any time you see any kind of financial

749
00:30:37,700 --> 00:30:40,070
website says we're down you know Sunday

750
00:30:40,070 --> 00:30:43,190
at 3:00 a.m. there probably may probably

751
00:30:43,190 --> 00:30:44,389
not running a trivet a system but they

752
00:30:44,389 --> 00:30:46,129
they're moving data around and doing

753
00:30:46,129 --> 00:30:47,509
maintenance things but if you're an

754
00:30:47,509 --> 00:30:52,149
online website you can't do that yes

755
00:31:01,299 --> 00:31:03,710
your question is what's the advantage of

756
00:31:03,710 --> 00:31:05,330
doing this versus having a single node

757
00:31:05,330 --> 00:31:09,109
with like like you know instead of

758
00:31:09,109 --> 00:31:11,210
having two nodes run these to hold these

759
00:31:11,210 --> 00:31:12,739
two partitions what have a single node

760
00:31:12,739 --> 00:31:14,779
and you say well this CPU socket has

761
00:31:14,779 --> 00:31:16,879
this disk in this memory to run this

762
00:31:16,879 --> 00:31:18,919
partition and then another socket has

763
00:31:18,919 --> 00:31:21,440
this memory in this desk that you're

764
00:31:21,440 --> 00:31:30,559
asking all right so your question yeah

765
00:31:30,559 --> 00:31:32,710
instead of having two separate machines

766
00:31:32,710 --> 00:31:35,299
that have you know disk memory and the

767
00:31:35,299 --> 00:31:38,059
CPU what if I had one machine that just

768
00:31:38,059 --> 00:31:39,649
had the same amount of resources that it

769
00:31:39,649 --> 00:31:41,809
has put across two machines but now in

770
00:31:41,809 --> 00:31:49,759
you know a single unit so questions what

771
00:31:49,759 --> 00:31:51,049
were the advantages of doing the shiva

772
00:31:51,049 --> 00:32:00,350
Davis so one is if the you get

773
00:32:00,350 --> 00:32:02,269
diminishing returns as you scale up

774
00:32:02,269 --> 00:32:04,159
Hardware vertically so there's

775
00:32:04,159 --> 00:32:06,679
horizontal represent scalability as is

776
00:32:06,679 --> 00:32:09,080
adding new machines vertical scale

777
00:32:09,080 --> 00:32:10,290
voting to take my one machine

778
00:32:10,290 --> 00:32:11,940
and adding more resources to make it

779
00:32:11,940 --> 00:32:15,870
more powerful going vertically is way

780
00:32:15,870 --> 00:32:18,780
more expensive usually and you get

781
00:32:18,780 --> 00:32:20,370
diminishing returns and there's

782
00:32:20,370 --> 00:32:22,080
obviously an upper upper bound how how

783
00:32:22,080 --> 00:32:26,370
big you can make a machine right leave

784
00:32:26,370 --> 00:32:28,440
one example in the early days when I was

785
00:32:28,440 --> 00:32:31,830
in grad school we visited PayPal because

786
00:32:31,830 --> 00:32:34,920
PayPal was running Oracle and they were

787
00:32:34,920 --> 00:32:36,090
freaking out that because every

788
00:32:36,090 --> 00:32:38,640
Christmas they were they would hit the

789
00:32:38,640 --> 00:32:40,590
they were running Oracle on a single

790
00:32:40,590 --> 00:32:41,610
machine they bought the most expensive

791
00:32:41,610 --> 00:32:43,800
machine you could buy from IBM right and

792
00:32:43,800 --> 00:32:44,790
you had to buy two of them because you

793
00:32:44,790 --> 00:32:48,210
need a hot standby right so the every

794
00:32:48,210 --> 00:32:50,700
every every holiday season they were

795
00:32:50,700 --> 00:32:52,020
freaking out because that Oracle machine

796
00:32:52,020 --> 00:32:54,210
was was was hitting the in the limit

797
00:32:54,210 --> 00:32:55,590
what the harbor can do and they couldn't

798
00:32:55,590 --> 00:32:57,720
buy a more expensive machine right so

799
00:32:57,720 --> 00:32:58,890
they couldn't scale anyone any more

800
00:32:58,890 --> 00:33:00,390
vertically so they were mainly moving

801
00:33:00,390 --> 00:33:02,100
portions of the database off that the

802
00:33:02,100 --> 00:33:03,420
humans are moving the portions Davies

803
00:33:03,420 --> 00:33:05,700
off in like November to these separate

804
00:33:05,700 --> 00:33:07,830
machines on the side just to get to the

805
00:33:07,830 --> 00:33:09,450
holidays and then they moved it all back

806
00:33:09,450 --> 00:33:11,880
so in that environment if they had a

807
00:33:11,880 --> 00:33:14,190
distributed database system with cheaper

808
00:33:14,190 --> 00:33:16,170
machines then they'd say oh the holidays

809
00:33:16,170 --> 00:33:17,670
coming up I'm going to buy or turn on a

810
00:33:17,670 --> 00:33:20,370
bunch of new machines and had the

811
00:33:20,370 --> 00:33:21,990
systems scale out that way handle my

812
00:33:21,990 --> 00:33:24,030
high demand then when the demand goes

813
00:33:24,030 --> 00:33:25,860
down I can start turning them off and

814
00:33:25,860 --> 00:33:29,720
coalesce into smaller machines

815
00:33:45,250 --> 00:33:47,560
your question is a so these is the

816
00:33:47,560 --> 00:33:50,140
advantage of attributed versus scaling

817
00:33:50,140 --> 00:33:51,790
horizontal versus scaling vertically is

818
00:33:51,790 --> 00:33:53,620
the advantage that you can scale out

819
00:33:53,620 --> 00:33:58,450
much more cheaply horizontally for quick

820
00:33:58,450 --> 00:34:00,490
but it is there's trade-offs right like

821
00:34:00,490 --> 00:34:02,560
as we'll see as we talk about how we

822
00:34:02,560 --> 00:34:04,170
actually manage attributed a data system

823
00:34:04,170 --> 00:34:06,070
communication is now more expensive I

824
00:34:06,070 --> 00:34:07,990
can definitely run faster if I'm on a

825
00:34:07,990 --> 00:34:09,190
single node because I don't need to

826
00:34:09,190 --> 00:34:10,389
coordinate between other different nodes

827
00:34:10,389 --> 00:34:12,219
and send messages over the network but

828
00:34:12,219 --> 00:34:15,550
as I said like you you can start to hit

829
00:34:15,550 --> 00:34:23,770
scale go to bottlenecks right the the

830
00:34:23,770 --> 00:34:25,780
trend in data systems up into the 90s

831
00:34:25,780 --> 00:34:27,730
was always scale vertically the trend

832
00:34:27,730 --> 00:34:29,739
now is to scale horizontally because it

833
00:34:29,739 --> 00:34:34,330
just it's it's considered you get better

834
00:34:34,330 --> 00:34:38,650
performance and for getting the better

835
00:34:38,650 --> 00:34:41,010
performance you pay less you pay less

836
00:34:41,010 --> 00:34:43,929
is that always true yeah I think that's

837
00:34:43,929 --> 00:34:44,830
that's the convention wasn't that's

838
00:34:44,830 --> 00:34:55,770
always true yes a statement is like a

839
00:34:55,770 --> 00:34:59,070
isn't it better for disaster because

840
00:34:59,070 --> 00:35:01,660
well again if you're running like a five

841
00:35:01,660 --> 00:35:03,550
million dollar machine from IBM you're

842
00:35:03,550 --> 00:35:05,080
not plugging into the wall outlet right

843
00:35:05,080 --> 00:35:07,480
you have you have generators you have

844
00:35:07,480 --> 00:35:11,500
backup power right but I would say that

845
00:35:11,500 --> 00:35:12,880
the issues really would be the never get

846
00:35:12,880 --> 00:35:15,280
severed right if you're if the the

847
00:35:15,280 --> 00:35:17,140
network to the Machine even then even

848
00:35:17,140 --> 00:35:19,810
then you'll still have redundant Knicks

849
00:35:19,810 --> 00:35:22,450
going into it but even then if you can't

850
00:35:22,450 --> 00:35:24,400
communicate the database but potentially

851
00:35:24,400 --> 00:35:25,840
on how you design your Chuba database

852
00:35:25,840 --> 00:35:27,910
system you could have the database

853
00:35:27,910 --> 00:35:30,370
spread across different data centers and

854
00:35:30,370 --> 00:35:32,950
then you can still be available well it

855
00:35:32,950 --> 00:35:34,630
discuss more of this on

856
00:35:34,630 --> 00:35:39,280
on Wednesday but this is this is this is

857
00:35:39,280 --> 00:35:40,570
one of the trade-offs you get between

858
00:35:40,570 --> 00:35:41,800
the no single guys versus the

859
00:35:41,800 --> 00:35:44,350
traditional or new sequel or relational

860
00:35:44,350 --> 00:35:46,360
database systems the no sequel guys were

861
00:35:46,360 --> 00:35:48,850
caring about availability so they no

862
00:35:48,850 --> 00:35:50,320
matter what they wanted a website to be

863
00:35:50,320 --> 00:35:54,640
online and available and so in exchange

864
00:35:54,640 --> 00:35:56,350
they would give up transactions to make

865
00:35:56,350 --> 00:35:58,050
that happen because

866
00:35:58,050 --> 00:35:59,240
if you have to have new transactions

867
00:35:59,240 --> 00:36:01,830
then that the communication is more

868
00:36:01,830 --> 00:36:03,570
expensive you make sure that everybody

869
00:36:03,570 --> 00:36:06,990
is up in order to make changes and and

870
00:36:06,990 --> 00:36:09,000
and they argue that was that was less

871
00:36:09,000 --> 00:36:11,880
than ideal for some applications I think

872
00:36:11,880 --> 00:36:13,200
that makes sense for for anything

873
00:36:13,200 --> 00:36:15,930
financial that doesn't make sense I will

874
00:36:15,930 --> 00:36:24,540
cover the next class okay so distributed

875
00:36:24,540 --> 00:36:28,440
aliases are old some of the first ones

876
00:36:28,440 --> 00:36:32,090
were built in the late 1970s muffin was

877
00:36:32,090 --> 00:36:34,380
created by one of my advisers Mike snow

878
00:36:34,380 --> 00:36:35,880
breaker the guy who built post grass and

879
00:36:35,880 --> 00:36:38,430
ingress and vertical and volt EB he had

880
00:36:38,430 --> 00:36:39,120
a system called

881
00:36:39,120 --> 00:36:41,700
muffin that was a trivet version of of

882
00:36:41,700 --> 00:36:46,920
ingress sdd one was a I actually thought

883
00:36:46,920 --> 00:36:48,300
it was actually a real system turns out

884
00:36:48,300 --> 00:36:49,560
it was just a prototype they actually

885
00:36:49,560 --> 00:36:51,150
never actually had anything running but

886
00:36:51,150 --> 00:36:52,770
there's a lot of seminal papers in the

887
00:36:52,770 --> 00:36:54,390
late 70s written by felt the the great

888
00:36:54,390 --> 00:36:57,870
Phil Bernstein on on how to build a

889
00:36:57,870 --> 00:36:59,250
tributed database and do transactions

890
00:36:59,250 --> 00:37:01,800
across them a lot of a transactional

891
00:37:01,800 --> 00:37:03,000
theory that we talked about in this

892
00:37:03,000 --> 00:37:05,460
class all right all that early work was

893
00:37:05,460 --> 00:37:08,940
done by Phil system R star was a

894
00:37:08,940 --> 00:37:10,800
research project at of IBM that was the

895
00:37:10,800 --> 00:37:15,420
tribute version of of system or that

896
00:37:15,420 --> 00:37:16,830
never became a product although there is

897
00:37:16,830 --> 00:37:19,260
a distributor version of db2 today gamma

898
00:37:19,260 --> 00:37:20,670
was an influential system out of

899
00:37:20,670 --> 00:37:22,980
Wisconsin by Dave DeWitt that was one of

900
00:37:22,980 --> 00:37:25,320
the first like high-performance Toshiba

901
00:37:25,320 --> 00:37:28,170
Devi systems and then non-stop sequel of

902
00:37:28,170 --> 00:37:29,910
all these is the only was the only

903
00:37:29,910 --> 00:37:31,350
commercial distributor database system

904
00:37:31,350 --> 00:37:34,470
and that was that was helped built or

905
00:37:34,470 --> 00:37:35,970
Jim Gray helped build this Jim Gray was

906
00:37:35,970 --> 00:37:39,060
the guy who was at IBM invented like

907
00:37:39,060 --> 00:37:40,380
two-phase locking and a lot of the early

908
00:37:40,380 --> 00:37:42,090
stuff that we talked about under system

909
00:37:42,090 --> 00:37:44,340
are so nonstop was an interesting

910
00:37:44,340 --> 00:37:47,310
company they originally were selling

911
00:37:47,310 --> 00:37:49,260
these like super fault-tolerant machines

912
00:37:49,260 --> 00:37:50,970
like think of like redundant hardware

913
00:37:50,970 --> 00:37:53,310
like space shuttle level redundancy like

914
00:37:53,310 --> 00:37:54,990
you have for CPU is running and if one

915
00:37:54,990 --> 00:37:56,250
goes down the other three you can keep

916
00:37:56,250 --> 00:37:58,170
on running so they would sell a database

917
00:37:58,170 --> 00:38:00,000
system that would sort of build on this

918
00:38:00,000 --> 00:38:02,430
architecture um it's still round today a

919
00:38:02,430 --> 00:38:04,350
lot of financial systems actually still

920
00:38:04,350 --> 00:38:07,650
still still use this and it's amazing

921
00:38:07,650 --> 00:38:08,640
how long it still runs

922
00:38:08,640 --> 00:38:10,830
I guess it's nonstop right

923
00:38:10,830 --> 00:38:14,530
all right so uh all right so now we now

924
00:38:14,530 --> 00:38:17,590
that we understand the the what the

925
00:38:17,590 --> 00:38:20,050
architecture looks like a lot of you

926
00:38:20,050 --> 00:38:21,490
have these questions that like hey how

927
00:38:21,490 --> 00:38:23,380
is this thing actually gonna work how to

928
00:38:23,380 --> 00:38:24,910
actually find data how do we actually

929
00:38:24,910 --> 00:38:26,110
make sure that everything is consistent

930
00:38:26,110 --> 00:38:28,420
so all these things we need to be

931
00:38:28,420 --> 00:38:30,580
mindful of now when we build and shape a

932
00:38:30,580 --> 00:38:31,600
data system and there's trade-offs

933
00:38:31,600 --> 00:38:32,650
because we do not be able to do

934
00:38:32,650 --> 00:38:34,930
everything so we're not gonna have a

935
00:38:34,930 --> 00:38:36,460
system to be guaranteed online all the

936
00:38:36,460 --> 00:38:36,970
time

937
00:38:36,970 --> 00:38:39,460
and make sure that we always support

938
00:38:39,460 --> 00:38:41,320
transactions and and not lose any data

939
00:38:41,320 --> 00:38:43,930
or have inconsistent results so as we go

940
00:38:43,930 --> 00:38:44,670
along

941
00:38:44,670 --> 00:38:47,110
we'll see what these trade-offs are and

942
00:38:47,110 --> 00:38:49,140
why you're not gonna achieve everything

943
00:38:49,140 --> 00:38:51,220
the other big question we're gonna have

944
00:38:51,220 --> 00:38:53,380
is how we actually execute the queries

945
00:38:53,380 --> 00:38:56,290
on this distributed data and so I showed

946
00:38:56,290 --> 00:38:58,360
two examples so far I showed the example

947
00:38:58,360 --> 00:39:00,070
on shared disk where the compute nodes

948
00:39:00,070 --> 00:39:02,470
pull the data from the shared disk

949
00:39:02,470 --> 00:39:05,050
system into their local memory and

950
00:39:05,050 --> 00:39:07,330
compute the compute the result and then

951
00:39:07,330 --> 00:39:10,390
in the case of the share nothing system

952
00:39:10,390 --> 00:39:12,520
we would send the query to where the

953
00:39:12,520 --> 00:39:14,890
data was located run that locally and

954
00:39:14,890 --> 00:39:16,420
then get back the result

955
00:39:16,420 --> 00:39:18,340
so there's trade-off between how you

956
00:39:18,340 --> 00:39:19,870
actually want to better doing a pusher

957
00:39:19,870 --> 00:39:23,350
or a pool so the last thing to talk

958
00:39:23,350 --> 00:39:25,750
about too is what does the architecture

959
00:39:25,750 --> 00:39:27,250
look like in terms of what are the nodes

960
00:39:27,250 --> 00:39:29,020
doing in the cluster for the Shiva

961
00:39:29,020 --> 00:39:32,260
database and there's basis you know

962
00:39:32,260 --> 00:39:33,490
there's just two approaches you either

963
00:39:33,490 --> 00:39:35,140
have a homogeneous cluster or a

964
00:39:35,140 --> 00:39:36,760
heterogeneous cluster so in a

965
00:39:36,760 --> 00:39:39,010
homogeneous cluster every single node in

966
00:39:39,010 --> 00:39:41,710
the database cluster is can perform

967
00:39:41,710 --> 00:39:44,080
every single kind of task you'd ever

968
00:39:44,080 --> 00:39:46,960
have so I mean like you could send a

969
00:39:46,960 --> 00:39:48,670
query to any single node and that node

970
00:39:48,670 --> 00:39:51,580
will figure out how to get the result

971
00:39:51,580 --> 00:39:53,530
that you're looking for and they're all

972
00:39:53,530 --> 00:39:54,730
gonna give me a potentially background

973
00:39:54,730 --> 00:39:57,820
tasks and and other things so the

974
00:39:57,820 --> 00:40:00,730
advantage of this approach is that it

975
00:40:00,730 --> 00:40:02,620
makes provisioning and failover

976
00:40:02,620 --> 00:40:05,380
potentially easier to handle and support

977
00:40:05,380 --> 00:40:08,260
because now I just add new nodes and

978
00:40:08,260 --> 00:40:09,850
long as I'm you know like I can move

979
00:40:09,850 --> 00:40:13,030
data around safely I can add new nodes

980
00:40:13,030 --> 00:40:14,740
and there's you know the system gets

981
00:40:14,740 --> 00:40:20,590
gets stronger it gets better in a up

982
00:40:20,590 --> 00:40:22,240
into a point which we'll see you next

983
00:40:22,240 --> 00:40:24,430
class another approach to do header

984
00:40:24,430 --> 00:40:26,590
heterogeneous cluster where you can have

985
00:40:26,590 --> 00:40:29,890
specific nodes or members of the the

986
00:40:29,890 --> 00:40:32,280
database system be responsible for

987
00:40:32,280 --> 00:40:36,820
separate tasks and so now I can't to

988
00:40:36,820 --> 00:40:38,260
make a decision say if I'm running my

989
00:40:38,260 --> 00:40:39,910
systems running slower I want to add new

990
00:40:39,910 --> 00:40:42,250
nodes I have to know what I should add a

991
00:40:42,250 --> 00:40:44,530
node for this type of node or this other

992
00:40:44,530 --> 00:40:46,240
class of node right I have to make a

993
00:40:46,240 --> 00:40:49,570
decision at that at that level so give

994
00:40:49,570 --> 00:40:52,570
me example what a one of these

995
00:40:52,570 --> 00:40:54,160
architectures I always like to use

996
00:40:54,160 --> 00:40:56,800
MongoDB because that's the most basic

997
00:40:56,800 --> 00:40:57,790
one to understand

998
00:40:57,790 --> 00:41:00,130
so MongoDB uses what is known as a

999
00:41:00,130 --> 00:41:01,900
heterogeneous cluster architecture

1000
00:41:01,900 --> 00:41:03,700
so you have you have special-purpose

1001
00:41:03,700 --> 00:41:05,260
nodes that are responsible or doing

1002
00:41:05,260 --> 00:41:08,770
specific tasks in the system so in the

1003
00:41:08,770 --> 00:41:10,300
application wants to send a request or

1004
00:41:10,300 --> 00:41:13,060
execute a query it always goes to this

1005
00:41:13,060 --> 00:41:15,790
router and and so the router looks at

1006
00:41:15,790 --> 00:41:17,200
the request and says you know I want to

1007
00:41:17,200 --> 00:41:20,560
look at I want to get record with PID

1008
00:41:20,560 --> 00:41:23,560
equal 101 these guys are stateless they

1009
00:41:23,560 --> 00:41:25,330
don't know about what any of the data is

1010
00:41:25,330 --> 00:41:28,030
on the actual shards so goes to this

1011
00:41:28,030 --> 00:41:31,150
config server node that it's responsible

1012
00:41:31,150 --> 00:41:33,400
for spending sending is sending out back

1013
00:41:33,400 --> 00:41:36,490
the information about where to find data

1014
00:41:36,490 --> 00:41:39,280
on these different partitions or these

1015
00:41:39,280 --> 00:41:41,470
shards here so that's all this thing

1016
00:41:41,470 --> 00:41:42,640
does this thing is responsible it is

1017
00:41:42,640 --> 00:41:45,250
like a global State for what the

1018
00:41:45,250 --> 00:41:48,130
configuration of the system is so now

1019
00:41:48,130 --> 00:41:49,870
the router used gets this routing table

1020
00:41:49,870 --> 00:41:51,520
from the config server and then it can

1021
00:41:51,520 --> 00:41:54,460
send the request to the manga D or the

1022
00:41:54,460 --> 00:41:56,740
shard server and then that sort actually

1023
00:41:56,740 --> 00:41:59,170
excuse the query and get gets back the

1024
00:41:59,170 --> 00:42:00,330
result

1025
00:42:00,330 --> 00:42:03,010
so under this architecture again if I

1026
00:42:03,010 --> 00:42:05,670
notice that oh my my router

1027
00:42:05,670 --> 00:42:07,870
infrastructure is my bottleneck that I

1028
00:42:07,870 --> 00:42:09,400
can scale this thing out and add more

1029
00:42:09,400 --> 00:42:11,620
new nodes without touching the config

1030
00:42:11,620 --> 00:42:14,770
server or the or the the Charlotte

1031
00:42:14,770 --> 00:42:15,540
servers

1032
00:42:15,540 --> 00:42:18,540
yes

1033
00:42:20,680 --> 00:42:22,030
this question is what sin is able to

1034
00:42:22,030 --> 00:42:23,470
test so like garbage question we talk

1035
00:42:23,470 --> 00:42:27,580
about MVCC or building indexes or moving

1036
00:42:27,580 --> 00:42:29,320
data around because I'm I'm I'm I'm

1037
00:42:29,320 --> 00:42:34,870
scaling up or scaling down again like

1038
00:42:34,870 --> 00:42:36,490
you can't send a query to this guy here

1039
00:42:36,490 --> 00:42:38,200
he can only tell you what the

1040
00:42:38,200 --> 00:42:39,310
configuration of the system looks like

1041
00:42:39,310 --> 00:42:42,280
and this guy can't hold any data you can

1042
00:42:42,280 --> 00:42:43,510
only tell you how to send your you know

1043
00:42:43,510 --> 00:42:49,960
where to send your query so the other

1044
00:42:49,960 --> 00:42:51,820
thing we sort of briefly touched upon is

1045
00:42:51,820 --> 00:42:53,590
about this notion of data transparency

1046
00:42:53,590 --> 00:42:56,560
in a distributed system and that's where

1047
00:42:56,560 --> 00:42:59,380
we don't want ideally the application to

1048
00:42:59,380 --> 00:43:03,160
know anything about how the data is is

1049
00:43:03,160 --> 00:43:06,040
split up and divided or replicated

1050
00:43:06,040 --> 00:43:07,840
across the different nodes in our

1051
00:43:07,840 --> 00:43:11,020
cluster so the same sequel query or

1052
00:43:11,020 --> 00:43:12,700
whatever query language I'm using and my

1053
00:43:12,700 --> 00:43:14,680
application for my database system I if

1054
00:43:14,680 --> 00:43:16,780
I'm running on one node that same query

1055
00:43:16,780 --> 00:43:18,580
should still work and still produce the

1056
00:43:18,580 --> 00:43:21,400
correct sync same result if now I'm

1057
00:43:21,400 --> 00:43:24,940
scared out on a thousand nodes because

1058
00:43:24,940 --> 00:43:26,290
otherwise if I have a query says like

1059
00:43:26,290 --> 00:43:27,370
you know a select star statement and

1060
00:43:27,370 --> 00:43:28,450
then you have like some special thing

1061
00:43:28,450 --> 00:43:30,520
that says you know we're node equals one

1062
00:43:30,520 --> 00:43:32,350
two three if one two three gets now

1063
00:43:32,350 --> 00:43:33,790
split up across multiple Sheen's or not

1064
00:43:33,790 --> 00:43:36,070
one two three goes away I don't want to

1065
00:43:36,070 --> 00:43:37,510
go back and rewrite all my out I'm all

1066
00:43:37,510 --> 00:43:40,270
all my application code so we're gonna

1067
00:43:40,270 --> 00:43:42,940
hide all the details from the

1068
00:43:42,940 --> 00:43:45,040
application where the data is actually

1069
00:43:45,040 --> 00:43:47,200
being stored although we can push some

1070
00:43:47,200 --> 00:43:50,620
information to the client level at a

1071
00:43:50,620 --> 00:43:52,840
driver it allowed to figure out what

1072
00:43:52,840 --> 00:43:54,640
node he wants to go talk to you but our

1073
00:43:54,640 --> 00:43:55,390
application code

1074
00:43:55,390 --> 00:43:57,550
you know the Joe Schmoe programmer

1075
00:43:57,550 --> 00:43:58,750
should not know anything about how the

1076
00:43:58,750 --> 00:44:01,960
data is split up ideally it's not always

1077
00:44:01,960 --> 00:44:05,890
the case but this is what we want so now

1078
00:44:05,890 --> 00:44:07,060
to talk about how we're gonna split the

1079
00:44:07,060 --> 00:44:08,350
data up we've already sort of touched on

1080
00:44:08,350 --> 00:44:10,780
this a little bit we're gonna use we're

1081
00:44:10,780 --> 00:44:12,970
going to use partitioning why can't we

1082
00:44:12,970 --> 00:44:15,690
talk about this as well when we did I

1083
00:44:15,690 --> 00:44:18,430
think was the this time one of the types

1084
00:44:18,430 --> 00:44:19,570
of boarding protocols talked about this

1085
00:44:19,570 --> 00:44:20,830
and we talked about us with with

1086
00:44:20,830 --> 00:44:23,410
parallel execution the idea here is that

1087
00:44:23,410 --> 00:44:24,880
we're going to take our database and

1088
00:44:24,880 --> 00:44:28,090
split it up into disjoint subsets that

1089
00:44:28,090 --> 00:44:30,400
were then gonna assign to different

1090
00:44:30,400 --> 00:44:32,530
different resources if you're coming

1091
00:44:32,530 --> 00:44:34,360
from the no sequel world they're going

1092
00:44:34,360 --> 00:44:37,150
called a sharding but partitions and

1093
00:44:37,150 --> 00:44:40,660
shards are essentially the same thing so

1094
00:44:40,660 --> 00:44:41,920
now what's gonna happen as the Davis

1095
00:44:41,920 --> 00:44:44,500
system is going to get a query and it's

1096
00:44:44,500 --> 00:44:47,020
gonna look at what data the different

1097
00:44:47,020 --> 00:44:48,310
parts of that query plan need to access

1098
00:44:48,310 --> 00:44:51,370
and then it may potentially need to send

1099
00:44:51,370 --> 00:44:54,190
fragments of the plan to different nodes

1100
00:44:54,190 --> 00:44:56,080
to go have them exit that part of the

1101
00:44:56,080 --> 00:44:59,110
query and then send back the result that

1102
00:44:59,110 --> 00:45:01,150
they generated and we can use that same

1103
00:45:01,150 --> 00:45:02,620
exchange operator we talked about before

1104
00:45:02,620 --> 00:45:04,450
under the iterator model when we did a

1105
00:45:04,450 --> 00:45:05,920
parallel queries that same exchange

1106
00:45:05,920 --> 00:45:08,020
operator is how we can paralyze things

1107
00:45:08,020 --> 00:45:12,040
in attribute environment so let's talk

1108
00:45:12,040 --> 00:45:13,180
about how we actually split our tables

1109
00:45:13,180 --> 00:45:17,050
up so the most simplest way to do table

1110
00:45:17,050 --> 00:45:19,900
partitioning is you just take a single

1111
00:45:19,900 --> 00:45:21,790
table and you have every single node you

1112
00:45:21,790 --> 00:45:23,650
have each node store one one of those

1113
00:45:23,650 --> 00:45:26,380
tables so I have three tables a B and C

1114
00:45:26,380 --> 00:45:30,040
node one gets a no two gets be no 3 gets

1115
00:45:30,040 --> 00:45:31,780
C that's the easiest way to do

1116
00:45:31,780 --> 00:45:34,360
partitioning alright for this one

1117
00:45:34,360 --> 00:45:35,680
obviously have to assume that the table

1118
00:45:35,680 --> 00:45:39,040
can fit on a single node but for valve

1119
00:45:39,040 --> 00:45:39,670
it's fine

1120
00:45:39,670 --> 00:45:42,220
so I have two tables one and two I just

1121
00:45:42,220 --> 00:45:45,280
take all again all the tuples in table

1122
00:45:45,280 --> 00:45:46,810
one goes to one partition all the tuples

1123
00:45:46,810 --> 00:45:49,330
in table to go to another partition so

1124
00:45:49,330 --> 00:45:51,130
the ideal query in this environment is

1125
00:45:51,130 --> 00:45:54,460
any query that obviously touches one

1126
00:45:54,460 --> 00:45:57,700
table because now I don't need to

1127
00:45:57,700 --> 00:46:00,010
communicate through between these

1128
00:46:00,010 --> 00:46:01,450
different nodes I just send my query to

1129
00:46:01,450 --> 00:46:03,130
this one node it runs and I send back

1130
00:46:03,130 --> 00:46:06,610
the result again I'll get parallelism

1131
00:46:06,610 --> 00:46:08,980
assuming that I my workload is easily

1132
00:46:08,980 --> 00:46:11,500
divided across these two two tables but

1133
00:46:11,500 --> 00:46:12,610
we obviously know that's not always the

1134
00:46:12,610 --> 00:46:16,530
case that's not realistic so the only

1135
00:46:16,530 --> 00:46:18,820
very few systems will let you do this I

1136
00:46:18,820 --> 00:46:20,710
know DB can DB you can say

1137
00:46:20,710 --> 00:46:22,510
in their world they called a collection

1138
00:46:22,510 --> 00:46:24,760
instead of a table you can tell Bangi to

1139
00:46:24,760 --> 00:46:28,030
be store a table on this one you know on

1140
00:46:28,030 --> 00:46:32,380
a single node by itself but this this is

1141
00:46:32,380 --> 00:46:34,090
it this isn't that common in other

1142
00:46:34,090 --> 00:46:40,630
systems yes this question is what are

1143
00:46:40,630 --> 00:46:44,730
these partitions doesn't matter

1144
00:46:45,059 --> 00:46:47,640
for simplicity assume is shared nothing

1145
00:46:47,640 --> 00:46:50,650
actually yep in assume is shared nothing

1146
00:46:50,650 --> 00:46:52,329
in a shared disk architecture you don't

1147
00:46:52,329 --> 00:46:54,999
have you don't necessarily have fine

1148
00:46:54,999 --> 00:46:58,960
grain control like this you could could

1149
00:46:58,960 --> 00:47:00,789
you basically you could to say in like

1150
00:47:00,789 --> 00:47:02,769
in like s3 you just have different

1151
00:47:02,769 --> 00:47:07,180
buckets for different tables but you you

1152
00:47:07,180 --> 00:47:08,349
don't know any information you don't

1153
00:47:08,349 --> 00:47:09,309
have any information where it's actually

1154
00:47:09,309 --> 00:47:11,259
being stored so assume this is shared

1155
00:47:11,259 --> 00:47:16,960
nothing what is more common we get most

1156
00:47:16,960 --> 00:47:17,979
people think about in a distributed a

1157
00:47:17,979 --> 00:47:20,039
this is to do horizontal partitioning

1158
00:47:20,039 --> 00:47:22,119
Frost again we're assuming we're doing a

1159
00:47:22,119 --> 00:47:24,849
roast door system so for this one we're

1160
00:47:24,849 --> 00:47:27,630
gonna split the table up row by row by

1161
00:47:27,630 --> 00:47:30,700
looking at one or more columns as as the

1162
00:47:30,700 --> 00:47:34,390
partitioning key and examining the value

1163
00:47:34,390 --> 00:47:35,529
of those partitioning keys and then

1164
00:47:35,529 --> 00:47:39,390
deciding what partition to assign it to

1165
00:47:39,390 --> 00:47:43,029
so again in in a shared disk system so I

1166
00:47:43,029 --> 00:47:44,410
shared nothing system you do physical

1167
00:47:44,410 --> 00:47:46,479
partitioning because every nodes gonna

1168
00:47:46,479 --> 00:47:48,430
have actually store locally on at the

1169
00:47:48,430 --> 00:47:52,180
local disk its partition and then in a

1170
00:47:52,180 --> 00:47:54,359
shared disk system you would do a

1171
00:47:54,359 --> 00:47:56,650
logical partitioning where you fit you

1172
00:47:56,650 --> 00:47:59,140
assign a compute node to be allowed to

1173
00:47:59,140 --> 00:48:03,309
access a particular partition so that

1174
00:48:03,309 --> 00:48:04,900
you know you don't have a copy of the

1175
00:48:04,900 --> 00:48:07,660
same page across multiple multiple nodes

1176
00:48:07,660 --> 00:48:09,039
to reduce the amount of coordination you

1177
00:48:09,039 --> 00:48:11,829
have to do so let's look symposium it

1178
00:48:11,829 --> 00:48:14,049
like this let's say that we select this

1179
00:48:14,049 --> 00:48:16,960
column as the partitioning key and we're

1180
00:48:16,960 --> 00:48:19,420
gonna do hash partitioning which is just

1181
00:48:19,420 --> 00:48:21,039
we're gonna scan through and look at the

1182
00:48:21,039 --> 00:48:22,539
value for every single to boil for this

1183
00:48:22,539 --> 00:48:24,880
particular column and there's gonna hash

1184
00:48:24,880 --> 00:48:27,190
it mod by the number of partitions we

1185
00:48:27,190 --> 00:48:30,249
have and then that will tell us where

1186
00:48:30,249 --> 00:48:31,569
we're to actually want we want to go

1187
00:48:31,569 --> 00:48:35,019
send the data so now if a query shows up

1188
00:48:35,019 --> 00:48:36,819
and it's like you know select star from

1189
00:48:36,819 --> 00:48:39,249
table where partition key equals some

1190
00:48:39,249 --> 00:48:42,009
value we just take that value running

1191
00:48:42,009 --> 00:48:43,450
through our same hash function and now

1192
00:48:43,450 --> 00:48:46,950
we know exactly where our partition is

1193
00:48:47,099 --> 00:48:49,180
so this is hash partitioning you also

1194
00:48:49,180 --> 00:48:52,650
you can do range partitioning where

1195
00:48:52,650 --> 00:48:54,670
which I've shown before you basically

1196
00:48:54,670 --> 00:48:56,890
say you know this contiguous segments of

1197
00:48:56,890 --> 00:48:58,150
the value space

1198
00:48:58,150 --> 00:49:00,579
column goes to this partition then the

1199
00:49:00,579 --> 00:49:02,380
next we know 100 keys go to this next

1200
00:49:02,380 --> 00:49:04,750
partition and then same thing the query

1201
00:49:04,750 --> 00:49:06,130
shows up you look at the value they're

1202
00:49:06,130 --> 00:49:09,130
trying to do a lookup one and I you know

1203
00:49:09,130 --> 00:49:10,750
I know where to route the data that I

1204
00:49:10,750 --> 00:49:12,369
want or go route the query to find the

1205
00:49:12,369 --> 00:49:35,349
data where I want yes yes her question

1206
00:49:35,349 --> 00:49:39,690
is the just rephrase your question

1207
00:49:39,690 --> 00:49:41,920
selecting what partitioning key to use

1208
00:49:41,920 --> 00:49:43,510
is an actually an np-complete problem

1209
00:49:43,510 --> 00:49:44,529
because there's so many different

1210
00:49:44,529 --> 00:49:46,900
combinations I could do how do I know

1211
00:49:46,900 --> 00:49:51,520
what to do so this is something I

1212
00:49:51,520 --> 00:49:53,109
actually have done research on there's a

1213
00:49:53,109 --> 00:49:54,789
there's like a forty fifty year history

1214
00:49:54,789 --> 00:49:56,319
of people developing different methods

1215
00:49:56,319 --> 00:49:57,940
and algorithms to pick the partitioning

1216
00:49:57,940 --> 00:50:02,230
key again my advisers adviser wrote one

1217
00:50:02,230 --> 00:50:04,329
in the 70s and he's dead I wrote one

1218
00:50:04,329 --> 00:50:07,990
right it's basically it's like a search

1219
00:50:07,990 --> 00:50:09,099
optimization problem I look at my

1220
00:50:09,099 --> 00:50:09,520
workload

1221
00:50:09,520 --> 00:50:11,319
I see how I'm accessing like my queries

1222
00:50:11,319 --> 00:50:13,329
my queries are accessing the table and

1223
00:50:13,329 --> 00:50:15,099
I'm seeing this thing you know partition

1224
00:50:15,099 --> 00:50:16,539
key you know something equals something

1225
00:50:16,539 --> 00:50:18,039
over and over again then that's

1226
00:50:18,039 --> 00:50:19,869
obviously the one I want to choose for

1227
00:50:19,869 --> 00:50:22,630
all TV applications oftentimes you can

1228
00:50:22,630 --> 00:50:25,000
we'll talk about snacks class you can

1229
00:50:25,000 --> 00:50:26,589
almost develop like a tree schema and

1230
00:50:26,589 --> 00:50:29,559
identify like brain or passed down to

1231
00:50:29,559 --> 00:50:31,089
the tree that you then split everything

1232
00:50:31,089 --> 00:50:34,029
up so for example like say Amazon

1233
00:50:34,029 --> 00:50:38,710
divides up its its database based on

1234
00:50:38,710 --> 00:50:42,400
like state where the customer is located

1235
00:50:42,400 --> 00:50:43,599
so here's all the customers in

1236
00:50:43,599 --> 00:50:46,200
Pennsylvania and then here's all the

1237
00:50:46,200 --> 00:50:48,609
orders for the customers in Pennsylvania

1238
00:50:48,609 --> 00:50:50,020
here's all the items that they bought in

1239
00:50:50,020 --> 00:50:51,730
Pennsylvania so I can take all of the

1240
00:50:51,730 --> 00:50:55,299
Pennsylvania customers and put them in

1241
00:50:55,299 --> 00:50:58,000
one partition all the the Maryland ones

1242
00:50:58,000 --> 00:51:00,460
going on another partition so it's a lot

1243
00:51:00,460 --> 00:51:02,619
of times it's sort of obvious what that

1244
00:51:02,619 --> 00:51:05,829
key should be for OTP for all apps of it

1245
00:51:05,829 --> 00:51:06,789
more tricky you definitely have to look

1246
00:51:06,789 --> 00:51:08,140
at the queries what the queries are

1247
00:51:08,140 --> 00:51:10,180
because again you want to minimize mount

1248
00:51:10,180 --> 00:51:11,260
of coordination or data you're sending

1249
00:51:11,260 --> 00:51:11,680
between diff

1250
00:51:11,680 --> 00:51:25,540
partitions yes question is if we have an

1251
00:51:25,540 --> 00:51:29,319
index on the partitioning key will this

1252
00:51:29,319 --> 00:51:30,910
have an impact on the design I mean the

1253
00:51:30,910 --> 00:51:33,640
selection of the partitioning key what

1254
00:51:33,640 --> 00:51:48,250
do you my design alright so this

1255
00:51:48,250 --> 00:51:51,190
question is will get that this question

1256
00:51:51,190 --> 00:51:53,230
is this is my query my application sends

1257
00:51:53,230 --> 00:51:57,190
us how do I know that where to go what

1258
00:51:57,190 --> 00:51:58,589
partition has the data I'm looking for

1259
00:51:58,589 --> 00:52:00,970
like how does it know that it uses hash

1260
00:52:00,970 --> 00:52:04,170
function and send the query so if it's a

1261
00:52:04,170 --> 00:52:05,950
heterogeneous system you could have a

1262
00:52:05,950 --> 00:52:07,839
front-end query router like a did

1263
00:52:07,839 --> 00:52:10,300
it say oh I know the sharding key is

1264
00:52:10,300 --> 00:52:12,849
this thing here so let me go pick that

1265
00:52:12,849 --> 00:52:14,559
out of the query hash this value and

1266
00:52:14,559 --> 00:52:15,970
then I say that's where I want to go if

1267
00:52:15,970 --> 00:52:18,940
it's a shared nothing system with the

1268
00:52:18,940 --> 00:52:21,490
homogeneous architecture you could say I

1269
00:52:21,490 --> 00:52:24,670
land on p1 p1 says oh you won't actually

1270
00:52:24,670 --> 00:52:26,559
this query but I don't have this data p3

1271
00:52:26,559 --> 00:52:28,390
has it so there's rot your query for you

1272
00:52:28,390 --> 00:52:30,250
or it sends your query down here runs it

1273
00:52:30,250 --> 00:52:31,359
and then sends back the result through

1274
00:52:31,359 --> 00:52:36,430
p1 there's different ways to do this all

1275
00:52:36,430 --> 00:52:40,510
right so I'm showing showing hash

1276
00:52:40,510 --> 00:52:43,510
partitioning here right we just take the

1277
00:52:43,510 --> 00:52:44,950
hash value mod by the number of

1278
00:52:44,950 --> 00:52:47,170
partitions I have and that tells me

1279
00:52:47,170 --> 00:52:49,059
where I need to go what's the problem of

1280
00:52:49,059 --> 00:52:59,440
this is this collision ignoring

1281
00:52:59,440 --> 00:53:00,369
collision assume we have a good hash

1282
00:53:00,369 --> 00:53:02,640
value

1283
00:53:04,570 --> 00:53:10,400
so he says well yeah so if you do hash

1284
00:53:10,400 --> 00:53:11,870
partitioning if you do its crunch will

1285
00:53:11,870 --> 00:53:13,730
scan like if this is a range predicate

1286
00:53:13,730 --> 00:53:15,230
set of a quality predicate hash

1287
00:53:15,230 --> 00:53:17,060
partition is a bad idea because I can't

1288
00:53:17,060 --> 00:53:18,680
hash a range just the same time of the

1289
00:53:18,680 --> 00:53:27,380
hash table but not something else so his

1290
00:53:27,380 --> 00:53:29,360
question is if I update the partition

1291
00:53:29,360 --> 00:53:31,670
key right if I said a partitioning on

1292
00:53:31,670 --> 00:53:33,170
this column I partition this column I

1293
00:53:33,170 --> 00:53:35,120
got to move everything around yes but

1294
00:53:35,120 --> 00:53:37,070
that doesn't happen that often right

1295
00:53:37,070 --> 00:53:40,160
like like think about like your your

1296
00:53:40,160 --> 00:53:43,130
your Amazon account ID that they're not

1297
00:53:43,130 --> 00:53:44,690
they're not gonna say all right we're

1298
00:53:44,690 --> 00:53:45,620
not partitioning on that and what were

1299
00:53:45,620 --> 00:53:46,760
purchasing on his other than your email

1300
00:53:46,760 --> 00:53:53,150
address that rarely ever happens bingo

1301
00:53:53,150 --> 00:53:56,240
so he says if I if I had a 5th partition

1302
00:53:56,240 --> 00:53:58,970
here I have that same problem I had when

1303
00:53:58,970 --> 00:54:00,500
I we were talking on hash tables and I

1304
00:54:00,500 --> 00:54:01,580
see why we have to talk about the single

1305
00:54:01,580 --> 00:54:03,350
node stuff first if I had a 5th

1306
00:54:03,350 --> 00:54:05,570
partition here now if I rehash all the

1307
00:54:05,570 --> 00:54:07,220
values and modify five they're not

1308
00:54:07,220 --> 00:54:08,660
guaranteed to be the same partitions I

1309
00:54:08,660 --> 00:54:11,120
may end up moving the entire database

1310
00:54:11,120 --> 00:54:12,830
everyone might might be swapping and

1311
00:54:12,830 --> 00:54:17,920
moving to another location so that's bad

1312
00:54:17,920 --> 00:54:21,500
so we need a way to handle that who here

1313
00:54:21,500 --> 00:54:23,740
has ever heard of consistent hashing

1314
00:54:23,740 --> 00:54:27,530
very few good ok perfect so consistent

1315
00:54:27,530 --> 00:54:28,940
hashing was a technique developed in the

1316
00:54:28,940 --> 00:54:31,550
early 2000s and the way it basically has

1317
00:54:31,550 --> 00:54:32,870
to do it allowed to do incremental

1318
00:54:32,870 --> 00:54:35,570
updates and removals of partitions in

1319
00:54:35,570 --> 00:54:37,760
your cluster without having to move

1320
00:54:37,760 --> 00:54:40,280
everything around so the way to think

1321
00:54:40,280 --> 00:54:42,710
about this is that the hashing space is

1322
00:54:42,710 --> 00:54:47,150
just a ring 0 to 1 and so I'm gonna have

1323
00:54:47,150 --> 00:54:50,570
say three partitions a B and C so the

1324
00:54:50,570 --> 00:54:51,740
way to think about this is like if I

1325
00:54:51,740 --> 00:54:54,410
hash now key and I don't modify the

1326
00:54:54,410 --> 00:54:56,030
number of partitions I just hash it and

1327
00:54:56,030 --> 00:54:58,910
say you put it between 0 1 Sally and at

1328
00:54:58,910 --> 00:55:01,640
this point in the ring so then I travel

1329
00:55:01,640 --> 00:55:04,310
forward going clockwise motion until I

1330
00:55:04,310 --> 00:55:07,310
find the node that has the first node

1331
00:55:07,310 --> 00:55:09,290
that shows up and that's where I know my

1332
00:55:09,290 --> 00:55:11,960
data it's gonna be located so I hash it

1333
00:55:11,960 --> 00:55:14,360
I get a value put it between 0 & 1 and I

1334
00:55:14,360 --> 00:55:16,400
know that in between this you know

1335
00:55:16,400 --> 00:55:18,289
from doing this and this is a so the

1336
00:55:18,289 --> 00:55:21,410
data I want is on that right same thing

1337
00:55:21,410 --> 00:55:23,539
over here I hash to I land here so we're

1338
00:55:23,539 --> 00:55:25,670
in the ring space and I jump here to go

1339
00:55:25,670 --> 00:55:28,849
to sea so again the way to the the key

1340
00:55:28,849 --> 00:55:32,509
space for all these guys is is from from

1341
00:55:32,509 --> 00:55:34,940
the bueno punchin starts back into the

1342
00:55:34,940 --> 00:55:39,259
next partition right that's fine that's

1343
00:55:39,259 --> 00:55:39,829
not so great

1344
00:55:39,829 --> 00:55:42,170
what matters now is that when I add new

1345
00:55:42,170 --> 00:55:45,230
nodes against AI my disturb a database

1346
00:55:45,230 --> 00:55:46,670
can't keep up with the traffic I'm

1347
00:55:46,670 --> 00:55:48,499
trying to support so I want to add new

1348
00:55:48,499 --> 00:55:50,900
machine it's and scale out so let's say

1349
00:55:50,900 --> 00:55:53,930
I had a new partition here deep so if I

1350
00:55:53,930 --> 00:55:55,339
was doing the static hashing technique

1351
00:55:55,339 --> 00:55:58,099
that I showed in the last slide then I

1352
00:55:58,099 --> 00:56:01,130
add now a fourth fourth partition and I

1353
00:56:01,130 --> 00:56:03,349
got a Reno rehash and mod bye for now

1354
00:56:03,349 --> 00:56:05,089
everybody and we had to move potentially

1355
00:56:05,089 --> 00:56:07,099
move all of data around but the way

1356
00:56:07,099 --> 00:56:08,690
because system hashing works is that I

1357
00:56:08,690 --> 00:56:11,119
add my guy into the ring here and now

1358
00:56:11,119 --> 00:56:13,059
the only thing I need to transfer is

1359
00:56:13,059 --> 00:56:16,880
whatever C used to have weird emails

1360
00:56:16,880 --> 00:56:20,269
located so it's just this part here of

1361
00:56:20,269 --> 00:56:21,829
all that all the batteries that are that

1362
00:56:21,829 --> 00:56:23,059
are in this partition that would be

1363
00:56:23,059 --> 00:56:25,400
covered by this part of the Ring I send

1364
00:56:25,400 --> 00:56:28,670
them down and everybody else in my

1365
00:56:28,670 --> 00:56:32,900
cluster stays where they are at so I can

1366
00:56:32,900 --> 00:56:34,039
add new part you know getting new

1367
00:56:34,039 --> 00:56:36,680
partitions and they just update the ring

1368
00:56:36,680 --> 00:56:40,009
and add a new space in and likewise if I

1369
00:56:40,009 --> 00:56:42,710
take take a partition away then anything

1370
00:56:42,710 --> 00:56:48,079
here just goes up to where C was so it's

1371
00:56:48,079 --> 00:56:49,160
really interesting about this technique

1372
00:56:49,160 --> 00:56:50,599
as well is the way to do replication

1373
00:56:50,599 --> 00:56:52,430
okay and we'll cover more this next

1374
00:56:52,430 --> 00:56:53,809
class but let's say I want to do a

1375
00:56:53,809 --> 00:56:56,390
replication factor of 3 so for every

1376
00:56:56,390 --> 00:56:58,910
single in tape tuple I insert to my

1377
00:56:58,910 --> 00:57:01,220
database I wanted it to be I wanted to

1378
00:57:01,220 --> 00:57:02,779
be replicated on three different nodes

1379
00:57:02,779 --> 00:57:04,730
or three different partitions so that

1380
00:57:04,730 --> 00:57:07,160
way if one of them goes down I have two

1381
00:57:07,160 --> 00:57:10,190
others available for me that can serve

1382
00:57:10,190 --> 00:57:11,509
as a backup and my database doesn't go

1383
00:57:11,509 --> 00:57:15,880
down so now say I'm replicating a and

1384
00:57:15,880 --> 00:57:18,289
I'm gonna replicate it on three nodes so

1385
00:57:18,289 --> 00:57:20,299
I have on an a councils one then two and

1386
00:57:20,299 --> 00:57:22,730
three so any right to a any key that was

1387
00:57:22,730 --> 00:57:26,960
an a is also going to be on F and B so

1388
00:57:26,960 --> 00:57:28,430
now when my query shows up

1389
00:57:28,430 --> 00:57:30,019
same thing I hash to this point in the

1390
00:57:30,019 --> 00:57:30,300
rain

1391
00:57:30,300 --> 00:57:34,200
and I can get it from either a F or B

1392
00:57:34,200 --> 00:57:39,450
and I'm there it's guaranteed to be

1393
00:57:39,450 --> 00:57:41,070
there assume you're doing transactions

1394
00:57:41,070 --> 00:57:43,380
we'll talk about next class so this now

1395
00:57:43,380 --> 00:57:44,730
actually gets into the consistency issue

1396
00:57:44,730 --> 00:57:46,920
that we that we sort of glossed over and

1397
00:57:46,920 --> 00:57:47,970
we talked about transactions before and

1398
00:57:47,970 --> 00:57:50,430
talked about acid right if I do a write

1399
00:57:50,430 --> 00:57:52,830
on a how do I know that it's been

1400
00:57:52,830 --> 00:57:56,880
propagated to F and F F F and B well I

1401
00:57:56,880 --> 00:57:58,200
you have to wait until they all

1402
00:57:58,200 --> 00:57:59,510
acknowledge that they got the right

1403
00:57:59,510 --> 00:58:01,620
which could be bad cuz one of these guys

1404
00:58:01,620 --> 00:58:03,720
could go down one I'm waiting for the

1405
00:58:03,720 --> 00:58:06,030
acknowledgement and I'm stalling or I

1406
00:58:06,030 --> 00:58:08,250
say I don't wait but now I have this

1407
00:58:08,250 --> 00:58:11,070
issue where I I may do a write on a and

1408
00:58:11,070 --> 00:58:12,750
then merely try to read that thing on B

1409
00:58:12,750 --> 00:58:16,070
and I might not see what I expect to see

1410
00:58:16,070 --> 00:58:18,180
again so this will we'll cover this more

1411
00:58:18,180 --> 00:58:19,260
in next class but this is the

1412
00:58:19,260 --> 00:58:21,780
consistency this is the C and acid that

1413
00:58:21,780 --> 00:58:23,430
I said we were gonna gloss over for

1414
00:58:23,430 --> 00:58:25,140
single node databases but matters in

1415
00:58:25,140 --> 00:58:27,720
distributed databases so consistent

1416
00:58:27,720 --> 00:58:29,970
hashing is a really cool technique and

1417
00:58:29,970 --> 00:58:31,170
it's actually used in some distribute

1418
00:58:31,170 --> 00:58:33,420
databases so the three most famous ones

1419
00:58:33,420 --> 00:58:37,170
are memcache D which is that caching

1420
00:58:37,170 --> 00:58:37,790
service

1421
00:58:37,790 --> 00:58:41,790
Cassandra and dynamodb like Dino DD I

1422
00:58:41,790 --> 00:58:42,870
think was that had the first paper

1423
00:58:42,870 --> 00:58:45,030
discussed an architecture using this and

1424
00:58:45,030 --> 00:58:48,120
then at Facebook the one of the

1425
00:58:48,120 --> 00:58:50,130
cofounders of cloud era he saw the

1426
00:58:50,130 --> 00:58:51,750
DynamoDB Bieber hope that was a good

1427
00:58:51,750 --> 00:58:53,010
idea started but in Cassandra at

1428
00:58:53,010 --> 00:58:53,580
Facebook

1429
00:58:53,580 --> 00:58:55,200
facebook says I don't we actually don't

1430
00:58:55,200 --> 00:58:56,640
need this anymore and they decided not

1431
00:58:56,640 --> 00:58:58,500
to use Cassandra so then they just open

1432
00:58:58,500 --> 00:59:01,080
sourced it and put it out there and then

1433
00:59:01,080 --> 00:59:02,250
that people picked it up and started

1434
00:59:02,250 --> 00:59:04,260
started making Cassandra actually be you

1435
00:59:04,260 --> 00:59:06,840
know a quality system so these probably

1436
00:59:06,840 --> 00:59:08,100
three most most famous systems that use

1437
00:59:08,100 --> 00:59:11,120
this consistent hashing technique

1438
00:59:11,120 --> 00:59:16,740
alright so the new ones have briefly

1439
00:59:16,740 --> 00:59:17,610
about what the distinction is between

1440
00:59:17,610 --> 00:59:18,960
logical partitioning and physical

1441
00:59:18,960 --> 00:59:20,610
partitioning so again this the idea is

1442
00:59:20,610 --> 00:59:22,770
the same that you have this hash

1443
00:59:22,770 --> 00:59:24,540
function or range range function that

1444
00:59:24,540 --> 00:59:25,710
allows you to divide up the data base

1445
00:59:25,710 --> 00:59:27,050
and two disjoint subsets

1446
00:59:27,050 --> 00:59:29,700
but under the shared gist system you

1447
00:59:29,700 --> 00:59:31,620
have to do logical partitioning because

1448
00:59:31,620 --> 00:59:34,800
you don't have control over how the data

1449
00:59:34,800 --> 00:59:36,420
is actually being written to the shared

1450
00:59:36,420 --> 00:59:38,340
disk thing right Amazon controls this

1451
00:59:38,340 --> 00:59:39,590
you don't

1452
00:59:39,590 --> 00:59:42,120
so the basic ways where it works is that

1453
00:59:42,120 --> 00:59:43,730
you

1454
00:59:43,730 --> 00:59:46,310
have a son you sign some portion of the

1455
00:59:46,310 --> 00:59:48,530
database to these different compute

1456
00:59:48,530 --> 00:59:50,359
nodes so that again the application

1457
00:59:50,359 --> 00:59:52,040
server knows that if I want X you to

1458
00:59:52,040 --> 00:59:54,320
query here's here's the machine to go

1459
00:59:54,320 --> 00:59:56,770
get the you know go to run it right

1460
00:59:56,770 --> 00:59:58,820
likewise from down here he's responsible

1461
00:59:58,820 --> 01:00:02,150
for forth for three shared-nothing

1462
01:00:02,150 --> 01:00:03,980
systems are running when you do physical

1463
01:00:03,980 --> 01:00:05,780
partitioning again this is where you

1464
01:00:05,780 --> 01:00:08,300
have the the each each node is assigned

1465
01:00:08,300 --> 01:00:11,930
the the the partition a portion of the

1466
01:00:11,930 --> 01:00:13,700
data that's managed by partition so

1467
01:00:13,700 --> 01:00:15,140
again same thing I know how to get the

1468
01:00:15,140 --> 01:00:17,359
data that I'm looking for from these

1469
01:00:17,359 --> 01:00:21,740
different nodes alright so we have like

1470
01:00:21,740 --> 01:00:23,000
ten minutes left so let's finish up and

1471
01:00:23,000 --> 01:00:24,859
then that'll set us up for from

1472
01:00:24,859 --> 01:00:28,070
Wednesday's class so when we want to

1473
01:00:28,070 --> 01:00:29,599
start actually transactions this is when

1474
01:00:29,599 --> 01:00:32,690
things get hard and this one things get

1475
01:00:32,690 --> 01:00:34,940
expensive this is why I see her question

1476
01:00:34,940 --> 01:00:37,430
is her question before was Oh doesn't

1477
01:00:37,430 --> 01:00:38,540
always make sense maybe try to scale

1478
01:00:38,540 --> 01:00:40,160
vertically why would you ever want to

1479
01:00:40,160 --> 01:00:42,650
scale horizontally there are gonna be

1480
01:00:42,650 --> 01:00:44,359
just as if there's diminishing returns

1481
01:00:44,359 --> 01:00:46,460
if you scale vertically the harbor can't

1482
01:00:46,460 --> 01:00:48,260
actually get any better cuz you just you

1483
01:00:48,260 --> 01:00:50,119
you can't buy a machine that gets in you

1484
01:00:50,119 --> 01:00:51,650
know there's immediately faster it's

1485
01:00:51,650 --> 01:00:52,970
also assumed your software can actually

1486
01:00:52,970 --> 01:00:54,109
scale and it's not gonna be plagued

1487
01:00:54,109 --> 01:00:56,900
plagued by cersei concurrency

1488
01:00:56,900 --> 01:00:59,390
bottlenecks and other things if you're

1489
01:00:59,390 --> 01:01:02,180
not scared hosana lee then you're also

1490
01:01:02,180 --> 01:01:03,380
gonna have diminishing returns and

1491
01:01:03,380 --> 01:01:05,480
performance gains because now you're

1492
01:01:05,480 --> 01:01:06,230
gonna end up with what are called

1493
01:01:06,230 --> 01:01:09,500
distributions actions so if I have

1494
01:01:09,500 --> 01:01:11,240
something that has to update data on a

1495
01:01:11,240 --> 01:01:13,280
single node we know how to do that we've

1496
01:01:13,280 --> 01:01:14,569
card an entire semester about this and

1497
01:01:14,569 --> 01:01:15,530
that's gonna be the fast

1498
01:01:15,530 --> 01:01:17,329
the best-case scenario where my

1499
01:01:17,329 --> 01:01:19,790
transaction that I needed mean to touch

1500
01:01:19,790 --> 01:01:21,470
data it's all in a single node I can run

1501
01:01:21,470 --> 01:01:22,490
that with ever without having to

1502
01:01:22,490 --> 01:01:24,770
correlate with anybody else if I need to

1503
01:01:24,770 --> 01:01:27,349
have state across altima nodes then now

1504
01:01:27,349 --> 01:01:29,180
I need a way to make sure that if I make

1505
01:01:29,180 --> 01:01:30,829
a right here and I make a right here

1506
01:01:30,829 --> 01:01:33,980
when my transaction says commit that it

1507
01:01:33,980 --> 01:01:35,480
actually does commit because I only make

1508
01:01:35,480 --> 01:01:36,560
sure that all might change their atomic

1509
01:01:36,560 --> 01:01:38,599
and durable just as just as I wasn't a

1510
01:01:38,599 --> 01:01:40,760
single node system and that's gonna get

1511
01:01:40,760 --> 01:01:42,050
expensive because how do I make sure

1512
01:01:42,050 --> 01:01:45,200
that if I say I commit then everyone

1513
01:01:45,200 --> 01:01:47,980
actually truly commits

1514
01:01:49,110 --> 01:01:52,340
so the way we can do this is through a

1515
01:01:52,340 --> 01:01:55,440
sewer transaction coordinator so you

1516
01:01:55,440 --> 01:01:56,880
sort of think of this is like a traffic

1517
01:01:56,880 --> 01:01:59,550
cop for the entire system it allows a

1518
01:01:59,550 --> 01:02:01,410
way to determine who's allowed to do

1519
01:02:01,410 --> 01:02:04,680
what and when when it goes time to

1520
01:02:04,680 --> 01:02:06,810
commit that everyone agrees that we're

1521
01:02:06,810 --> 01:02:09,870
actually going to go ahead and commit so

1522
01:02:09,870 --> 01:02:10,920
the two different approaches are do

1523
01:02:10,920 --> 01:02:12,570
centralized decentralized a centralized

1524
01:02:12,570 --> 01:02:13,950
one is where everyone goes to some

1525
01:02:13,950 --> 01:02:15,990
centralized location that has a complete

1526
01:02:15,990 --> 01:02:18,060
view of everything going on inside us

1527
01:02:18,060 --> 01:02:19,980
inside the system and then it makes

1528
01:02:19,980 --> 01:02:21,060
decisions about whether you're allowed

1529
01:02:21,060 --> 01:02:23,310
to commit and it is decentralized

1530
01:02:23,310 --> 01:02:24,690
approach where the nodes try to organize

1531
01:02:24,690 --> 01:02:26,610
themselves and make a decision about yes

1532
01:02:26,610 --> 01:02:28,260
we this this transaction made these

1533
01:02:28,260 --> 01:02:30,090
changes and we're allowed to commit and

1534
01:02:30,090 --> 01:02:32,070
we can notify whoever else is involved

1535
01:02:32,070 --> 01:02:33,990
in the transaction that they've

1536
01:02:33,990 --> 01:02:38,010
committed successfully so the very first

1537
01:02:38,010 --> 01:02:39,390
version of one of these transact our

1538
01:02:39,390 --> 01:02:40,680
transaction coordinators was this thing

1539
01:02:40,680 --> 01:02:42,600
called a TP monitor from the 1970s in

1540
01:02:42,600 --> 01:02:43,200
1980s

1541
01:02:43,200 --> 01:02:44,970
nowadays I think if you look at the

1542
01:02:44,970 --> 01:02:47,010
Wikipedia article TP stands for a

1543
01:02:47,010 --> 01:02:49,590
transaction processing monitor back in

1544
01:02:49,590 --> 01:02:51,590
the 70s they call these things telecom

1545
01:02:51,590 --> 01:02:53,820
processing monitors because these things

1546
01:02:53,820 --> 01:02:55,350
are built for like the early you know

1547
01:02:55,350 --> 01:02:57,090
the phone companies that come back in

1548
01:02:57,090 --> 01:02:58,350
the day because they were the ones that

1549
01:02:58,350 --> 01:02:59,790
had most of the traffic I mean you know

1550
01:02:59,790 --> 01:03:04,920
most of the data so the way to think

1551
01:03:04,920 --> 01:03:06,900
about this this TP monitor is that it's

1552
01:03:06,900 --> 01:03:09,440
the standalone piece of software that

1553
01:03:09,440 --> 01:03:12,120
everybody has to talk to in order to

1554
01:03:12,120 --> 01:03:13,170
figure out whether they're a lotta doo

1555
01:03:13,170 --> 01:03:15,900
doo do certain operations on a

1556
01:03:15,900 --> 01:03:18,120
distributed database so the database

1557
01:03:18,120 --> 01:03:20,100
system itself could be stored across

1558
01:03:20,100 --> 01:03:21,690
different nodes and they don't really

1559
01:03:21,690 --> 01:03:23,340
know that they're actually involved in

1560
01:03:23,340 --> 01:03:24,600
the distributed transaction or

1561
01:03:24,600 --> 01:03:26,070
distributed database if you just take my

1562
01:03:26,070 --> 01:03:27,810
sequel whatever single node system you

1563
01:03:27,810 --> 01:03:30,270
want run that separately and then up

1564
01:03:30,270 --> 01:03:31,710
above you have this to be monitored

1565
01:03:31,710 --> 01:03:32,820
allow you to figure out whether you're

1566
01:03:32,820 --> 01:03:35,670
allowed to do certain things so it looks

1567
01:03:35,670 --> 01:03:36,960
like this right so we have application

1568
01:03:36,960 --> 01:03:40,410
server we have four partitions so save

1569
01:03:40,410 --> 01:03:42,210
your transaction with such these these

1570
01:03:42,210 --> 01:03:44,400
three partitions so we're gonna begin

1571
01:03:44,400 --> 01:03:45,990
our transaction by going to coordinator

1572
01:03:45,990 --> 01:03:48,810
and say hey we want we want to modify

1573
01:03:48,810 --> 01:03:50,880
some data at these partitions we need to

1574
01:03:50,880 --> 01:03:52,680
acquire the locks for them are we

1575
01:03:52,680 --> 01:03:55,050
allowed to do that and then the

1576
01:03:55,050 --> 01:03:56,250
coordinator says well I know it's where

1577
01:03:56,250 --> 01:03:57,210
else is running the system because

1578
01:03:57,210 --> 01:03:59,550
everyone has to go through me yes well I

1579
01:03:59,550 --> 01:04:00,870
see these locks are available so I'm

1580
01:04:00,870 --> 01:04:02,550
gonna sign them to you

1581
01:04:02,550 --> 01:04:04,260
then tell you that you you know you've

1582
01:04:04,260 --> 01:04:05,760
acquired them and then now the

1583
01:04:05,760 --> 01:04:07,890
application server can go to the

1584
01:04:07,890 --> 01:04:09,360
different partitions do whatever it is

1585
01:04:09,360 --> 01:04:11,580
that wants to do to make the changes and

1586
01:04:11,580 --> 01:04:14,580
wants to make and then when it wants to

1587
01:04:14,580 --> 01:04:16,200
go to ahead and commit it goes to the

1588
01:04:16,200 --> 01:04:17,760
coordinator and says hey I want to

1589
01:04:17,760 --> 01:04:19,380
commit I made these changes these

1590
01:04:19,380 --> 01:04:21,360
partitions am I allowed to do this and

1591
01:04:21,360 --> 01:04:23,280
the coordinator is responsible for going

1592
01:04:23,280 --> 01:04:24,750
and communicating with these guys down

1593
01:04:24,750 --> 01:04:27,270
here and say hey I think you know about

1594
01:04:27,270 --> 01:04:28,920
this transaction because it told me it

1595
01:04:28,920 --> 01:04:30,690
was gonna touch you did it actually do

1596
01:04:30,690 --> 01:04:32,610
anything and then they come back and say

1597
01:04:32,610 --> 01:04:34,950
yes you know these changes were what

1598
01:04:34,950 --> 01:04:37,080
happened and they're okay or safe to

1599
01:04:37,080 --> 01:04:39,780
commit and then then once once we

1600
01:04:39,780 --> 01:04:41,610
everybody agrees once the cornea

1601
01:04:41,610 --> 01:04:42,840
recognized that everyone agrees that we

1602
01:04:42,840 --> 01:04:44,370
can go and commit we can stand back our

1603
01:04:44,370 --> 01:04:49,860
knowledge meant question questions I

1604
01:04:49,860 --> 01:04:51,420
know what scenario would it be not safe

1605
01:04:51,420 --> 01:04:56,460
to commit so let's say I violate a you

1606
01:04:56,460 --> 01:04:58,080
know integrity constraint here my

1607
01:04:58,080 --> 01:05:00,390
transaction aborts right I try to insert

1608
01:05:00,390 --> 01:05:02,880
a duplicate key the coiner doesn't know

1609
01:05:02,880 --> 01:05:04,500
what you did it says hey I want to pry

1610
01:05:04,500 --> 01:05:06,270
the locks on these things and I want to

1611
01:05:06,270 --> 01:05:07,470
I want to commit in a distributed

1612
01:05:07,470 --> 01:05:10,170
fashion you have to go ask them whether

1613
01:05:10,170 --> 01:05:11,250
that's a lot like they were allowed to

1614
01:05:11,250 --> 01:05:15,870
do that for simplicity his question are

1615
01:05:15,870 --> 01:05:17,460
we lucky the whole partition simplicity

1616
01:05:17,460 --> 01:05:21,780
yes right there's a I think it's like

1617
01:05:21,780 --> 01:05:23,010
the XA if there's a protocol that allows

1618
01:05:23,010 --> 01:05:24,960
you do more fine-grained locking just

1619
01:05:24,960 --> 01:05:28,970
stupid partitions makes it simple okay

1620
01:05:31,680 --> 01:05:38,220
so again there's a bunch of a lot of the

1621
01:05:38,220 --> 01:05:40,470
enterprise software vendors sell you

1622
01:05:40,470 --> 01:05:42,450
something that that is a TV monitor

1623
01:05:42,450 --> 01:05:45,150
Oracle has this thing called tuxedo IBM

1624
01:05:45,150 --> 01:05:46,650
sells this thing called tranzec

1625
01:05:46,650 --> 01:05:48,390
which actually was a senior startup like

1626
01:05:48,390 --> 01:05:50,370
the guy that did the af-s stuff in the

1627
01:05:50,370 --> 01:05:52,230
80s they did a startup called tranzec I

1628
01:05:52,230 --> 01:05:54,170
got bought by IBM and IBM still sells us

1629
01:05:54,170 --> 01:05:56,160
there's a project you can't really read

1630
01:05:56,160 --> 01:05:58,320
the logo it's called Apache Omid it was

1631
01:05:58,320 --> 01:06:01,170
built by Yahoo it's a it's basically a

1632
01:06:01,170 --> 01:06:04,050
teepee monitor for HBase anos ecosystem

1633
01:06:04,050 --> 01:06:05,460
that's actually used by a couple other

1634
01:06:05,460 --> 01:06:08,070
systems today so you can build a tribute

1635
01:06:08,070 --> 01:06:09,570
database without worrying about

1636
01:06:09,570 --> 01:06:11,310
transactions could you just rely on

1637
01:06:11,310 --> 01:06:12,900
these guys to figure things out for you

1638
01:06:12,900 --> 01:06:14,970
and you just do all the single node

1639
01:06:14,970 --> 01:06:17,990
stuff that you'd bet you're normally

1640
01:06:18,680 --> 01:06:21,150
but pi more common is to use a

1641
01:06:21,150 --> 01:06:22,650
centralized coordinator as a middleware

1642
01:06:22,650 --> 01:06:24,749
where you had this piece of software

1643
01:06:24,749 --> 01:06:26,489
that sits between the application server

1644
01:06:26,489 --> 01:06:29,009
and the database partitions all queries

1645
01:06:29,009 --> 01:06:31,709
go through this middleware and the

1646
01:06:31,709 --> 01:06:33,180
middleware is responsible figuring out

1647
01:06:33,180 --> 01:06:34,890
oh this data wants this query want to

1648
01:06:34,890 --> 01:06:37,289
touch this data this partition so it

1649
01:06:37,289 --> 01:06:39,150
looks at its it's it's you know it's its

1650
01:06:39,150 --> 01:06:40,890
global lock table or information about

1651
01:06:40,890 --> 01:06:43,170
partitions are there and it routes the

1652
01:06:43,170 --> 01:06:46,589
queries as needed for you so you look

1653
01:06:46,589 --> 01:06:48,959
like you're talking to a single single

1654
01:06:48,959 --> 01:06:50,249
node database system through the

1655
01:06:50,249 --> 01:06:52,200
middleware but in the backend its

1656
01:06:52,200 --> 01:06:53,579
distributed and broken across these

1657
01:06:53,579 --> 01:06:56,609
different partitions so when the commit

1658
01:06:56,609 --> 01:06:57,779
request shows up by the application

1659
01:06:57,779 --> 01:06:59,640
server de moda where does the same thing

1660
01:06:59,640 --> 01:07:01,319
as the TP monitor does it communicate to

1661
01:07:01,319 --> 01:07:02,789
these guys and say hey are we allowed to

1662
01:07:02,789 --> 01:07:06,450
commit and only when everyone agrees do

1663
01:07:06,450 --> 01:07:09,779
you then send back the acknowledgment so

1664
01:07:09,779 --> 01:07:11,069
this one this this approach is actually

1665
01:07:11,069 --> 01:07:13,200
very very common like Facebook is

1666
01:07:13,200 --> 01:07:14,549
probably most famous one Facebook runs

1667
01:07:14,549 --> 01:07:16,759
the world's largest my sequel cluster

1668
01:07:16,759 --> 01:07:19,289
and they have a middleware assistant to

1669
01:07:19,289 --> 01:07:21,599
do all this routing for you Google used

1670
01:07:21,599 --> 01:07:24,019
to do this for for my sequel in the ads

1671
01:07:24,019 --> 01:07:26,160
there's a planet-scale that came out of

1672
01:07:26,160 --> 01:07:27,779
YouTube but this approach is actually

1673
01:07:27,779 --> 01:07:29,459
very very common you take us you know

1674
01:07:29,459 --> 01:07:31,619
Postgres my sequel whatever you know

1675
01:07:31,619 --> 01:07:32,819
your favorite single node database

1676
01:07:32,819 --> 01:07:34,619
system is and you build this little

1677
01:07:34,619 --> 01:07:36,319
wrapper layer in front of it

1678
01:07:36,319 --> 01:07:40,499
eBay did this with Oracle it's very

1679
01:07:40,499 --> 01:07:44,729
common the other approach is the last

1680
01:07:44,729 --> 01:07:46,009
versions to do centralized coordination

1681
01:07:46,009 --> 01:07:48,719
where you don't have a coordinator you

1682
01:07:48,719 --> 01:07:50,609
don't have a centralized view of what's

1683
01:07:50,609 --> 01:07:52,769
going on in the system the application

1684
01:07:52,769 --> 01:07:55,799
never communicates with some home

1685
01:07:55,799 --> 01:07:57,930
partition or base partition some some

1686
01:07:57,930 --> 01:07:59,609
master node that's been responsible for

1687
01:07:59,609 --> 01:08:01,619
this given transaction all their notes

1688
01:08:01,619 --> 01:08:03,089
will be master nodes if you're assume

1689
01:08:03,089 --> 01:08:05,759
you're a homogeneous architecture so you

1690
01:08:05,759 --> 01:08:07,499
send all the query requests either to

1691
01:08:07,499 --> 01:08:09,329
directly to the master node or to

1692
01:08:09,329 --> 01:08:10,890
individual partitions it doesn't matter

1693
01:08:10,890 --> 01:08:13,559
but it's when you want to go commit you

1694
01:08:13,559 --> 01:08:15,690
go to the master node and say hey I made

1695
01:08:15,690 --> 01:08:17,729
these changes I want to go ahead of

1696
01:08:17,729 --> 01:08:19,738
commitment transaction and then it's

1697
01:08:19,738 --> 01:08:21,060
responsible for communicating with the

1698
01:08:21,060 --> 01:08:22,290
other partitions and deciding whether

1699
01:08:22,290 --> 01:08:24,000
you're allowed to commit and if yes then

1700
01:08:24,000 --> 01:08:27,049
you send back the acknowledgment

1701
01:08:27,819 --> 01:08:30,549
all right so the thing that I call

1702
01:08:30,549 --> 01:08:32,799
Stover is that part of a how do we

1703
01:08:32,799 --> 01:08:34,229
figure out whether it's safe to commit

1704
01:08:34,229 --> 01:08:40,540
question this question is how do you

1705
01:08:40,540 --> 01:08:45,640
take the locks so it would be say again

1706
01:08:45,640 --> 01:08:47,140
assume I'm doing a lot project locking

1707
01:08:47,140 --> 01:08:49,120
the whole partition so when the query

1708
01:08:49,120 --> 01:08:52,569
shows up right you you you try to

1709
01:08:52,569 --> 01:08:57,279
acquire the lock at that point so the

1710
01:08:57,279 --> 01:09:00,490
masternode would only know information

1711
01:09:00,490 --> 01:09:01,930
and potentially about what partitions

1712
01:09:01,930 --> 01:09:03,640
you touched doesn't know what you did at

1713
01:09:03,640 --> 01:09:06,520
them right and it's respond to the

1714
01:09:06,520 --> 01:09:07,630
application responsible are saying hey I

1715
01:09:07,630 --> 01:09:08,859
couldn't get the lock of this partition

1716
01:09:08,859 --> 01:09:10,299
I have to abort my transaction so you go

1717
01:09:10,299 --> 01:09:12,149
back to this guy hey say I aborted

1718
01:09:12,149 --> 01:09:14,020
alternatively you just send all the

1719
01:09:14,020 --> 01:09:15,100
requests that this guy and he's

1720
01:09:15,100 --> 01:09:16,510
responsible for farming out to the

1721
01:09:16,510 --> 01:09:21,700
different machines at the masternode if

1722
01:09:21,700 --> 01:09:23,350
you touch the data at the masternode

1723
01:09:23,350 --> 01:09:29,890
sure yes okay

1724
01:09:29,890 --> 01:09:34,450
so we'll cover this in more detail next

1725
01:09:34,450 --> 01:09:37,990
class it's an impress upon you and then

1726
01:09:37,990 --> 01:09:39,430
you'll think about it and see on

1727
01:09:39,430 --> 01:09:40,750
Wednesday why you know how hard it

1728
01:09:40,750 --> 01:09:43,180
actually is say we're doing two-phase

1729
01:09:43,180 --> 01:09:45,189
locking my last example and say that my

1730
01:09:45,189 --> 01:09:48,250
nose or over the white or never one note

1731
01:09:48,250 --> 01:09:49,390
is in Pittsburgh one node is in San

1732
01:09:49,390 --> 01:09:49,990
Francisco

1733
01:09:49,990 --> 01:09:51,970
so at the same time I have two

1734
01:09:51,970 --> 01:09:53,560
applications trying to update the

1735
01:09:53,560 --> 01:09:56,290
database right and the very beginning I

1736
01:09:56,290 --> 01:09:58,780
get a lock on my node here for a this

1737
01:09:58,780 --> 01:10:01,270
guy gets the lock on B but now I want to

1738
01:10:01,270 --> 01:10:04,450
update this guy wants to update B the

1739
01:10:04,450 --> 01:10:06,310
other guy wants to update a so now I got

1740
01:10:06,310 --> 01:10:08,050
to go over the network and send a lock

1741
01:10:08,050 --> 01:10:10,480
request to get the other lock on the

1742
01:10:10,480 --> 01:10:12,070
other thing the other guys doing the

1743
01:10:12,070 --> 01:10:12,670
same thing

1744
01:10:12,670 --> 01:10:13,930
I'm obviously ending up with the

1745
01:10:13,930 --> 01:10:16,360
deadlock here so how do I actually

1746
01:10:16,360 --> 01:10:18,460
figure out who's actually should should

1747
01:10:18,460 --> 01:10:21,130
be allowed to commit this again if I'm

1748
01:10:21,130 --> 01:10:22,660
doing a decentralized architecture if I

1749
01:10:22,660 --> 01:10:24,970
don't have that TP monitor but even if I

1750
01:10:24,970 --> 01:10:27,190
do I men have not the fine-grained

1751
01:10:27,190 --> 01:10:29,410
information about what exactly it's

1752
01:10:29,410 --> 01:10:30,550
doing on each node because you can't

1753
01:10:30,550 --> 01:10:31,840
always know what the queries gonna do

1754
01:10:31,840 --> 01:10:34,420
before you actually run it

1755
01:10:34,420 --> 01:10:36,550
someone needs to figure out I have this

1756
01:10:36,550 --> 01:10:37,900
weight to a graph of a cycle I need to

1757
01:10:37,900 --> 01:10:40,230
kill somebody

1758
01:10:40,500 --> 01:10:42,000
and then so let's say this guy says oh

1759
01:10:42,000 --> 01:10:44,340
I'm gonna back off I have a deadlock if

1760
01:10:44,340 --> 01:10:45,540
I'm doing dog prevention I kill myself

1761
01:10:45,540 --> 01:10:48,710
this guy could be doing the same thing

1762
01:10:48,710 --> 01:10:50,550
so this is what we're gonna talk about

1763
01:10:50,550 --> 01:10:52,680
on on Wednesday how do you actually do

1764
01:10:52,680 --> 01:10:54,690
to ship it control how do you how do you

1765
01:10:54,690 --> 01:10:56,910
figure out you take two phase locking

1766
01:10:56,910 --> 01:10:59,340
timestamp ordering and run it in

1767
01:10:59,340 --> 01:11:00,330
attribute environment where you don't

1768
01:11:00,330 --> 01:11:01,620
have a complete global view of

1769
01:11:01,620 --> 01:11:03,360
everything's going on side the submit a

1770
01:11:03,360 --> 01:11:05,610
and a given time we're also gonna spend

1771
01:11:05,610 --> 01:11:08,010
time on when my transaction says go

1772
01:11:08,010 --> 01:11:10,080
ahead and commit how do I guarantee that

1773
01:11:10,080 --> 01:11:14,010
I that I commit everywhere because what

1774
01:11:14,010 --> 01:11:15,120
happens if a node goes down while I'm

1775
01:11:15,120 --> 01:11:16,550
trying to commit what should I do

1776
01:11:16,550 --> 01:11:19,460
that's actually super hard to get right

1777
01:11:19,460 --> 01:11:21,990
so if you're injured in these kind of

1778
01:11:21,990 --> 01:11:25,260
things there's this great is this great

1779
01:11:25,260 --> 01:11:27,300
website called the Jepsen project by

1780
01:11:27,300 --> 01:11:29,930
this guy called Kyle Kingsbury so he was

1781
01:11:29,930 --> 01:11:32,970
he's basically he he built this torture

1782
01:11:32,970 --> 01:11:34,830
chamber for distributing databases

1783
01:11:34,830 --> 01:11:36,840
Britain enclosure which is a bit gnarly

1784
01:11:36,840 --> 01:11:39,540
but he basically has this test suite

1785
01:11:39,540 --> 01:11:40,440
where he can take your distributing

1786
01:11:40,440 --> 01:11:42,690
database run through these weird edge

1787
01:11:42,690 --> 01:11:45,540
cases and identify that it's not always

1788
01:11:45,540 --> 01:11:47,430
correct and has and has has problems on

1789
01:11:47,430 --> 01:11:50,070
a guaranteeing reliability availability

1790
01:11:50,070 --> 01:11:52,710
or correctness of transactions so right

1791
01:11:52,710 --> 01:11:54,300
now he he has a consult company people

1792
01:11:54,300 --> 01:11:56,040
pay him money to go actually run this t

1793
01:11:56,040 --> 01:11:57,300
v-- build his website he has these

1794
01:11:57,300 --> 01:12:00,630
write-ups which are super super detailed

1795
01:12:00,630 --> 01:12:02,310
and take a long time to read to

1796
01:12:02,310 --> 01:12:03,330
understand what he's actually talking

1797
01:12:03,330 --> 01:12:04,980
about but he talks about how these

1798
01:12:04,980 --> 01:12:06,180
different data systems he's tried this

1799
01:12:06,180 --> 01:12:08,250
against they claim that there may be

1800
01:12:08,250 --> 01:12:10,260
transactions correctly they claim that

1801
01:12:10,260 --> 01:12:11,340
they can always support high

1802
01:12:11,340 --> 01:12:13,740
availability or good performance and his

1803
01:12:13,740 --> 01:12:16,260
thing shows that that they don't so they

1804
01:12:16,260 --> 01:12:17,940
paying him money to go run his thing on

1805
01:12:17,940 --> 01:12:19,350
their database system and then if they

1806
01:12:19,350 --> 01:12:20,340
pass they can announce that they're

1807
01:12:20,340 --> 01:12:22,800
certified there's one day to his company

1808
01:12:22,800 --> 01:12:24,390
was aerospike which is a distributed

1809
01:12:24,390 --> 01:12:26,160
key-value store they used to claim on

1810
01:12:26,160 --> 01:12:27,840
their website they had you know they had

1811
01:12:27,840 --> 01:12:30,180
strong consistency guarantees he ran his

1812
01:12:30,180 --> 01:12:31,650
thing against theirs crushed it showed

1813
01:12:31,650 --> 01:12:32,790
how it wasn't and they had to go back

1814
01:12:32,790 --> 01:12:34,320
and change all the marketing crap to

1815
01:12:34,320 --> 01:12:37,470
remove it because t humiliated it says

1816
01:12:37,470 --> 01:12:39,690
websites awesome his Twitter feed not so

1817
01:12:39,690 --> 01:12:42,120
much you'll see why to go look at it uh

1818
01:12:42,120 --> 01:12:45,480
it's not my thing but he's a he's a

1819
01:12:45,480 --> 01:12:46,920
really sharp dude I think there's a

1820
01:12:46,920 --> 01:12:49,260
really good website alright next class

1821
01:12:49,260 --> 01:12:51,000
just Roberto to me systems replication

1822
01:12:51,000 --> 01:12:53,160
cat theorem and then real-world examples

1823
01:12:53,160 --> 01:12:54,379
again we'll go through

1824
01:12:54,379 --> 01:12:56,239
start worrying about how we're actually

1825
01:12:56,239 --> 01:12:57,469
gonna run transactions in a t-shirt

1826
01:12:57,469 --> 01:12:58,579
environment will tell them about no

1827
01:12:58,579 --> 01:12:59,929
sequel systems and see why they don't

1828
01:12:59,929 --> 01:13:02,119
want to do transactions because it's

1829
01:13:02,119 --> 01:13:03,349
gonna affect performance and

1830
01:13:03,349 --> 01:13:06,530
availability okay all right guys awesome

1831
01:13:06,530 --> 01:13:30,739
see you on Wednesday ricochet jelly hit

1832
01:13:30,739 --> 01:13:42,760
the deli for the one yeah don't drink it

1833
01:13:42,969 --> 01:13:47,649
the sake don't know your phone can't pay

