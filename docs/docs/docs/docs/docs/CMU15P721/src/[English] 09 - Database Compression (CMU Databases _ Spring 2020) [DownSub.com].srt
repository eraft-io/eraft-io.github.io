1
00:00:01,300 --> 00:00:05,129
[Music]

2
00:00:05,200 --> 00:00:05,930
[Applause]

3
00:00:05,930 --> 00:00:08,630
[Music]

4
00:00:08,630 --> 00:00:10,020
[Applause]

5
00:00:10,020 --> 00:00:11,960
[Music]

6
00:00:11,960 --> 00:00:15,240
so today there is actually one of my

7
00:00:15,240 --> 00:00:16,650
favorite lectures we're talk about Davis

8
00:00:16,650 --> 00:00:19,410
compression again the high-level idea

9
00:00:19,410 --> 00:00:20,910
here is we're just gonna be able to

10
00:00:20,910 --> 00:00:23,580
squeeze down the size of our data in our

11
00:00:23,580 --> 00:00:26,460
database and you know save memory save

12
00:00:26,460 --> 00:00:28,289
storage but in some cases actually we're

13
00:00:28,289 --> 00:00:29,400
also gonna be able to execute queries

14
00:00:29,400 --> 00:00:31,890
faster because of the way we're going to

15
00:00:31,890 --> 00:00:33,510
compress things we can we can be aware

16
00:00:33,510 --> 00:00:35,640
of how things are laid out and do things

17
00:00:35,640 --> 00:00:38,160
more efficiently quickly Before we jump

18
00:00:38,160 --> 00:00:40,320
into that just reminder that there's a

19
00:00:40,320 --> 00:00:44,160
tech talk today at 4:30 over in here in

20
00:00:44,160 --> 00:00:47,460
Neil Simon from Oracle the one of the

21
00:00:47,460 --> 00:00:48,379
guys that works on the in-memory

22
00:00:48,379 --> 00:00:50,910
database team so he answered in that

23
00:00:50,910 --> 00:00:52,949
come check it out the other thing I want

24
00:00:52,949 --> 00:00:55,530
talk about too is last class we were

25
00:00:55,530 --> 00:00:59,789
looking at mem sequel and we had this

26
00:00:59,789 --> 00:01:01,920
bit type and we were like oh how can a

27
00:01:01,920 --> 00:01:06,840
bit type be a bytes and I I read the

28
00:01:06,840 --> 00:01:08,460
documentation for in the sequel standard

29
00:01:08,460 --> 00:01:11,070
and and in other systems it's actually a

30
00:01:11,070 --> 00:01:13,380
bit string not a single bit so you can

31
00:01:13,380 --> 00:01:16,830
specify like how many bits you want but

32
00:01:16,830 --> 00:01:18,299
I emailed the mem sequel guy last night

33
00:01:18,299 --> 00:01:19,409
said this documentation is kind of

34
00:01:19,409 --> 00:01:21,600
confusing because like you can set an

35
00:01:21,600 --> 00:01:23,790
arbitrary size for this but it's unclear

36
00:01:23,790 --> 00:01:26,310
like why is it 8 bytes right like if I

37
00:01:26,310 --> 00:01:28,829
set it to be bit one it's just drove one

38
00:01:28,829 --> 00:01:30,390
bit and I really gonna store that to be

39
00:01:30,390 --> 00:01:34,500
8 bytes he said they're trying to it's

40
00:01:34,500 --> 00:01:36,360
some kind of compatibility of my sequel

41
00:01:36,360 --> 00:01:37,770
they're trying to follow and then

42
00:01:37,770 --> 00:01:39,270
actually in their new column store

43
00:01:39,270 --> 00:01:41,340
system that they call single store they

44
00:01:41,340 --> 00:01:42,750
can actually compress compress this

45
00:01:42,750 --> 00:01:44,790
bitmap I don't know exactly how they do

46
00:01:44,790 --> 00:01:46,320
that but we'll see how to do bitmap

47
00:01:46,320 --> 00:01:48,600
compression in this clip in this lecture

48
00:01:48,600 --> 00:01:50,340
earlier just again just to show you what

49
00:01:50,340 --> 00:01:51,990
this looks like so this is actually

50
00:01:51,990 --> 00:01:55,860
Postgres so I can declare a table that

51
00:01:55,860 --> 00:02:00,060
has a bit type right and you can specify

52
00:02:00,060 --> 00:02:01,799
the number of bits you want and then

53
00:02:01,799 --> 00:02:03,149
this one here you can specify that it's

54
00:02:03,149 --> 00:02:05,520
it's a verbal number of bits so - I

55
00:02:05,520 --> 00:02:08,128
don't like this little varying thing

56
00:02:08,128 --> 00:02:10,258
this is from the post documentation so

57
00:02:10,258 --> 00:02:12,660
you guys replace this

58
00:02:12,660 --> 00:02:18,960
with with barb it like that right and

59
00:02:18,960 --> 00:02:21,450
then now if I certain to the table I can

60
00:02:21,450 --> 00:02:23,340
specify that I want a bit string like

61
00:02:23,340 --> 00:02:25,200
this right and go ahead and store that

62
00:02:25,200 --> 00:02:27,540
and then when I do a select I get back

63
00:02:27,540 --> 00:02:30,690
Fitch drinks and in the case of Postgres

64
00:02:30,690 --> 00:02:32,940
that there's two bar bit and regular bit

65
00:02:32,940 --> 00:02:35,100
is if I try to now insert something that

66
00:02:35,100 --> 00:02:36,810
in the first one because I'm saying

67
00:02:36,810 --> 00:02:38,790
there has to be three bits if I try and

68
00:02:38,790 --> 00:02:41,920
search something that's just two bits

69
00:02:41,920 --> 00:02:43,100
[Music]

70
00:02:43,100 --> 00:02:46,590
like that right it'll throw an error

71
00:02:46,590 --> 00:02:48,060
good because it says the bit string has

72
00:02:48,060 --> 00:02:52,320
to be three is that it - why do we need

73
00:02:52,320 --> 00:02:55,140
what why would you ever do want to need

74
00:02:55,140 --> 00:02:56,400
a bitmap to store natively in your

75
00:02:56,400 --> 00:03:03,990
database with a bitmap as so statements

76
00:03:03,990 --> 00:03:05,910
do you know as a bar chart that would be

77
00:03:05,910 --> 00:03:08,340
wasteful so what they really do is these

78
00:03:08,340 --> 00:03:09,810
case of Postgres I don't know exactly

79
00:03:09,810 --> 00:03:11,550
what them seagulls doing like Postgres

80
00:03:11,550 --> 00:03:13,140
will say all right you're saying this

81
00:03:13,140 --> 00:03:15,600
thing has can be two bits I can store

82
00:03:15,600 --> 00:03:17,580
that as a single byte because I just

83
00:03:17,580 --> 00:03:18,150
wrote eight bits

84
00:03:18,150 --> 00:03:30,840
yeah yes it seemed to Mac size that I

85
00:03:30,840 --> 00:03:32,520
can store is five so I try to do this

86
00:03:32,520 --> 00:03:34,890
right it won't store it

87
00:03:34,890 --> 00:03:36,120
well actually didn't like the first one

88
00:03:36,120 --> 00:03:40,080
but like if I go make this one one right

89
00:03:40,080 --> 00:03:44,160
it's too big for for the bar bit one but

90
00:03:44,160 --> 00:03:45,090
the point I'm trying to make is like for

91
00:03:45,090 --> 00:03:47,010
the bar bit one I can store this be one

92
00:03:47,010 --> 00:03:51,680
bit and it'll let me do that because

93
00:03:56,209 --> 00:03:58,410
this story that the regular pit and the

94
00:03:58,410 --> 00:04:00,830
bar bit one

95
00:04:08,290 --> 00:04:10,910
it'll store in the case of this they'll

96
00:04:10,910 --> 00:04:13,280
store the size plus the bit the

97
00:04:13,280 --> 00:04:18,290
screen itself yeah okay you don't here

98
00:04:18,290 --> 00:04:20,450
look a bit strings in the post grassman

99
00:04:20,450 --> 00:04:21,620
that's not the point of this lecture I

100
00:04:21,620 --> 00:04:23,060
just wanted to clarify because we I

101
00:04:23,060 --> 00:04:24,640
didn't know the answer last time okay

102
00:04:24,640 --> 00:04:26,630
alright so we're gonna talk about today

103
00:04:26,630 --> 00:04:29,510
is compression schemes so we'll force

104
00:04:29,510 --> 00:04:31,310
provide some background a sort of

105
00:04:31,310 --> 00:04:33,530
obvious but why ever you want to do

106
00:04:33,530 --> 00:04:35,000
compression actually it's more obvious

107
00:04:35,000 --> 00:04:36,290
maybe in a disk based system but we'll

108
00:04:36,290 --> 00:04:37,700
see why we still want to do it in memory

109
00:04:37,700 --> 00:04:39,530
system then we'll talk about how to do a

110
00:04:39,530 --> 00:04:42,200
basic naive compression scheme where the

111
00:04:42,200 --> 00:04:43,400
database system doesn't know anything

112
00:04:43,400 --> 00:04:44,720
about how they did is actually being

113
00:04:44,720 --> 00:04:47,600
compressed then we'll look at the more

114
00:04:47,600 --> 00:04:49,150
sophisticated techniques of doing

115
00:04:49,150 --> 00:04:52,490
columnar columnar storage compression

116
00:04:52,490 --> 00:04:54,440
where the database system itself it was

117
00:04:54,440 --> 00:04:55,670
responsible for running the compression

118
00:04:55,670 --> 00:04:57,860
album and then it knows that had a

119
00:04:57,860 --> 00:04:59,630
natively stored this compress data and

120
00:04:59,630 --> 00:05:00,860
they can operate queries on top of them

121
00:05:00,860 --> 00:05:02,450
and then we have time at the end we'll

122
00:05:02,450 --> 00:05:03,740
finish up talking about some research

123
00:05:03,740 --> 00:05:05,450
we've done here on compressing

124
00:05:05,450 --> 00:05:10,340
old items okay okay so we didn't talk

125
00:05:10,340 --> 00:05:12,110
about compression in the disk that the

126
00:05:12,110 --> 00:05:13,430
in the intro class when we talk about

127
00:05:13,430 --> 00:05:16,430
disk or any databases but it's sort of

128
00:05:16,430 --> 00:05:18,200
obvious that if you're assume your

129
00:05:18,200 --> 00:05:19,400
databases on disk and you're fetching a

130
00:05:19,400 --> 00:05:21,350
lot of pages from disk to a process

131
00:05:21,350 --> 00:05:23,360
queries then compression is gonna be a

132
00:05:23,360 --> 00:05:24,650
no brainer in that environment because

133
00:05:24,650 --> 00:05:27,740
the disk i/o so expensive so paying some

134
00:05:27,740 --> 00:05:30,500
additional CPU overhead to compress the

135
00:05:30,500 --> 00:05:33,110
data when it comes into memory is is

136
00:05:33,110 --> 00:05:34,960
going to be a good trade-off for us

137
00:05:34,960 --> 00:05:37,850
right because the disk isn't getting

138
00:05:37,850 --> 00:05:40,640
that much faster and something that you

139
00:05:40,640 --> 00:05:44,030
but we have more cores we can Xu there's

140
00:05:44,030 --> 00:05:45,530
more computational power available to us

141
00:05:45,530 --> 00:05:47,630
than there possibly is for us to get the

142
00:05:47,630 --> 00:05:50,390
data in off a disk but now for a memory

143
00:05:50,390 --> 00:05:52,360
database this trade-off isn't so obvious

144
00:05:52,360 --> 00:05:54,530
right because now everything is in

145
00:05:54,530 --> 00:05:58,669
memory that I'm not paying that big big

146
00:05:58,669 --> 00:06:02,090
IO penalty and I'll also allow to do

147
00:06:02,090 --> 00:06:04,669
byte level access to to jump to you know

148
00:06:04,669 --> 00:06:06,020
different offsets and my comms or my

149
00:06:06,020 --> 00:06:07,700
data and access single actually is very

150
00:06:07,700 --> 00:06:09,530
efficiently whereas in a disk based

151
00:06:09,530 --> 00:06:11,330
system I'm bringing in four kilobyte

152
00:06:11,330 --> 00:06:14,570
pages at a time so the scope of what I

153
00:06:14,570 --> 00:06:18,020
can compress is is slightly different so

154
00:06:18,020 --> 00:06:18,830
for it

155
00:06:18,830 --> 00:06:20,150
database we're gonna have make this

156
00:06:20,150 --> 00:06:21,920
trade-off between speed and compression

157
00:06:21,920 --> 00:06:24,770
ratio and to the best my knowledge in

158
00:06:24,770 --> 00:06:27,200
most cases from mme databases that are

159
00:06:27,200 --> 00:06:29,600
going to support compress data they're

160
00:06:29,600 --> 00:06:31,100
gonna choose speed over compression

161
00:06:31,100 --> 00:06:32,990
ratio that means that there's gonna be

162
00:06:32,990 --> 00:06:34,370
some compression algorithms that are

163
00:06:34,370 --> 00:06:37,340
gonna get an amazing compression ratio

164
00:06:37,340 --> 00:06:38,720
you can take the original data and

165
00:06:38,720 --> 00:06:39,890
compress it down to a much much smaller

166
00:06:39,890 --> 00:06:41,900
size but it's very computationally

167
00:06:41,900 --> 00:06:44,210
expensive so they're not going to do

168
00:06:44,210 --> 00:06:45,620
that they're gonna instead choose a

169
00:06:45,620 --> 00:06:47,630
compression scheme oppression protocol

170
00:06:47,630 --> 00:06:52,430
that maybe isn't as as doesn't produce

171
00:06:52,430 --> 00:06:55,700
as good of a compression ratio but it

172
00:06:55,700 --> 00:06:56,900
can run fast yeah

173
00:06:56,900 --> 00:06:58,730
quickly and then there's also be the

174
00:06:58,730 --> 00:06:59,600
straight awful by the way you want to

175
00:06:59,600 --> 00:07:01,160
compress very quickly or decompress very

176
00:07:01,160 --> 00:07:02,780
quickly and in most cases we're gonna

177
00:07:02,780 --> 00:07:05,330
care about decompressing quickly so I'll

178
00:07:05,330 --> 00:07:07,690
because that's one new most of the time

179
00:07:07,690 --> 00:07:11,110
so why we want to do this right well

180
00:07:11,110 --> 00:07:13,460
DRAM is not cheap I have to pay for to

181
00:07:13,460 --> 00:07:14,600
buy that hardware and actually pay to

182
00:07:14,600 --> 00:07:16,190
keep refreshing it to keep the charge in

183
00:07:16,190 --> 00:07:18,260
that cost energy so if I can store my

184
00:07:18,260 --> 00:07:21,350
database on a smaller machine with less

185
00:07:21,350 --> 00:07:24,020
memory then that's gonna be a good win

186
00:07:24,020 --> 00:07:26,240
for us and in some cases when we when we

187
00:07:26,240 --> 00:07:27,290
compress the data we're actually gonna

188
00:07:27,290 --> 00:07:28,880
get better performance as well because

189
00:07:28,880 --> 00:07:30,350
now things be able to fit me CPU caches

190
00:07:30,350 --> 00:07:33,950
or for that for a for a looking at a

191
00:07:33,950 --> 00:07:35,930
smaller portion of data we can still

192
00:07:35,930 --> 00:07:38,960
compute the same result for query so

193
00:07:38,960 --> 00:07:40,220
this is gonna be a win-win almost all

194
00:07:40,220 --> 00:07:43,610
around so to understand why we can

195
00:07:43,610 --> 00:07:44,690
compress the data we're gonna start to

196
00:07:44,690 --> 00:07:46,160
understand a little bit what what real

197
00:07:46,160 --> 00:07:48,890
databases actually looked like and so

198
00:07:48,890 --> 00:07:51,020
the first thing is that the real data

199
00:07:51,020 --> 00:07:53,270
sets are going to be very skewed right

200
00:07:53,270 --> 00:07:54,650
that means that like the the

201
00:07:54,650 --> 00:07:56,750
distribution of values for a given

202
00:07:56,750 --> 00:07:59,660
column are not going to be uniform right

203
00:07:59,660 --> 00:08:01,550
to using zip codes as an example there's

204
00:08:01,550 --> 00:08:03,140
more people that live in New York City

205
00:08:03,140 --> 00:08:05,630
then live in Montana so when I look at

206
00:08:05,630 --> 00:08:07,100
if I have a list of all you know people

207
00:08:07,100 --> 00:08:09,170
and I'm storing their zip codes there's

208
00:08:09,170 --> 00:08:10,820
many more people living in New York than

209
00:08:10,820 --> 00:08:13,070
in Montana and I could take advantage of

210
00:08:13,070 --> 00:08:15,410
that in my compression compression

211
00:08:15,410 --> 00:08:18,770
protocol another good example is there's

212
00:08:18,770 --> 00:08:22,130
also the brown corpus and this exhibits

213
00:08:22,130 --> 00:08:23,120
this property called the zipping

214
00:08:23,120 --> 00:08:25,190
distribution so the brown corpus was

215
00:08:25,190 --> 00:08:28,280
this project in the 1960s at Brown

216
00:08:28,280 --> 00:08:31,520
University where they basically took the

217
00:08:31,520 --> 00:08:32,630
what they considered to be the

218
00:08:32,630 --> 00:08:35,179
the the most emblematic books in the

219
00:08:35,179 --> 00:08:37,340
English language and they just counted

220
00:08:37,340 --> 00:08:38,539
the number of words that were in these

221
00:08:38,539 --> 00:08:41,000
books and we suggest let's take a guess

222
00:08:41,000 --> 00:08:44,420
what the most common word was duh yes

223
00:08:44,420 --> 00:08:46,730
right th e but then what happen is the

224
00:08:46,730 --> 00:08:49,550
next Muskaan word was the word a and the

225
00:08:49,550 --> 00:08:52,880
the word a or so word the appears twice

226
00:08:52,880 --> 00:08:55,520
as many times as the word a and a

227
00:08:55,520 --> 00:08:57,320
appears twice as many times as the third

228
00:08:57,320 --> 00:08:58,610
word and so forth so you have this sort

229
00:08:58,610 --> 00:09:00,920
of power law curve so a lot of real

230
00:09:00,920 --> 00:09:02,330
workloads are gonna look like that and

231
00:09:02,330 --> 00:09:03,650
these are the kind of things we can take

232
00:09:03,650 --> 00:09:04,820
advantage of oh you think you like

233
00:09:04,820 --> 00:09:06,920
reddit or hacker news there's a small

234
00:09:06,920 --> 00:09:08,630
number of articles that actually get a

235
00:09:08,630 --> 00:09:10,550
lot of comments most people submit crap

236
00:09:10,550 --> 00:09:14,930
and no one ever sees it right and so if

237
00:09:14,930 --> 00:09:16,280
we know this then there's a bunch of

238
00:09:16,280 --> 00:09:17,990
things you know the long tail stuff we

239
00:09:17,990 --> 00:09:19,160
can then maybe compress in a different

240
00:09:19,160 --> 00:09:23,210
way or get a good compression ratio the

241
00:09:23,210 --> 00:09:24,740
other thing also too in our data sets is

242
00:09:24,740 --> 00:09:25,960
that they're gonna be highly correlated

243
00:09:25,960 --> 00:09:28,670
so that means that the values of

244
00:09:28,670 --> 00:09:30,560
different attributes in the same tuple

245
00:09:30,560 --> 00:09:32,900
or often times gonna be correlated or

246
00:09:32,900 --> 00:09:35,390
related to each other alright so like my

247
00:09:35,390 --> 00:09:37,460
zip code for my address that can be

248
00:09:37,460 --> 00:09:40,310
correlated to my to my city so I know my

249
00:09:40,310 --> 00:09:42,020
city name and state I know its zip code

250
00:09:42,020 --> 00:09:44,920
I'm in or if I buy something on Amazon

251
00:09:44,920 --> 00:09:47,450
oftentimes the the date that something

252
00:09:47,450 --> 00:09:48,950
was shipped or started the date that

253
00:09:48,950 --> 00:09:51,050
somebody placed the order is only gonna

254
00:09:51,050 --> 00:09:54,110
be a few days it's only a few days later

255
00:09:54,110 --> 00:09:55,370
would actually be shipped so I buy

256
00:09:55,370 --> 00:09:56,240
something today

257
00:09:56,240 --> 00:09:57,830
the Amazons ship and out the same

258
00:09:57,830 --> 00:09:59,750
day like I buy something today it's

259
00:09:59,750 --> 00:10:01,250
gonna be shipped tomorrow so maybe

260
00:10:01,250 --> 00:10:04,820
instead of storing a full 64-bit date

261
00:10:04,820 --> 00:10:06,860
and time stamp for the order date and

262
00:10:06,860 --> 00:10:09,020
ship date if I just store this and I can

263
00:10:09,020 --> 00:10:11,090
now maybe store the Delta which is like

264
00:10:11,090 --> 00:10:13,370
you know a couple hours off of this then

265
00:10:13,370 --> 00:10:16,490
I can say space there so these are a lot

266
00:10:16,490 --> 00:10:19,160
of these techniques about these aspects

267
00:10:19,160 --> 00:10:21,530
of real datasets is why we're gonna be

268
00:10:21,530 --> 00:10:23,900
able to get a good compression ratio and

269
00:10:23,900 --> 00:10:25,760
employ these techniques without slowing

270
00:10:25,760 --> 00:10:29,270
down the system entirely so what do we

271
00:10:29,270 --> 00:10:31,910
want in our compression scheme well for

272
00:10:31,910 --> 00:10:34,670
most is that we need these compression

273
00:10:34,670 --> 00:10:36,320
schemes to produce fixed length values

274
00:10:36,320 --> 00:10:38,510
because again remember we had this fixed

275
00:10:38,510 --> 00:10:39,980
length data pool and that's the sort of

276
00:10:39,980 --> 00:10:43,640
the the the you know the the the the

277
00:10:43,640 --> 00:10:46,230
main primary location of a tuple

278
00:10:46,230 --> 00:10:47,430
so in order for me to jump down to

279
00:10:47,430 --> 00:10:49,410
different offsets within that fixed

280
00:10:49,410 --> 00:10:51,510
thing a fixed-length datapool all my

281
00:10:51,510 --> 00:10:53,970
values need to be fixed length so if I

282
00:10:53,970 --> 00:10:56,100
start compressing strings that you know

283
00:10:56,100 --> 00:10:58,139
from two random sizes that's not going

284
00:10:58,139 --> 00:10:59,750
to work for us so everything has to be

285
00:10:59,750 --> 00:11:01,950
fixed length datable the other exception

286
00:11:01,950 --> 00:11:03,449
would be when we shove things out the

287
00:11:03,449 --> 00:11:05,279
bar length datapool since that were

288
00:11:05,279 --> 00:11:08,459
recording the size of the data in the

289
00:11:08,459 --> 00:11:10,529
very lengthy de poule it's okay for that

290
00:11:10,529 --> 00:11:13,050
to be variable sized but the most of the

291
00:11:13,050 --> 00:11:15,420
data need to be fixed length the other

292
00:11:15,420 --> 00:11:18,360
thing we want to be able do is postpone

293
00:11:18,360 --> 00:11:21,720
the the had the gauge of some weight as

294
00:11:21,720 --> 00:11:23,850
long as possible before it actually has

295
00:11:23,850 --> 00:11:27,089
to decompress the data in the lifetime

296
00:11:27,089 --> 00:11:29,610
of the lifecycle of a query so what I

297
00:11:29,610 --> 00:11:31,139
mean by that is if I have a bunch of

298
00:11:31,139 --> 00:11:33,570
compressed datum I want to try to do my

299
00:11:33,570 --> 00:11:35,639
query X you might as much as my query I

300
00:11:35,639 --> 00:11:38,760
can all in that compressed data and only

301
00:11:38,760 --> 00:11:40,500
until I have to then produce a result to

302
00:11:40,500 --> 00:11:41,730
a human or to the outside world of the

303
00:11:41,730 --> 00:11:43,500
database ISM then I go ahead and

304
00:11:43,500 --> 00:11:46,139
decompress it all right that's gonna be

305
00:11:46,139 --> 00:11:47,190
a big deal this is called late

306
00:11:47,190 --> 00:11:48,930
materialization and this is why we're

307
00:11:48,930 --> 00:11:50,339
gonna want to do native compression

308
00:11:50,339 --> 00:11:52,079
inside of our database system instead of

309
00:11:52,079 --> 00:11:53,459
using a look at general-purpose Alvin

310
00:11:53,459 --> 00:11:54,990
what we do doesn't know anything about

311
00:11:54,990 --> 00:11:57,720
what's actually being stored and the

312
00:11:57,720 --> 00:12:00,089
last one is sort of obvious as well but

313
00:12:00,089 --> 00:12:01,230
we should just sort of bring it up

314
00:12:01,230 --> 00:12:02,490
because there's different types of

315
00:12:02,490 --> 00:12:03,449
compression algorithms and we make sure

316
00:12:03,449 --> 00:12:05,519
that we only pick ones that we actually

317
00:12:05,519 --> 00:12:07,290
want to use so any compression scheme

318
00:12:07,290 --> 00:12:10,529
you want to use has to be lossless what

319
00:12:10,529 --> 00:12:13,350
does that mean what's that

320
00:12:13,350 --> 00:12:14,430
yeah you don't well yes you don't lose

321
00:12:14,430 --> 00:12:16,529
data but like if I take some original

322
00:12:16,529 --> 00:12:18,449
data and I compress it and then I

323
00:12:18,449 --> 00:12:19,920
decompress it I should be able to get

324
00:12:19,920 --> 00:12:21,000
back that original data

325
00:12:21,000 --> 00:12:22,740
what's saving is a ball of a lossy

326
00:12:22,740 --> 00:12:26,579
compression compression algorithm and b3

327
00:12:26,579 --> 00:12:27,360
exactly yes

328
00:12:27,360 --> 00:12:30,060
so in our database and we have to be

329
00:12:30,060 --> 00:12:33,240
almost always be lossless because people

330
00:12:33,240 --> 00:12:35,339
don't like it when you lose data but

331
00:12:35,339 --> 00:12:37,140
again if it's your bank account you put

332
00:12:37,140 --> 00:12:38,699
$100 in and they come back to say no you

333
00:12:38,699 --> 00:12:40,529
have you have 80 dollars you're gonna be

334
00:12:40,529 --> 00:12:42,750
pissed so any compression scheme can't

335
00:12:42,750 --> 00:12:46,019
that we want to use can't do this if we

336
00:12:46,019 --> 00:12:48,709
do want to do a lossy compression scheme

337
00:12:48,709 --> 00:12:51,000
then that must be done at the

338
00:12:51,000 --> 00:12:53,339
application level meaning some human has

339
00:12:53,339 --> 00:12:54,990
to make a value judgment that it's okay

340
00:12:54,990 --> 00:12:56,579
to lose data in our database the

341
00:12:56,579 --> 00:12:58,800
database system we're not gonna figure

342
00:12:58,800 --> 00:12:59,760
that on Onis own

343
00:12:59,760 --> 00:13:02,050
because the best compression scheme is

344
00:13:02,050 --> 00:13:03,400
to write everything that dev know and

345
00:13:03,400 --> 00:13:05,800
lose everything right it's gonna have an

346
00:13:05,800 --> 00:13:07,060
amazing compression ratio it's gonna be

347
00:13:07,060 --> 00:13:08,590
really fast but of course you don't have

348
00:13:08,590 --> 00:13:10,630
any data so in that sort of an extreme

349
00:13:10,630 --> 00:13:12,520
example but that's the basic idea the

350
00:13:12,520 --> 00:13:13,750
data stem doesn't know what you actually

351
00:13:13,750 --> 00:13:16,600
care about so it's not gonna do anything

352
00:13:16,600 --> 00:13:21,370
that's lossing the we're not going to

353
00:13:21,370 --> 00:13:22,780
talk about this but there's there are

354
00:13:22,780 --> 00:13:24,400
techniques call it approximate queries

355
00:13:24,400 --> 00:13:26,340
that sort of look like lossy compression

356
00:13:26,340 --> 00:13:31,390
so if I have like a column of people

357
00:13:31,390 --> 00:13:33,310
visiting my website and I want to count

358
00:13:33,310 --> 00:13:35,920
the number of people that visited the

359
00:13:35,920 --> 00:13:38,980
website in the last month I can you know

360
00:13:38,980 --> 00:13:40,720
look at every single tuple and can add

361
00:13:40,720 --> 00:13:43,060
that all that up and count it or maybe I

362
00:13:43,060 --> 00:13:45,340
could sample it or maybe just you know

363
00:13:45,340 --> 00:13:47,050
skip some portion of it and then try to

364
00:13:47,050 --> 00:13:50,110
approximate that count and the idea

365
00:13:50,110 --> 00:13:52,300
there is like you know who cares if I

366
00:13:52,300 --> 00:13:53,800
had a you know I had a million users to

367
00:13:53,800 --> 00:13:55,810
my website but then my approximate count

368
00:13:55,810 --> 00:13:58,060
comes back with you know 980 thousand or

369
00:13:58,060 --> 00:13:59,560
some like that like it's close enough

370
00:13:59,560 --> 00:14:02,290
and so again we're not gonna like this

371
00:14:02,290 --> 00:14:03,430
but these are called approximate queries

372
00:14:03,430 --> 00:14:05,670
this is actually now being showing up in

373
00:14:05,670 --> 00:14:08,320
in the commercial systems they'll

374
00:14:08,320 --> 00:14:09,880
support this today like the habit

375
00:14:09,880 --> 00:14:11,490
explicit like approximate count

376
00:14:11,490 --> 00:14:13,990
operation or aggregate aggregation

377
00:14:13,990 --> 00:14:16,150
function in Oracle and sequel server and

378
00:14:16,150 --> 00:14:18,220
I think TB 2 might have this so we're

379
00:14:18,220 --> 00:14:20,080
not gonna talk about that but like ok

380
00:14:20,080 --> 00:14:20,860
this is the next slide

381
00:14:20,860 --> 00:14:25,330
sorry this is I mean so this technique

382
00:14:25,330 --> 00:14:28,570
actually does exist and then a sequel

383
00:14:28,570 --> 00:14:29,980
server I think just added this within

384
00:14:29,980 --> 00:14:31,720
the last year again the idea is you're

385
00:14:31,720 --> 00:14:33,190
just sampling on the data and produce

386
00:14:33,190 --> 00:14:36,160
counts pretty segregations a another

387
00:14:36,160 --> 00:14:38,710
type of technique is to do zone maps and

388
00:14:38,710 --> 00:14:40,780
so I would categorize both these guys as

389
00:14:40,780 --> 00:14:42,580
in data skipping meaning like we're

390
00:14:42,580 --> 00:14:44,860
trying to avoid having a look at a bunch

391
00:14:44,860 --> 00:14:46,540
of data the compression scheme we're

392
00:14:46,540 --> 00:14:47,620
gonna talk about today we stopped to

393
00:14:47,620 --> 00:14:49,270
look at everything but the things we're

394
00:14:49,270 --> 00:14:50,770
looking at is compressed these are

395
00:14:50,770 --> 00:14:52,240
techniques to say like well I can miss

396
00:14:52,240 --> 00:14:54,850
ignore large chunks of things so zone

397
00:14:54,850 --> 00:14:56,080
maps I think we covered in the

398
00:14:56,080 --> 00:14:57,700
introduction class it's not really a

399
00:14:57,700 --> 00:14:59,140
compression scheme it's just a technique

400
00:14:59,140 --> 00:15:01,450
for in for skipping data the idea is

401
00:15:01,450 --> 00:15:03,310
that you pre compute some some

402
00:15:03,310 --> 00:15:05,710
aggregations on columns and then when

403
00:15:05,710 --> 00:15:07,300
you start executing queries you can look

404
00:15:07,300 --> 00:15:08,950
at those those pre computed aggregations

405
00:15:08,950 --> 00:15:10,510
and decide whether there's anything you

406
00:15:10,510 --> 00:15:12,070
even want to look at and give

407
00:15:12,070 --> 00:15:15,519
Wacka datum so they're called zone maps

408
00:15:15,519 --> 00:15:17,829
by Oracle I'm sure that they have

409
00:15:17,829 --> 00:15:20,380
patents on this sometimes they call

410
00:15:20,380 --> 00:15:22,060
pre-computer aggregations but in general

411
00:15:22,060 --> 00:15:24,220
in databases if you say zone maps even

412
00:15:24,220 --> 00:15:25,300
if you're not talking on the Oracle

413
00:15:25,300 --> 00:15:26,380
system people know what you're talking

414
00:15:26,380 --> 00:15:28,180
on did you talk about this thing so

415
00:15:28,180 --> 00:15:29,740
let's say I have a column here it has

416
00:15:29,740 --> 00:15:32,649
five values and so I can compute of zone

417
00:15:32,649 --> 00:15:34,630
map where I'm going to have all the

418
00:15:34,630 --> 00:15:36,880
aggregation functions I may want to run

419
00:15:36,880 --> 00:15:39,430
compute it for this column so the main

420
00:15:39,430 --> 00:15:42,339
max average sum and count and then now

421
00:15:42,339 --> 00:15:44,079
if a query shows up like select star

422
00:15:44,079 --> 00:15:45,810
from table where values greater than 600

423
00:15:45,810 --> 00:15:48,160
instead of scanning through the column

424
00:15:48,160 --> 00:15:50,019
assuming this is a block of data said of

425
00:15:50,019 --> 00:15:51,970
having to scan through this and find all

426
00:15:51,970 --> 00:15:53,680
the records that where values greater

427
00:15:53,680 --> 00:15:56,170
than 600 I can use instead look at my

428
00:15:56,170 --> 00:15:59,440
max value for my for the zone map and

429
00:15:59,440 --> 00:16:01,540
see it the VAX value for this block is

430
00:16:01,540 --> 00:16:03,790
400 so therefore I know there could not

431
00:16:03,790 --> 00:16:06,670
be any tuple in here that is going to be

432
00:16:06,670 --> 00:16:08,800
600 or greater so I don't even bother

433
00:16:08,800 --> 00:16:10,360
looking at this I just look at the zone

434
00:16:10,360 --> 00:16:11,920
map see that thing doesn't satisfy my

435
00:16:11,920 --> 00:16:18,420
predicate and I can ignore this so yes

436
00:16:21,449 --> 00:16:23,740
so this question is if you're running a

437
00:16:23,740 --> 00:16:27,660
BTC when would you update zone map so

438
00:16:27,660 --> 00:16:30,760
you so for this and the compression

439
00:16:30,760 --> 00:16:32,769
stuff what about today you typically

440
00:16:32,769 --> 00:16:34,660
only do this on cold data that's not

441
00:16:34,660 --> 00:16:36,430
being modified all the time so you would

442
00:16:36,430 --> 00:16:38,500
leave it uncompressed don't compute a

443
00:16:38,500 --> 00:16:40,690
zone map and only when you recognize it

444
00:16:40,690 --> 00:16:41,769
know when we've come and updated this

445
00:16:41,769 --> 00:16:43,240
block in a while let me go ahead and

446
00:16:43,240 --> 00:16:47,980
compute these things all right like I

447
00:16:47,980 --> 00:16:50,139
said this only appears in the more

448
00:16:50,139 --> 00:16:53,139
high-end commercial systems going back

449
00:16:53,139 --> 00:16:56,230
here Oracle has this Vertica mem sequel

450
00:16:56,230 --> 00:16:59,079
mateesah was a fpga accelerate database

451
00:16:59,079 --> 00:17:00,760
that IBM paulette they killed at they

452
00:17:00,760 --> 00:17:03,370
killed that off last year but I think

453
00:17:03,370 --> 00:17:04,869
zoom Maps originally originated from

454
00:17:04,869 --> 00:17:05,919
these guys and then everyone else

455
00:17:05,919 --> 00:17:08,079
adopted the same idea but only Oracle

456
00:17:08,079 --> 00:17:09,970
calls them zone maps cuz because that

457
00:17:09,970 --> 00:17:11,619
that's the Oracle term and everyone else

458
00:17:11,619 --> 00:17:13,209
causing pre-computer aggregations or

459
00:17:13,209 --> 00:17:16,619
something or okay all right

460
00:17:16,619 --> 00:17:18,699
it'll is that okay let's focus on the

461
00:17:18,699 --> 00:17:20,020
compression stuff that that's that's the

462
00:17:20,020 --> 00:17:22,150
main thing we won't talk about today so

463
00:17:22,150 --> 00:17:24,309
if you want to add compression to our

464
00:17:24,309 --> 00:17:25,440
database system

465
00:17:25,440 --> 00:17:26,490
the first question we have to think

466
00:17:26,490 --> 00:17:28,890
about and in sense weird ones actually

467
00:17:28,890 --> 00:17:31,080
building the system is what are we

468
00:17:31,080 --> 00:17:33,780
trying to compress and based on that

469
00:17:33,780 --> 00:17:36,390
decision that'll tell us what kind of

470
00:17:36,390 --> 00:17:38,510
compression scheme we could possibly do

471
00:17:38,510 --> 00:17:41,940
so the first approach is just do it on a

472
00:17:41,940 --> 00:17:44,070
block right it could be a block of

473
00:17:44,070 --> 00:17:46,140
columns or it could be a row store it

474
00:17:46,140 --> 00:17:47,190
doesn't matter but it's a block of two

475
00:17:47,190 --> 00:17:49,530
floats within the same table you could

476
00:17:49,530 --> 00:17:51,060
even do even more fine-grain than that

477
00:17:51,060 --> 00:17:52,380
you could do another tuple level so

478
00:17:52,380 --> 00:17:54,420
within a single tuple you compress all

479
00:17:54,420 --> 00:17:57,270
the values even more fine-grain is you

480
00:17:57,270 --> 00:18:00,180
have a single attribute within one tuple

481
00:18:00,180 --> 00:18:02,670
and you want to compress that all right

482
00:18:02,670 --> 00:18:03,990
that sounds crazy but if you have a

483
00:18:03,990 --> 00:18:05,580
large text field that that might make

484
00:18:05,580 --> 00:18:07,380
sense like Wikipedia has those all those

485
00:18:07,380 --> 00:18:10,050
articles are all every revision of a

486
00:18:10,050 --> 00:18:12,330
Wikipedia article is its own accurate

487
00:18:12,330 --> 00:18:14,880
value in a tuple and you want to

488
00:18:14,880 --> 00:18:17,490
compress that or we can compress it on

489
00:18:17,490 --> 00:18:19,470
on the entire column so we take all the

490
00:18:19,470 --> 00:18:21,450
values for a single attribute across

491
00:18:21,450 --> 00:18:22,950
multiple tuples and then the table and

492
00:18:22,950 --> 00:18:24,720
we'll go ahead and compress this and

493
00:18:24,720 --> 00:18:26,940
this primarily we only want to do this

494
00:18:26,940 --> 00:18:31,950
if it's a column story so most systems

495
00:18:31,950 --> 00:18:34,020
most of the OLAP systems are going to do

496
00:18:34,020 --> 00:18:36,420
this and any other system that does do

497
00:18:36,420 --> 00:18:38,370
compression may be like a naive

498
00:18:38,370 --> 00:18:39,810
compression look at my Segal we'll see

499
00:18:39,810 --> 00:18:41,570
in a few more slides they'll do this

500
00:18:41,570 --> 00:18:44,100
there are some systems that can support

501
00:18:44,100 --> 00:18:45,650
this like sequel server can do this

502
00:18:45,650 --> 00:18:48,030
there's a variant of rocks DVD called

503
00:18:48,030 --> 00:18:50,070
Terra at a China and they can do this

504
00:18:50,070 --> 00:18:52,470
this appears for only variable length

505
00:18:52,470 --> 00:18:54,720
fields if I have a large blob large text

506
00:18:54,720 --> 00:18:56,760
field then I can compress this but again

507
00:18:56,760 --> 00:18:58,020
the compression scheme you're gonna want

508
00:18:58,020 --> 00:19:00,150
to use for all of these are gonna be

509
00:19:00,150 --> 00:19:02,580
much different but for this I could do

510
00:19:02,580 --> 00:19:04,470
naive compression like gzip or snappy

511
00:19:04,470 --> 00:19:06,660
but for this one we'll use the

512
00:19:06,660 --> 00:19:07,530
dictionary coding or the other

513
00:19:07,530 --> 00:19:11,100
techniques okay all right so let's let's

514
00:19:11,100 --> 00:19:13,080
first focus on naive compression because

515
00:19:13,080 --> 00:19:15,540
this is like if you're coming from you

516
00:19:15,540 --> 00:19:16,800
know if you if you know something

517
00:19:16,800 --> 00:19:19,200
roughly about compression and I'm when

518
00:19:19,200 --> 00:19:20,250
I'm saying oh we're gonna compress data

519
00:19:20,250 --> 00:19:21,510
this might be the first thing you think

520
00:19:21,510 --> 00:19:23,970
of so the idea here with naive

521
00:19:23,970 --> 00:19:26,040
compression is that we're gonna take an

522
00:19:26,040 --> 00:19:27,090
off-the-shelf general-purpose

523
00:19:27,090 --> 00:19:28,740
compression algorithm that our people

524
00:19:28,740 --> 00:19:30,930
using already and other things in our

525
00:19:30,930 --> 00:19:33,030
databases and we're just going to take

526
00:19:33,030 --> 00:19:35,940
our sequence of bytes from whatever data

527
00:19:35,940 --> 00:19:37,770
we're trying to compress whether it's a

528
00:19:37,770 --> 00:19:38,880
tuple whether this thing will add should

529
00:19:38,880 --> 00:19:39,030
be

530
00:19:39,030 --> 00:19:41,520
with its column or block and we're gonna

531
00:19:41,520 --> 00:19:42,900
run run it through these algorithms and

532
00:19:42,900 --> 00:19:45,600
they're gonna produce a but hopefully a

533
00:19:45,600 --> 00:19:47,670
block of data that's smaller than what I

534
00:19:47,670 --> 00:19:52,200
gave it so the sort of the the kana

535
00:19:52,200 --> 00:19:54,540
algorithm is people gonna want to use

536
00:19:54,540 --> 00:19:56,310
I said gzip but that's usually not what

537
00:19:56,310 --> 00:19:57,570
you want to use because gzip is an

538
00:19:57,570 --> 00:19:58,920
example of a heavyweight compression

539
00:19:58,920 --> 00:20:01,080
algorithm where it's gonna get a better

540
00:20:01,080 --> 00:20:02,730
compression compression ratio than these

541
00:20:02,730 --> 00:20:04,350
other ones but it's gonna do much more

542
00:20:04,350 --> 00:20:07,350
slowly with like L 0 or LZ 4 these are

543
00:20:07,350 --> 00:20:08,910
some of the first protocols people have

544
00:20:08,910 --> 00:20:11,730
developed where they are you don't get

545
00:20:11,730 --> 00:20:12,660
it's good a compression ratio but

546
00:20:12,660 --> 00:20:14,370
they're much faster

547
00:20:14,370 --> 00:20:16,620
snappy is from Google that's another one

548
00:20:16,620 --> 00:20:17,540
that's very common in database systems

549
00:20:17,540 --> 00:20:20,160
broadly is from Facebook this is not

550
00:20:20,160 --> 00:20:22,290
typically using databases because this

551
00:20:22,290 --> 00:20:24,840
is designed for like compressing

552
00:20:24,840 --> 00:20:26,610
webpages or there's some there's some

553
00:20:26,610 --> 00:20:27,750
magic in here that you know if you have

554
00:20:27,750 --> 00:20:29,550
a much HTML tags and you see over and

555
00:20:29,550 --> 00:20:31,410
over again they can make it in

556
00:20:31,410 --> 00:20:33,390
compression them better Oracle has this

557
00:20:33,390 --> 00:20:34,830
thing called a zip that's their own

558
00:20:34,830 --> 00:20:36,450
patented proprietary compression

559
00:20:36,450 --> 00:20:39,060
algorithm it's you know it's it's a

560
00:20:39,060 --> 00:20:42,600
variant of these like LZ for dictionary

561
00:20:42,600 --> 00:20:44,100
based compression schemes but what's

562
00:20:44,100 --> 00:20:46,950
fascinating about it Oracle bought Sun

563
00:20:46,950 --> 00:20:49,050
some was making the spark chips so for a

564
00:20:49,050 --> 00:20:51,090
while I don't think I think they

565
00:20:51,090 --> 00:20:52,950
discontinued spark I forget but for a

566
00:20:52,950 --> 00:20:55,260
while they were actually storing the the

567
00:20:55,260 --> 00:20:57,180
a zip decompression and compression

568
00:20:57,180 --> 00:20:59,370
algorithm on hardware right so you can

569
00:20:59,370 --> 00:21:01,110
actually do this as accelerated on if

570
00:21:01,110 --> 00:21:03,480
you but if you bought spark chips and

571
00:21:03,480 --> 00:21:05,550
ran Oracle on it Z standard is

572
00:21:05,550 --> 00:21:06,720
considered to be the sort of the

573
00:21:06,720 --> 00:21:09,540
state-of-the-art one today that gives

574
00:21:09,540 --> 00:21:12,900
you the the same speed as snappy but a

575
00:21:12,900 --> 00:21:15,690
better better compression ratio so a lot

576
00:21:15,690 --> 00:21:17,100
of times you'll see snappy but newer

577
00:21:17,100 --> 00:21:19,250
systems are starting to adopt Z standard

578
00:21:19,250 --> 00:21:21,450
so again we've already talked about this

579
00:21:21,450 --> 00:21:23,090
there's computational overhead of

580
00:21:23,090 --> 00:21:25,050
compressing the data and decompressing

581
00:21:25,050 --> 00:21:28,110
it and so we need be mindful of this and

582
00:21:28,110 --> 00:21:29,190
what we're always going to try to pick

583
00:21:29,190 --> 00:21:31,310
something back is faster than D kress

584
00:21:31,310 --> 00:21:33,270
even though if it's syllabus Florida to

585
00:21:33,270 --> 00:21:34,590
compress because most of the time I'm

586
00:21:34,590 --> 00:21:38,850
just gonna do decompression so let's see

587
00:21:38,850 --> 00:21:40,320
how my secret does this so this is my

588
00:21:40,320 --> 00:21:43,350
sequel in a DB so in what you can do and

589
00:21:43,350 --> 00:21:44,490
my sequel when you cook when you call

590
00:21:44,490 --> 00:21:46,380
create table you can pass in a flag and

591
00:21:46,380 --> 00:21:48,540
say I want to store this as I want to

592
00:21:48,540 --> 00:21:50,580
store this as a compress compress blocks

593
00:21:50,580 --> 00:21:53,700
and so what's gonna happen on

594
00:21:53,700 --> 00:21:55,260
again my sequels not not an in-memory

595
00:21:55,260 --> 00:21:56,550
system so this is a disk system but I

596
00:21:56,550 --> 00:21:58,050
just show you that the implications of

597
00:21:58,050 --> 00:21:59,580
this and it's the clears explanation I

598
00:21:59,580 --> 00:22:02,880
found how did you naive compression so

599
00:22:02,880 --> 00:22:04,680
what they're gonna have on disk is

600
00:22:04,680 --> 00:22:05,820
they're gonna have these compressed

601
00:22:05,820 --> 00:22:08,430
blocks that are gonna be of four

602
00:22:08,430 --> 00:22:10,770
different sizes all powers are two one

603
00:22:10,770 --> 00:22:12,960
two four eight kilobytes and the idea

604
00:22:12,960 --> 00:22:14,370
here is when you have the uncompressed

605
00:22:14,370 --> 00:22:16,830
block and then you compress it whatever

606
00:22:16,830 --> 00:22:19,020
is the ceiling of the size up to the

607
00:22:19,020 --> 00:22:20,790
next boundary that's you Pat it out to

608
00:22:20,790 --> 00:22:23,790
so if I have a block that's one point

609
00:22:23,790 --> 00:22:26,010
five kilobytes I'll Pat it out so it's

610
00:22:26,010 --> 00:22:27,630
two kilobytes and they do this or that

611
00:22:27,630 --> 00:22:29,850
they can pack things in and align them

612
00:22:29,850 --> 00:22:32,250
nicely out on disk so you're gonna have

613
00:22:32,250 --> 00:22:34,020
the compressed page but then you're also

614
00:22:34,020 --> 00:22:35,730
going to have this uncompressed mod log

615
00:22:35,730 --> 00:22:37,440
that sits in front of it think of this

616
00:22:37,440 --> 00:22:39,420
is like a delta store that represents

617
00:22:39,420 --> 00:22:41,070
the changes that I made to tuples that

618
00:22:41,070 --> 00:22:44,010
are represented in this in this block so

619
00:22:44,010 --> 00:22:45,600
now if I have a query comes along and

620
00:22:45,600 --> 00:22:49,200
wants to access some data then I bring

621
00:22:49,200 --> 00:22:50,490
it to my buffer pool and I keep it

622
00:22:50,490 --> 00:22:53,400
uncompressed and if the query is an

623
00:22:53,400 --> 00:22:56,370
update then all that need to do is just

624
00:22:56,370 --> 00:22:59,280
modify what's in the mod log so I can

625
00:22:59,280 --> 00:23:00,990
sort of Penda entry say I made a change

626
00:23:00,990 --> 00:23:03,060
to something down here to here the data

627
00:23:03,060 --> 00:23:04,830
that I know you have like so you have to

628
00:23:04,830 --> 00:23:07,020
have an index up above and say oh I'm

629
00:23:07,020 --> 00:23:08,970
here's the page that has the tuple

630
00:23:08,970 --> 00:23:10,680
you're looking for so let's say like I

631
00:23:10,680 --> 00:23:12,660
do a set value you know update to both

632
00:23:12,660 --> 00:23:15,210
set value equal ABC then they don't need

633
00:23:15,210 --> 00:23:16,410
to look to see what the original value

634
00:23:16,410 --> 00:23:19,140
was I just need to put my mod log and

635
00:23:19,140 --> 00:23:20,580
say the new value for this attribute is

636
00:23:20,580 --> 00:23:23,100
now ABC so if anybody comes along tries

637
00:23:23,100 --> 00:23:25,350
to read that you check the mod log just

638
00:23:25,350 --> 00:23:26,790
like you did in the in the BW tree when

639
00:23:26,790 --> 00:23:28,590
you pick me went to the virgin chain and

640
00:23:28,590 --> 00:23:30,240
you say oh this is the latest value for

641
00:23:30,240 --> 00:23:31,980
this particular tuple that's that's what

642
00:23:31,980 --> 00:23:34,170
I'm looking for but if the thing you

643
00:23:34,170 --> 00:23:36,270
want is not in the mod log then you got

644
00:23:36,270 --> 00:23:38,640
a decompressive and so what they

645
00:23:38,640 --> 00:23:40,200
actually do is when they decompress it

646
00:23:40,200 --> 00:23:42,240
they both keep the the compressed

647
00:23:42,240 --> 00:23:44,190
version and the decompressed version in

648
00:23:44,190 --> 00:23:47,220
memory and then decompressed version is

649
00:23:47,220 --> 00:23:50,130
always gonna be 16 kilobytes all right

650
00:23:50,130 --> 00:23:51,750
because they do this to align things

651
00:23:51,750 --> 00:23:53,160
again and in the buffer pool so they'll

652
00:23:53,160 --> 00:23:55,020
have us you know I've region 4 16

653
00:23:55,020 --> 00:23:56,760
kilobyte pages and then for all the

654
00:23:56,760 --> 00:23:58,470
different compressed page sizes they

655
00:23:58,470 --> 00:24:00,320
have a certain separate region for that

656
00:24:00,320 --> 00:24:03,960
so then now if I make any changes and I

657
00:24:03,960 --> 00:24:06,450
want to write them back

658
00:24:06,450 --> 00:24:08,580
if I modify this page then I'd have to

659
00:24:08,580 --> 00:24:11,460
replace this thing before I read it out

660
00:24:11,460 --> 00:24:12,900
otherwise if I don't modify it then I

661
00:24:12,900 --> 00:24:14,700
could blow this thing away and then if

662
00:24:14,700 --> 00:24:16,020
this thing's I modified I can blow that

663
00:24:16,020 --> 00:24:17,250
away as to it were free from the payment

664
00:24:17,250 --> 00:24:29,340
from the buffer pool so make yes these

665
00:24:29,340 --> 00:24:30,780
questions can be add things to the mob

666
00:24:30,780 --> 00:24:32,940
log beyond this like so this is a fixed

667
00:24:32,940 --> 00:24:34,890
size I'm about to fix size so you run

668
00:24:34,890 --> 00:24:36,630
out of space in here then you have to

669
00:24:36,630 --> 00:24:38,910
decompress it and and come you know

670
00:24:38,910 --> 00:24:40,310
compactor code I apply all the changes

671
00:24:40,310 --> 00:24:44,940
yeah it's like it's like the Delta store

672
00:24:44,940 --> 00:24:46,500
part for the bead every tree we have is

673
00:24:46,500 --> 00:24:49,380
a fix number of yeah okay so the main

674
00:24:49,380 --> 00:24:50,490
thing I want to point out about this is

675
00:24:50,490 --> 00:24:55,320
that as I said if I need to access

676
00:24:55,320 --> 00:24:58,470
something that's in the compressed data

677
00:24:58,470 --> 00:25:03,240
I have to decompress it because again

678
00:25:03,240 --> 00:25:06,030
assume I ran this through snappy or gzip

679
00:25:06,030 --> 00:25:07,260
or whatever you want to whatever album

680
00:25:07,260 --> 00:25:09,030
you want to use the database doesn't

681
00:25:09,030 --> 00:25:10,290
know anything of what's inside those

682
00:25:10,290 --> 00:25:13,710
bytes of the page right underneath the

683
00:25:13,710 --> 00:25:15,420
covers these algorithms are actually off

684
00:25:15,420 --> 00:25:17,730
oftentimes using dictionary compression

685
00:25:17,730 --> 00:25:20,070
but you don't you can't see that in the

686
00:25:20,070 --> 00:25:21,390
database system unless you know how to

687
00:25:21,390 --> 00:25:24,570
interpret the the compressed format of

688
00:25:24,570 --> 00:25:27,150
those different algorithms but I don't

689
00:25:27,150 --> 00:25:29,640
think any system actually does that so

690
00:25:29,640 --> 00:25:31,710
this now means that this is gonna limit

691
00:25:31,710 --> 00:25:33,000
the scope of what we can actually

692
00:25:33,000 --> 00:25:35,700
compress because if I try to compress an

693
00:25:35,700 --> 00:25:37,980
entire column and I need to get one

694
00:25:37,980 --> 00:25:40,350
tuple at the in that column now I got to

695
00:25:40,350 --> 00:25:46,050
decompress the entire thing right so the

696
00:25:46,050 --> 00:25:47,900
main takeaway from this is that because

697
00:25:47,900 --> 00:25:50,370
we're using an IE compression scheme

698
00:25:50,370 --> 00:25:51,390
with it is it doesn't know what's going

699
00:25:51,390 --> 00:25:53,610
on we're losing all high-level

700
00:25:53,610 --> 00:25:54,840
understanding of what's in the what's in

701
00:25:54,840 --> 00:25:56,400
the thing we're compressing and what are

702
00:25:56,400 --> 00:25:58,140
the semantics of that data so we can't

703
00:25:58,140 --> 00:26:00,990
do any any we can't really do anything

704
00:26:00,990 --> 00:26:02,580
other than just decompress it every

705
00:26:02,580 --> 00:26:05,310
single time so this is now where we're

706
00:26:05,310 --> 00:26:06,510
gonna talk about these these better

707
00:26:06,510 --> 00:26:08,760
compression schemes where again the data

708
00:26:08,760 --> 00:26:10,170
system is natively gonna do this because

709
00:26:10,170 --> 00:26:11,400
it's gonna know how things got

710
00:26:11,400 --> 00:26:14,310
compressed what it compressed and then

711
00:26:14,310 --> 00:26:16,530
in some cases when it sees queries it

712
00:26:16,530 --> 00:26:18,000
can maybe just compress the things that

713
00:26:18,000 --> 00:26:19,140
the queries are looking like look

714
00:26:19,140 --> 00:26:20,400
they're trying to look up on

715
00:26:20,400 --> 00:26:22,050
actually the predicate of the queries

716
00:26:22,050 --> 00:26:23,970
and do your matching on that and you

717
00:26:23,970 --> 00:26:26,280
never have to decompress the original to

718
00:26:26,280 --> 00:26:29,309
pour the original data so so they say

719
00:26:29,309 --> 00:26:31,890
this is my table of users and I have

720
00:26:31,890 --> 00:26:33,030
their salary and I have just two tuples

721
00:26:33,030 --> 00:26:34,710
Andy and Matt and we have our fake

722
00:26:34,710 --> 00:26:38,220
salaries so if I compress it I'm not

723
00:26:38,220 --> 00:26:39,809
saying how I'm compressing this assume I

724
00:26:39,809 --> 00:26:43,710
could and for every single value in my

725
00:26:43,710 --> 00:26:45,809
table I may have a compress it with

726
00:26:45,809 --> 00:26:47,190
something that's smaller than what the

727
00:26:47,190 --> 00:26:50,100
original size was and so now if I want

728
00:26:50,100 --> 00:26:51,720
to say find all the users were named

729
00:26:51,720 --> 00:26:54,630
equals Andy you know don't assume I have

730
00:26:54,630 --> 00:26:56,940
it an index assume I'm gonna do a

731
00:26:56,940 --> 00:26:59,730
central scan well in order for me to do

732
00:26:59,730 --> 00:27:01,580
that match I'd have to decompress this

733
00:27:01,580 --> 00:27:03,890
but instead what if I actually just

734
00:27:03,890 --> 00:27:05,910
encoded the thing I was looking up one

735
00:27:05,910 --> 00:27:07,980
the same compression algorithm and now I

736
00:27:07,980 --> 00:27:11,030
do my match directly on compress data

737
00:27:11,030 --> 00:27:13,290
all right that's that's why we do we

738
00:27:13,290 --> 00:27:14,580
want the data system to do all the

739
00:27:14,580 --> 00:27:15,600
compression stuff ourselves we don't

740
00:27:15,600 --> 00:27:18,360
want to rely on external algorithm now

741
00:27:18,360 --> 00:27:19,650
for range predicates this is gonna be

742
00:27:19,650 --> 00:27:22,350
more tricky right find all users where

743
00:27:22,350 --> 00:27:24,870
names greater than Andy this is you know

744
00:27:24,870 --> 00:27:26,160
depending how we compress this it may

745
00:27:26,160 --> 00:27:27,690
not work but we'll see how to handle

746
00:27:27,690 --> 00:27:31,559
that in dictionary encoder okay all

747
00:27:31,559 --> 00:27:33,870
right so we're to talk about seven

748
00:27:33,870 --> 00:27:35,880
different question schemes today and

749
00:27:35,880 --> 00:27:38,370
again this is all gonna be for columnar

750
00:27:38,370 --> 00:27:40,950
compression in ole ETP systems you

751
00:27:40,950 --> 00:27:43,020
typically do not compress the data when

752
00:27:43,020 --> 00:27:45,420
it's in memory because as I said it's

753
00:27:45,420 --> 00:27:47,910
gonna be too expensive to to decompress

754
00:27:47,910 --> 00:27:49,320
and recompress over never again as I

755
00:27:49,320 --> 00:27:53,070
start making changes so this is only for

756
00:27:53,070 --> 00:27:55,500
cold data that's not to be modified and

757
00:27:55,500 --> 00:27:56,880
we can we already know we can store it

758
00:27:56,880 --> 00:27:59,850
as a column store so we're sort of a

759
00:27:59,850 --> 00:28:02,250
null suppression run decoding and bitmap

760
00:28:02,250 --> 00:28:04,050
coding does encoding and all these other

761
00:28:04,050 --> 00:28:05,880
ones are again there leading up this

762
00:28:05,880 --> 00:28:06,990
with the paper you guys read but these

763
00:28:06,990 --> 00:28:07,980
are much of other things I think are

764
00:28:07,980 --> 00:28:09,780
interesting to know about but dictionary

765
00:28:09,780 --> 00:28:11,309
encoding is going to be the most common

766
00:28:11,309 --> 00:28:13,140
one but it's not always gonna be the

767
00:28:13,140 --> 00:28:14,640
best one there are there's be some

768
00:28:14,640 --> 00:28:15,929
special cases where these other ones can

769
00:28:15,929 --> 00:28:20,100
do much better okay all right so the

770
00:28:20,100 --> 00:28:21,780
first one the easiest one is called null

771
00:28:21,780 --> 00:28:23,580
suppression and this is a variant of run

772
00:28:23,580 --> 00:28:25,410
run like for coding the basic idea is

773
00:28:25,410 --> 00:28:28,920
that if we have a table that is has

774
00:28:28,920 --> 00:28:31,530
attributes or columns that are sparsely

775
00:28:31,530 --> 00:28:34,170
populated meaning most

776
00:28:34,170 --> 00:28:37,230
the values are null then instead of

777
00:28:37,230 --> 00:28:38,820
storing the null repeatedly over and

778
00:28:38,820 --> 00:28:40,440
over again for every single instance of

779
00:28:40,440 --> 00:28:42,720
a tuple we can then instead maybe just

780
00:28:42,720 --> 00:28:44,940
record somewhere that says hey I have

781
00:28:44,940 --> 00:28:46,830
this many Noel's at this location and

782
00:28:46,830 --> 00:28:51,060
then I only record the exact non null

783
00:28:51,060 --> 00:28:54,950
values in in the table or in the column

784
00:28:54,950 --> 00:28:57,650
now how we actually can do that can vary

785
00:28:57,650 --> 00:29:01,350
one example would be this oral-b BBC

786
00:29:01,350 --> 00:29:03,870
self we'll see in a few more slides but

787
00:29:03,870 --> 00:29:05,610
like I said this is just like this is

788
00:29:05,610 --> 00:29:06,990
pretty common when you know things are

789
00:29:06,990 --> 00:29:08,580
gonna be if you know you have a lot of

790
00:29:08,580 --> 00:29:10,530
sparse data a lot of times the JSON

791
00:29:10,530 --> 00:29:12,060
databases can do this kind of things

792
00:29:12,060 --> 00:29:14,220
because if they organize it as a column

793
00:29:14,220 --> 00:29:15,960
store because most of the time you don't

794
00:29:15,960 --> 00:29:17,340
want maybe have all the attributes that

795
00:29:17,340 --> 00:29:20,910
that other documents have but those are

796
00:29:20,910 --> 00:29:22,110
get run that occurred run link decoding

797
00:29:22,110 --> 00:29:24,120
again is the is sort of be the higher

798
00:29:24,120 --> 00:29:26,130
higher level idea that nil suppression

799
00:29:26,130 --> 00:29:27,690
is and the null suppression is a sort of

800
00:29:27,690 --> 00:29:30,360
optimized version for nulls so the idea

801
00:29:30,360 --> 00:29:33,420
here is that with run like decoding as I

802
00:29:33,420 --> 00:29:35,880
said our values and our columns are

803
00:29:35,880 --> 00:29:37,860
gonna be oftentimes skewed so we have

804
00:29:37,860 --> 00:29:39,780
this a bunch of repeated values over and

805
00:29:39,780 --> 00:29:41,850
over again so if we know that we have

806
00:29:41,850 --> 00:29:44,400
runs of values in our columns that are

807
00:29:44,400 --> 00:29:46,950
all the same then it's again instead of

808
00:29:46,950 --> 00:29:48,630
storing that unique value over and over

809
00:29:48,630 --> 00:29:50,160
again for every single tuple I can

810
00:29:50,160 --> 00:29:52,410
instead store a triplet that says hey

811
00:29:52,410 --> 00:29:54,990
here's this value I met this all set in

812
00:29:54,990 --> 00:29:56,490
the column and here's number of times

813
00:29:56,490 --> 00:29:59,160
that has been repeated and then once you

814
00:29:59,160 --> 00:30:00,690
how I'll have a different value that's

815
00:30:00,690 --> 00:30:02,370
different than what's in that run then

816
00:30:02,370 --> 00:30:03,840
you have another triplet that records

817
00:30:03,840 --> 00:30:07,740
that information right so we'll see this

818
00:30:07,740 --> 00:30:09,840
in the next slide but we talked about

819
00:30:09,840 --> 00:30:12,870
store sorting columns column stores last

820
00:30:12,870 --> 00:30:16,110
last time with Vertica the rumble

821
00:30:16,110 --> 00:30:17,760
encoding the benefit or the compression

822
00:30:17,760 --> 00:30:19,650
ratio you can get with running encoding

823
00:30:19,650 --> 00:30:21,690
is gonna be amazing if you're able to

824
00:30:21,690 --> 00:30:22,980
pre sort your columns in such a way that

825
00:30:22,980 --> 00:30:25,500
you have these long runs of repeated

826
00:30:25,500 --> 00:30:28,440
values so let's see it a really simple

827
00:30:28,440 --> 00:30:30,570
example so we have a table here of

828
00:30:30,570 --> 00:30:34,050
people with their IDs and their sex and

829
00:30:34,050 --> 00:30:36,570
so we want to do run length encoding for

830
00:30:36,570 --> 00:30:39,180
the sex column because assuming that

831
00:30:39,180 --> 00:30:41,330
there's only two two sexes male female

832
00:30:41,330 --> 00:30:43,650
there's only two values we could ever

833
00:30:43,650 --> 00:30:45,300
have so we're gonna we know we're having

834
00:30:45,300 --> 00:30:46,740
much of these repeats over and over

835
00:30:46,740 --> 00:30:47,490
again

836
00:30:47,490 --> 00:30:48,929
so if you wanted to run length encoding

837
00:30:48,929 --> 00:30:51,090
now we can store this as a bunch of

838
00:30:51,090 --> 00:30:53,220
triplets where for each triplet would

839
00:30:53,220 --> 00:30:55,409
have what the original value was what

840
00:30:55,409 --> 00:30:57,659
are all set is in the column and then

841
00:30:57,659 --> 00:31:01,169
how many values got repeated right so in

842
00:31:01,169 --> 00:31:03,330
this case here the you know for the

843
00:31:03,330 --> 00:31:05,820
first one I have three M's one two three

844
00:31:05,820 --> 00:31:09,059
and so in this triplet I have M starting

845
00:31:09,059 --> 00:31:11,850
at position zero three times so now if I

846
00:31:11,850 --> 00:31:13,260
want to do a lookup and say alright well

847
00:31:13,260 --> 00:31:16,470
what is the sex of the the tuple at

848
00:31:16,470 --> 00:31:19,049
offset three then I would scan through

849
00:31:19,049 --> 00:31:20,370
this and say all right with a boundary

850
00:31:20,370 --> 00:31:21,720
of what I'm looking for it can be found

851
00:31:21,720 --> 00:31:23,490
in this this triplet and therefore I

852
00:31:23,490 --> 00:31:26,399
know what the value is you know other

853
00:31:26,399 --> 00:31:28,080
obvious optimizations too and now if

854
00:31:28,080 --> 00:31:29,929
you're doing like a group by count

855
00:31:29,929 --> 00:31:32,100
instead of again having to look at every

856
00:31:32,100 --> 00:31:34,649
single value in the entire column I

857
00:31:34,649 --> 00:31:37,200
Ignace rip through and just look at the

858
00:31:37,200 --> 00:31:40,770
count and the and and and the value and

859
00:31:40,770 --> 00:31:43,080
populate my my hash table between my

860
00:31:43,080 --> 00:31:46,620
aggregate very efficiently that way so

861
00:31:46,620 --> 00:31:48,000
what's one problem though with this

862
00:31:48,000 --> 00:31:50,580
example well one problem this example

863
00:31:50,580 --> 00:31:52,770
here is that it's actually not we're

864
00:31:52,770 --> 00:31:54,059
actually not getting a good compression

865
00:31:54,059 --> 00:31:55,830
ratio right for this small example but

866
00:31:55,830 --> 00:31:57,659
you'd imagine it if the table is you

867
00:31:57,659 --> 00:32:00,630
know billions of millions of people then

868
00:32:00,630 --> 00:32:02,610
this could be problematic because we

869
00:32:02,610 --> 00:32:04,289
have like alternating patterns of

870
00:32:04,289 --> 00:32:06,380
male-female over and over again right

871
00:32:06,380 --> 00:32:10,710
and so to store one tuple in one

872
00:32:10,710 --> 00:32:12,149
instance of this of this value here

873
00:32:12,149 --> 00:32:14,309
female I have to store a full triplet

874
00:32:14,309 --> 00:32:17,370
right because the run is only the run

875
00:32:17,370 --> 00:32:20,370
length is one so what I've quoted stored

876
00:32:20,370 --> 00:32:22,590
as maybe just eight bits over here now

877
00:32:22,590 --> 00:32:25,380
I'm storing as assuming this is like you

878
00:32:25,380 --> 00:32:27,659
know 64 bits for each one that's a bit

879
00:32:27,659 --> 00:32:30,059
small but you know and it takes me more

880
00:32:30,059 --> 00:32:31,860
space to store the triplet than I did

881
00:32:31,860 --> 00:32:35,760
the original value so but if I sort it

882
00:32:35,760 --> 00:32:38,190
now and now I have all the males first

883
00:32:38,190 --> 00:32:40,320
followed by all the females now when I

884
00:32:40,320 --> 00:32:41,640
do run length encoding and I'm getting

885
00:32:41,640 --> 00:32:42,960
I'm getting an amazing depression ratio

886
00:32:42,960 --> 00:32:45,149
because I just have one triplet and say

887
00:32:45,149 --> 00:32:47,220
here's all the males with the length of

888
00:32:47,220 --> 00:32:48,929
six here's all the females of the length

889
00:32:48,929 --> 00:32:52,529
of two and again now you imagine in a

890
00:32:52,529 --> 00:32:55,529
large column with a million people this

891
00:32:55,529 --> 00:32:56,850
thing if I do it this way it's gonna be

892
00:32:56,850 --> 00:32:57,200
amazing

893
00:32:57,200 --> 00:32:59,110
yes

894
00:32:59,110 --> 00:33:01,750
normally when it's uncompressed like you

895
00:33:01,750 --> 00:33:05,640
got like a one-to-one mapping like oh

896
00:33:10,230 --> 00:33:11,500
yeah

897
00:33:11,500 --> 00:33:13,539
this question is when it's uncompressed

898
00:33:13,539 --> 00:33:16,450
if I'm if I met like this offset here at

899
00:33:16,450 --> 00:33:18,370
6 and I want to know what what is what

900
00:33:18,370 --> 00:33:20,110
is the sex of this person I know how to

901
00:33:20,110 --> 00:33:21,850
jump to this offset and just see it but

902
00:33:21,850 --> 00:33:24,549
in this world I if I'm here I gotta then

903
00:33:24,549 --> 00:33:26,080
go scam from the beginning to figure out

904
00:33:26,080 --> 00:33:29,260
where I belong I were at what offset my

905
00:33:29,260 --> 00:33:32,559
boundary can be founded correct so one

906
00:33:32,559 --> 00:33:34,630
is think of this like I'm doing I'm

907
00:33:34,630 --> 00:33:36,399
doing I'm encoding per block so it's not

908
00:33:36,399 --> 00:33:38,260
like I'm scanning the entire tuple right

909
00:33:38,260 --> 00:33:40,750
and the other thing is like for OLAP

910
00:33:40,750 --> 00:33:43,299
queries know you really don't care what

911
00:33:43,299 --> 00:33:44,710
is the value for this single tuple I

912
00:33:44,710 --> 00:33:46,779
don't care what a nice X is I care to

913
00:33:46,779 --> 00:33:47,980
know what is the distribution of sexes

914
00:33:47,980 --> 00:33:49,299
across the entire university or the

915
00:33:49,299 --> 00:33:50,679
class or something like that yeah so

916
00:33:50,679 --> 00:33:52,090
you're doing full scans and that in that

917
00:33:52,090 --> 00:33:54,960
world it's fast

918
00:33:57,610 --> 00:33:59,799
I said overall makes up for it mean for

919
00:33:59,799 --> 00:34:01,149
the workload we're trying to target like

920
00:34:01,149 --> 00:34:03,669
the analytical operations this this

921
00:34:03,669 --> 00:34:06,010
would be a big win for OLTP when I'm

922
00:34:06,010 --> 00:34:07,899
saying go get this one record this is

923
00:34:07,899 --> 00:34:12,690
gonna be this this could be back yes

924
00:34:13,260 --> 00:34:15,820
this question is are our compression

925
00:34:15,820 --> 00:34:17,649
schemes using HTF systems yeah so this

926
00:34:17,649 --> 00:34:19,629
is like if you do the fraction mirror

927
00:34:19,629 --> 00:34:21,580
push we told it last time like Olga does

928
00:34:21,580 --> 00:34:22,929
this they had the the column stores

929
00:34:22,929 --> 00:34:24,969
compressed because you're almost never

930
00:34:24,969 --> 00:34:27,489
updating that and then you have the the

931
00:34:27,489 --> 00:34:30,609
roast or the uncompressed right so in a

932
00:34:30,609 --> 00:34:32,230
chat systems you can do this in our

933
00:34:32,230 --> 00:34:36,639
system what we do is we have a slightly

934
00:34:36,639 --> 00:34:39,699
different layout of the blocks when it's

935
00:34:39,699 --> 00:34:41,949
the data is cold so we actually do

936
00:34:41,949 --> 00:34:43,330
dictionary compression for the blocks

937
00:34:43,330 --> 00:34:46,889
when it's not being modified

938
00:34:54,899 --> 00:34:57,160
change button values the world if you

939
00:34:57,160 --> 00:34:59,440
think you have to change the ATM one

940
00:34:59,440 --> 00:35:02,380
fraction mirrors yeah so member from not

941
00:35:02,380 --> 00:35:03,670
everyone was in intro class but they

942
00:35:03,670 --> 00:35:05,380
they talked about this last clip today

943
00:35:05,380 --> 00:35:06,730
money talk about it today they talk

944
00:35:06,730 --> 00:35:07,840
about how like me a little put a little

945
00:35:07,840 --> 00:35:10,060
marker in on the fraction mirror side

946
00:35:10,060 --> 00:35:11,260
and the column sort of says this thing

947
00:35:11,260 --> 00:35:13,450
got modified don't go look here for this

948
00:35:13,450 --> 00:35:16,210
exact value go look in a row store yes

949
00:35:16,210 --> 00:35:20,860
if they handle that in peloton

950
00:35:20,860 --> 00:35:23,110
everything was all sort of an in in in

951
00:35:23,110 --> 00:35:24,970
the current system everything was always

952
00:35:24,970 --> 00:35:28,690
a a single storage base and it's just

953
00:35:28,690 --> 00:35:30,220
that the the layout of the data may

954
00:35:30,220 --> 00:35:31,930
change this was felt on the layout video

955
00:35:31,930 --> 00:35:34,570
may change if it was a roast or column

956
00:35:34,570 --> 00:35:36,790
store if it was hot or cold and we were

957
00:35:36,790 --> 00:35:39,580
doing a pen only MPC C so if you modify

958
00:35:39,580 --> 00:35:42,850
something in the column store then that

959
00:35:42,850 --> 00:35:45,280
version chain would then get updated to

960
00:35:45,280 --> 00:35:48,760
now point to the row store in our in our

961
00:35:48,760 --> 00:35:50,200
current system that secession we do the

962
00:35:50,200 --> 00:35:51,760
same thing to right we have a block of

963
00:35:51,760 --> 00:35:53,470
data that's compressed it can't be

964
00:35:53,470 --> 00:35:55,270
modified it's a beautiful so if you

965
00:35:55,270 --> 00:35:58,660
modify a tuple we instead of appending

966
00:35:58,660 --> 00:35:59,800
the virgin chain exactly where that is

967
00:35:59,800 --> 00:36:01,720
and now gets moved over some results we

968
00:36:01,720 --> 00:36:02,770
treated like a delete followed by an

969
00:36:02,770 --> 00:36:07,930
insert okay so the cool thing about our

970
00:36:07,930 --> 00:36:11,200
run length encoding or le I think I also

971
00:36:11,200 --> 00:36:14,050
did Vertigo's probably the most of all

972
00:36:14,050 --> 00:36:15,280
the columns for systems that out there

973
00:36:15,280 --> 00:36:16,750
they're the most one of the biggest

974
00:36:16,750 --> 00:36:18,100
heavy users or proponents on this

975
00:36:18,100 --> 00:36:19,870
approach most other systems we're going

976
00:36:19,870 --> 00:36:22,300
to do a dictionary encoding but the cool

977
00:36:22,300 --> 00:36:24,550
thing about RLE is that we actually can

978
00:36:24,550 --> 00:36:26,410
apply this in conjunction with other

979
00:36:26,410 --> 00:36:28,420
compression schemes and we get this like

980
00:36:28,420 --> 00:36:30,490
nice multiplicative effect we're like we

981
00:36:30,490 --> 00:36:31,950
can then compress the compress data

982
00:36:31,950 --> 00:36:33,690
which is kind of cool

983
00:36:33,690 --> 00:36:37,060
so with bitmap and coding the idea is

984
00:36:37,060 --> 00:36:39,970
that it's a bit so we're not we're gonna

985
00:36:39,970 --> 00:36:41,620
show how to do orally on bitmaps in a

986
00:36:41,620 --> 00:36:43,180
second but like this is another pressure

987
00:36:43,180 --> 00:36:44,620
scheme you could do where then you can

988
00:36:44,620 --> 00:36:46,060
also apply r-la but you can also apply

989
00:36:46,060 --> 00:36:48,730
orally for delta encoding so with bitmap

990
00:36:48,730 --> 00:36:50,920
decoding the idea is that it's a

991
00:36:50,920 --> 00:36:52,660
different way of organizing the datum

992
00:36:52,660 --> 00:36:55,930
where instead of storing for every

993
00:36:55,930 --> 00:36:59,110
single for a column you're storing the

994
00:36:59,110 --> 00:37:01,180
individual in attributes you're instead

995
00:37:01,180 --> 00:37:03,370
gonna have a separate bitmap for every

996
00:37:03,370 --> 00:37:05,290
single unique value that a column could

997
00:37:05,290 --> 00:37:07,300
have and then that

998
00:37:07,300 --> 00:37:09,340
way if I want to see this there's this

999
00:37:09,340 --> 00:37:11,530
attribute this offset had this value I

1000
00:37:11,530 --> 00:37:13,360
look in the corresponding bitmap and I

1001
00:37:13,360 --> 00:37:15,490
check to see whether it's it's 0 or 1 at

1002
00:37:15,490 --> 00:37:18,940
that position right so this is only

1003
00:37:18,940 --> 00:37:21,730
gonna work if the value cardinality is

1004
00:37:21,730 --> 00:37:23,430
low for the thing I'm trying to compress

1005
00:37:23,430 --> 00:37:25,930
so if I have a small number of possible

1006
00:37:25,930 --> 00:37:27,790
attribute I could be because I need to

1007
00:37:27,790 --> 00:37:29,320
have a bitmap for every single one then

1008
00:37:29,320 --> 00:37:31,180
this could be a big win but if I have a

1009
00:37:31,180 --> 00:37:32,470
lot of different possible batteries or

1010
00:37:32,470 --> 00:37:34,840
every single value is unique then this

1011
00:37:34,840 --> 00:37:36,160
is gonna be a terrible idea because I'm

1012
00:37:36,160 --> 00:37:38,080
gonna negative compression because my my

1013
00:37:38,080 --> 00:37:39,790
compressed data is gonna be larger than

1014
00:37:39,790 --> 00:37:42,010
the uncompressed data which is which is

1015
00:37:42,010 --> 00:37:45,820
obviously bad so as I say before it

1016
00:37:45,820 --> 00:37:47,830
we're gonna break things of the Davis up

1017
00:37:47,830 --> 00:37:50,080
to blocks or chunks so that we don't

1018
00:37:50,080 --> 00:37:51,670
that allocate these super large bit

1019
00:37:51,670 --> 00:37:53,800
bitmaps right we're have a bitmaps per

1020
00:37:53,800 --> 00:37:55,990
block so going back to our example here

1021
00:37:55,990 --> 00:37:58,750
we want a bitmap compress the the

1022
00:37:58,750 --> 00:38:01,390
sex-filled so again we only have two

1023
00:38:01,390 --> 00:38:03,040
values male or female

1024
00:38:03,040 --> 00:38:04,900
so in our compressed version we would

1025
00:38:04,900 --> 00:38:07,150
have a bitmap for the male a value and a

1026
00:38:07,150 --> 00:38:09,070
pit map for the female value and again

1027
00:38:09,070 --> 00:38:10,810
if I want to know what the value is at

1028
00:38:10,810 --> 00:38:13,240
this offset I just look in the in the

1029
00:38:13,240 --> 00:38:15,580
bitmap the bitmaps at this offset and I

1030
00:38:15,580 --> 00:38:20,770
check to see whether 0 1 right so this

1031
00:38:20,770 --> 00:38:23,530
starts to be a big win so in my little

1032
00:38:23,530 --> 00:38:25,180
toy example here I have 9 values

1033
00:38:25,180 --> 00:38:27,880
assuming I can store the the male female

1034
00:38:27,880 --> 00:38:30,580
as as a single byte or 8 bits so this is

1035
00:38:30,580 --> 00:38:33,100
gonna be 72 bits but for this one here

1036
00:38:33,100 --> 00:38:35,320
in the compressed version I have two

1037
00:38:35,320 --> 00:38:35,980
bitmaps

1038
00:38:35,980 --> 00:38:39,250
so that's each one bit piece one bit per

1039
00:38:39,250 --> 00:38:41,740
value that I have has 9 times 2 so

1040
00:38:41,740 --> 00:38:43,750
that's me 18 bits then I need to store

1041
00:38:43,750 --> 00:38:45,730
the original values but I only store

1042
00:38:45,730 --> 00:38:47,590
them once all right so that's gonna be 2

1043
00:38:47,590 --> 00:38:51,280
times 8 bits so 16 bytes plus 18 bytes

1044
00:38:51,280 --> 00:38:54,670
is what is 34 bytes so I can store my

1045
00:38:54,670 --> 00:38:56,950
original date with 72 bits I can get it

1046
00:38:56,950 --> 00:39:00,520
down to 34 bits again now extrapolate

1047
00:39:00,520 --> 00:39:02,080
across millions of users

1048
00:39:02,080 --> 00:39:15,070
this has to be a big deal yes your

1049
00:39:15,070 --> 00:39:19,120
question is why not store oh I think

1050
00:39:19,120 --> 00:39:21,400
what you're saying is this is the binary

1051
00:39:21,400 --> 00:39:23,470
male female so instead of storing

1052
00:39:23,470 --> 00:39:31,870
separate pitmaster 1 like yes oh his

1053
00:39:31,870 --> 00:39:37,810
David is if you have with with vectorize

1054
00:39:37,810 --> 00:39:40,450
instructions instead of storing this as

1055
00:39:40,450 --> 00:39:41,950
a separate bitmap for each of them

1056
00:39:41,950 --> 00:39:44,950
couldn't I just or a essentially doing

1057
00:39:44,950 --> 00:39:49,810
dictionary encoding storing a taking a

1058
00:39:49,810 --> 00:39:52,930
potentially larger size value and

1059
00:39:52,930 --> 00:39:54,730
compressing it down to a smaller

1060
00:39:54,730 --> 00:39:57,340
encoding store those contiguously and

1061
00:39:57,340 --> 00:39:59,740
then run sim D on that that's ridiculous

1062
00:39:59,740 --> 00:40:04,860
all right this is an alternative yeah

1063
00:40:05,310 --> 00:40:07,420
the other cool thing you actually can do

1064
00:40:07,420 --> 00:40:11,830
I don't think you could do this in the

1065
00:40:11,830 --> 00:40:12,970
dictionary codings that what you said

1066
00:40:12,970 --> 00:40:15,570
but one thing you can do is like we

1067
00:40:15,570 --> 00:40:17,530
already know that in the query optimizer

1068
00:40:17,530 --> 00:40:19,300
we're going to we're going to evaluate

1069
00:40:19,300 --> 00:40:20,980
the predicate s' than in our where

1070
00:40:20,980 --> 00:40:23,050
clause so that we try the more selective

1071
00:40:23,050 --> 00:40:24,400
ones first and we start throwing out

1072
00:40:24,400 --> 00:40:26,500
data as quickly as possible but this is

1073
00:40:26,500 --> 00:40:27,760
actually little bit different now to we

1074
00:40:27,760 --> 00:40:31,960
can actually order the the the the way

1075
00:40:31,960 --> 00:40:33,760
in which we check the bitmap so we know

1076
00:40:33,760 --> 00:40:36,640
one bitmap is super selective within a

1077
00:40:36,640 --> 00:40:38,500
single attribute we can actually check

1078
00:40:38,500 --> 00:40:40,780
that one at first that brings a hug a

1079
00:40:40,780 --> 00:40:43,030
whole nother level of optimization which

1080
00:40:43,030 --> 00:40:44,440
is totally different than we talked

1081
00:40:44,440 --> 00:40:46,210
about for I don't think we'll talk about

1082
00:40:46,210 --> 00:40:48,580
other bitmap encoding schemes but this

1083
00:40:48,580 --> 00:40:50,320
is there's other techniques you can do

1084
00:40:50,320 --> 00:40:53,200
that are really kind of cool if you

1085
00:40:53,200 --> 00:40:54,670
start encoding your values as bitmaps

1086
00:40:54,670 --> 00:40:57,130
not exactly in this way we're scribing

1087
00:40:57,130 --> 00:40:58,780
here but like you know if I'm looking

1088
00:40:58,780 --> 00:41:02,320
for values that are greater than some

1089
00:41:02,320 --> 00:41:03,550
you know looking for values that are

1090
00:41:03,550 --> 00:41:06,100
greater than some given value if these

1091
00:41:06,100 --> 00:41:07,870
things are sorted in such a way that I

1092
00:41:07,870 --> 00:41:09,040
know that you know I can look at the

1093
00:41:09,040 --> 00:41:11,290
first bitmap first and I could throw

1094
00:41:11,290 --> 00:41:12,400
away more data by looking at that

1095
00:41:12,400 --> 00:41:15,100
because that's more selective I'll try

1096
00:41:15,100 --> 00:41:15,940
to maybe cover that when talk about

1097
00:41:15,940 --> 00:41:19,060
vectorization later on alright so this

1098
00:41:19,060 --> 00:41:20,260
is this was a good good

1099
00:41:20,260 --> 00:41:22,359
this is an obvious win we all know if

1100
00:41:22,359 --> 00:41:24,550
it's male/female they we're gonna be

1101
00:41:24,550 --> 00:41:26,619
able to press it down by a lot let's do

1102
00:41:26,619 --> 00:41:28,180
example we're bitmap encoding could be a

1103
00:41:28,180 --> 00:41:30,609
bad idea so let's say now I have a table

1104
00:41:30,609 --> 00:41:32,859
of customers and I had the zip code

1105
00:41:32,859 --> 00:41:34,270
field here I want to try to bitmap

1106
00:41:34,270 --> 00:41:36,609
encoding on this so assume in my table I

1107
00:41:36,609 --> 00:41:38,770
have 10 million people and in the United

1108
00:41:38,770 --> 00:41:40,869
States there's roughly around 43,000 ZIP

1109
00:41:40,869 --> 00:41:43,350
codes it's really like 41,000 I think

1110
00:41:43,350 --> 00:41:46,660
but assuming that's the case so if my

1111
00:41:46,660 --> 00:41:48,760
uncompressed data but I have 10 million

1112
00:41:48,760 --> 00:41:50,950
people and assume I can store stored the

1113
00:41:50,950 --> 00:41:53,500
zip code is just 32 bits then the total

1114
00:41:53,500 --> 00:41:54,490
mount of space I'm storing for this

1115
00:41:54,490 --> 00:41:56,980
column is just 40 megabytes but now if I

1116
00:41:56,980 --> 00:41:58,930
do bitmap encoding I need to have a

1117
00:41:58,930 --> 00:42:02,320
bitmap for every scene single unique ZIP

1118
00:42:02,320 --> 00:42:04,030
code in the United States and that

1119
00:42:04,030 --> 00:42:06,040
bitmap needs to be 10 million bits long

1120
00:42:06,040 --> 00:42:07,540
because I need to represent for every

1121
00:42:07,540 --> 00:42:09,310
single to blow to my table so now the

1122
00:42:09,310 --> 00:42:12,820
total size would be 53 gigabytes right

1123
00:42:12,820 --> 00:42:14,320
so again this obviously be super stupid

1124
00:42:14,320 --> 00:42:17,260
to do so bitmap and Cody can get great

1125
00:42:17,260 --> 00:42:18,970
compression ratio but this is something

1126
00:42:18,970 --> 00:42:21,280
that like a human has to decide am I is

1127
00:42:21,280 --> 00:42:22,840
this the right way to actually do my

1128
00:42:22,840 --> 00:42:26,440
encoding so for this reason I don't

1129
00:42:26,440 --> 00:42:28,210
think bitmap coding is commonly used

1130
00:42:28,210 --> 00:42:30,310
it's only really you only really see the

1131
00:42:30,310 --> 00:42:31,840
Sinister the high-end commercial OLAP

1132
00:42:31,840 --> 00:42:34,420
systems this is not really an issue we

1133
00:42:34,420 --> 00:42:35,650
organize things plots we could imagine

1134
00:42:35,650 --> 00:42:38,340
if now I start serving tuples in two

1135
00:42:38,340 --> 00:42:41,369
different offsets in my bitmap I got

1136
00:42:41,369 --> 00:42:43,240
everything else and moving them down the

1137
00:42:43,240 --> 00:42:44,560
same problem we saw last time and that

1138
00:42:44,560 --> 00:42:45,760
can get really expensive if I have

1139
00:42:45,760 --> 00:42:49,320
43,000 bitmaps just from one attribute

1140
00:42:49,320 --> 00:42:54,100
so one thing we can do though in

1141
00:42:54,100 --> 00:42:56,230
addition to just doing that I'm coding

1142
00:42:56,230 --> 00:42:58,990
these bitmaps are just fight sequences

1143
00:42:58,990 --> 00:43:01,030
right there's ones and zeros like like

1144
00:43:01,030 --> 00:43:03,340
finish or like like all datum we

1145
00:43:03,340 --> 00:43:04,660
actually can do compression on the

1146
00:43:04,660 --> 00:43:06,700
bitmaps themselves to get even better

1147
00:43:06,700 --> 00:43:11,619
compression so the most simplest way to

1148
00:43:11,619 --> 00:43:13,660
do this is just run that same snappy or

1149
00:43:13,660 --> 00:43:16,660
lz4 or gzip algorithms we talked about

1150
00:43:16,660 --> 00:43:18,760
before take our bitmap that we're

1151
00:43:18,760 --> 00:43:20,859
storing and didn't run that through our

1152
00:43:20,859 --> 00:43:22,420
compression scheme and then we could end

1153
00:43:22,420 --> 00:43:23,710
up actually with pretty good results

1154
00:43:23,710 --> 00:43:25,300
because again most of the times it's

1155
00:43:25,300 --> 00:43:27,280
gonna be a bunch of zeros and these

1156
00:43:27,280 --> 00:43:29,770
algorithms are you know that's that's

1157
00:43:29,770 --> 00:43:32,380
the ideal case for for compressing bit

1158
00:43:32,380 --> 00:43:33,800
sequences

1159
00:43:33,800 --> 00:43:35,060
again we're not going to do this for an

1160
00:43:35,060 --> 00:43:36,500
enemy database because this is gonna be

1161
00:43:36,500 --> 00:43:37,640
expensive because now I have single time

1162
00:43:37,640 --> 00:43:38,540
I gotta do a low couple when these

1163
00:43:38,540 --> 00:43:40,100
bitmaps I got to decompress it and

1164
00:43:40,100 --> 00:43:41,990
that's gonna be slow you could say well

1165
00:43:41,990 --> 00:43:43,460
let me combine together a bunch of

1166
00:43:43,460 --> 00:43:45,740
bitmaps in my zip code example from zip

1167
00:43:45,740 --> 00:43:47,240
codes where nobody lives in like Montana

1168
00:43:47,240 --> 00:43:50,240
and Wyoming and I combine those together

1169
00:43:50,240 --> 00:43:51,470
and most the times they don't need to go

1170
00:43:51,470 --> 00:43:52,820
look at those but when I do then I have

1171
00:43:52,820 --> 00:43:54,920
to decompress them yes you could do that

1172
00:43:54,920 --> 00:43:56,990
but as far as I know this is a lot of

1173
00:43:56,990 --> 00:43:58,340
engineering overhead and no system

1174
00:43:58,340 --> 00:44:00,980
actually does this what you do see

1175
00:44:00,980 --> 00:44:03,950
instead though is to do again sort of

1176
00:44:03,950 --> 00:44:06,380
native database compression on the

1177
00:44:06,380 --> 00:44:08,870
bitmap codes where you know you're gonna

1178
00:44:08,870 --> 00:44:10,250
have a bunch of zeros and you can try to

1179
00:44:10,250 --> 00:44:11,690
take advantage of that and we use a

1180
00:44:11,690 --> 00:44:16,760
variant of a run length encoding so I'm

1181
00:44:16,760 --> 00:44:18,560
gonna describe now one technique that

1182
00:44:18,560 --> 00:44:20,450
Oracle used to do called vitelline

1183
00:44:20,450 --> 00:44:24,380
bitmap codes or BBC's the spoiler would

1184
00:44:24,380 --> 00:44:25,640
be there they don't actually use this

1185
00:44:25,640 --> 00:44:28,070
anymore and explain why at the end but

1186
00:44:28,070 --> 00:44:31,460
this is I think a good example of the

1187
00:44:31,460 --> 00:44:34,490
sort of low-level bit it's kind of bit

1188
00:44:34,490 --> 00:44:35,600
you know bit level compression you can

1189
00:44:35,600 --> 00:44:37,520
do on bitmaps if you know you're gonna

1190
00:44:37,520 --> 00:44:39,440
have a lot of zeros so I like this

1191
00:44:39,440 --> 00:44:40,460
because they're from my opinion this

1192
00:44:40,460 --> 00:44:42,560
this is easy to understand and then you

1193
00:44:42,560 --> 00:44:44,000
can then build upon this and do more

1194
00:44:44,000 --> 00:44:45,620
sophisticated things or things that are

1195
00:44:45,620 --> 00:44:49,340
better for today's hardware so again

1196
00:44:49,340 --> 00:44:50,810
assume we're doing bitmap coding so we

1197
00:44:50,810 --> 00:44:52,400
have these these long bitmaps for our

1198
00:44:52,400 --> 00:44:55,310
column and we want to compress it down

1199
00:44:55,310 --> 00:44:57,470
to B so that takes less space and again

1200
00:44:57,470 --> 00:44:59,150
most of the bits are going to be zeros

1201
00:44:59,150 --> 00:45:01,640
so we're going to organize it into these

1202
00:45:01,640 --> 00:45:04,730
chunks where we have a bunch of a bunch

1203
00:45:04,730 --> 00:45:07,040
of bytes whether all zeros followed by

1204
00:45:07,040 --> 00:45:08,870
some bytes whether there are some ones

1205
00:45:08,870 --> 00:45:11,750
so they're gonna designate any byte that

1206
00:45:11,750 --> 00:45:14,270
has all zeros as a gap byte and then any

1207
00:45:14,270 --> 00:45:16,910
byte that has a 1 in it in any position

1208
00:45:16,910 --> 00:45:19,610
they'll call that a tail byte and so

1209
00:45:19,610 --> 00:45:21,110
that we're doing we're gonna encode a

1210
00:45:21,110 --> 00:45:23,480
chunk where we have a bunch of gap bytes

1211
00:45:23,480 --> 00:45:26,260
followed by some number of tail bytes

1212
00:45:26,260 --> 00:45:29,510
and we're gonna break it up and compress

1213
00:45:29,510 --> 00:45:30,230
it that way

1214
00:45:30,230 --> 00:45:32,090
so the gap bytes will compress with run

1215
00:45:32,090 --> 00:45:33,890
length encoding and then for the tail

1216
00:45:33,890 --> 00:45:35,780
bytes where there are there is a 1 in it

1217
00:45:35,780 --> 00:45:39,470
will keep track of either where that one

1218
00:45:39,470 --> 00:45:41,390
is located if it's a special case we

1219
00:45:41,390 --> 00:45:43,780
just have one bite or weirdest or them

1220
00:45:43,780 --> 00:45:46,190
uncompressed or in in verbatim as they

1221
00:45:46,190 --> 00:45:47,180
call it

1222
00:45:47,180 --> 00:45:49,400
so let's look at visual example so say

1223
00:45:49,400 --> 00:45:52,160
this is my bitmap here and we can see

1224
00:45:52,160 --> 00:45:54,050
that we it's all zeros except for this

1225
00:45:54,050 --> 00:45:56,480
byte up here we have one one and then

1226
00:45:56,480 --> 00:45:58,160
this byte here these two bytes here

1227
00:45:58,160 --> 00:45:59,660
there's a one here and there's two ones

1228
00:45:59,660 --> 00:46:02,060
in this one so again what they're going

1229
00:46:02,060 --> 00:46:03,800
to do they're going to break up the the

1230
00:46:03,800 --> 00:46:07,100
bitmap into segments or chunks where we

1231
00:46:07,100 --> 00:46:08,810
have a bunch of gap bytes where they're

1232
00:46:08,810 --> 00:46:10,880
all zeros followed by some byte where

1233
00:46:10,880 --> 00:46:13,010
there's at least one one so for the

1234
00:46:13,010 --> 00:46:15,080
first one here again here's our gap

1235
00:46:15,080 --> 00:46:16,910
bytes that are all zeros and then here's

1236
00:46:16,910 --> 00:46:18,350
our tail byte where there's one one in

1237
00:46:18,350 --> 00:46:20,720
it so the way we're gonna encode this

1238
00:46:20,720 --> 00:46:22,550
all right here's the other one right

1239
00:46:22,550 --> 00:46:24,770
because we have to have down here so way

1240
00:46:24,770 --> 00:46:26,300
we're gonna coat this first one is that

1241
00:46:26,300 --> 00:46:28,880
we always have to have a header byte and

1242
00:46:28,880 --> 00:46:30,410
the header blights gonna tell us what is

1243
00:46:30,410 --> 00:46:34,430
in our our chunk so the first 3 bits are

1244
00:46:34,430 --> 00:46:35,900
gonna be organized to tell us how many

1245
00:46:35,900 --> 00:46:39,050
gap bytes that we have up into seven so

1246
00:46:39,050 --> 00:46:41,150
we can begin the code and this port this

1247
00:46:41,150 --> 00:46:42,680
portion of this header but you say we

1248
00:46:42,680 --> 00:46:45,230
have at most seven gap bytes in our

1249
00:46:45,230 --> 00:46:46,910
chunk and we have more than we have to

1250
00:46:46,910 --> 00:46:48,770
do something else so in this case here

1251
00:46:48,770 --> 00:46:51,440
we have two one two gap bytes so we say

1252
00:46:51,440 --> 00:46:52,880
record that we just have the number two

1253
00:46:52,880 --> 00:46:56,690
here then in the next bit this is a

1254
00:46:56,690 --> 00:46:58,730
special flag that says that if this

1255
00:46:58,730 --> 00:47:00,500
thing is set to true then we know

1256
00:47:00,500 --> 00:47:02,030
whatever tail byte we're looking at we

1257
00:47:02,030 --> 00:47:04,250
only have one in our chunk and that it

1258
00:47:04,250 --> 00:47:08,120
only has one one in it so this is saying

1259
00:47:08,120 --> 00:47:09,710
if this is set to one so we know that

1260
00:47:09,710 --> 00:47:11,480
it's a special case so then the next

1261
00:47:11,480 --> 00:47:13,940
four bytes tells us at what position in

1262
00:47:13,940 --> 00:47:17,120
our byte that one is located in so in

1263
00:47:17,120 --> 00:47:19,310
this one we're encoding for so in our

1264
00:47:19,310 --> 00:47:21,920
tail byte we have one two three four and

1265
00:47:21,920 --> 00:47:23,840
that's where the one is located all

1266
00:47:23,840 --> 00:47:25,790
right so we're able to take three bytes

1267
00:47:25,790 --> 00:47:28,550
in our bitmap and encode it down to a

1268
00:47:28,550 --> 00:47:32,270
single byte so that's pretty good let's

1269
00:47:32,270 --> 00:47:34,460
look at the next chunk so now here again

1270
00:47:34,460 --> 00:47:36,650
we have thirteen gap bytes followed by

1271
00:47:36,650 --> 00:47:38,870
two tail bytes so when we write out our

1272
00:47:38,870 --> 00:47:40,700
header we can't record that we have

1273
00:47:40,700 --> 00:47:44,720
thirteen gap bytes in it because we can

1274
00:47:44,720 --> 00:47:46,790
only store up to eight or seven in this

1275
00:47:46,790 --> 00:47:49,400
in this three bits so we set it all to

1276
00:47:49,400 --> 00:47:51,770
ones right and that's going to tell us

1277
00:47:51,770 --> 00:47:53,390
that this is a special case that we need

1278
00:47:53,390 --> 00:47:55,700
to look at the first byte after our

1279
00:47:55,700 --> 00:47:57,500
header and that's gonna tell us the

1280
00:47:57,500 --> 00:47:58,910
length of our cap so we're in court

1281
00:47:58,910 --> 00:48:00,740
thirteen here and that

1282
00:48:00,740 --> 00:48:03,110
represents our 13 cap lights right that

1283
00:48:03,110 --> 00:48:04,340
that's the that's the runtime from

1284
00:48:04,340 --> 00:48:07,220
coding part part then we look at this

1285
00:48:07,220 --> 00:48:09,200
special flag here and again that tells

1286
00:48:09,200 --> 00:48:10,850
us whether we only have one byte has one

1287
00:48:10,850 --> 00:48:11,510
one in it

1288
00:48:11,510 --> 00:48:14,270
this is set to zero so that means that

1289
00:48:14,270 --> 00:48:16,400
in the next four bits we're gonna record

1290
00:48:16,400 --> 00:48:19,070
how many verbatim bytes we have right

1291
00:48:19,070 --> 00:48:20,900
and this is where we encode exactly

1292
00:48:20,900 --> 00:48:22,730
whatever how the bits are found up here

1293
00:48:22,730 --> 00:48:26,119
down in here or just make a straight

1294
00:48:26,119 --> 00:48:40,300
copy of them yes yes yes I know sex yes

1295
00:48:40,300 --> 00:48:42,770
but high level I just saw the same okay

1296
00:48:42,770 --> 00:48:44,990
so in this example here I was able to

1297
00:48:44,990 --> 00:48:46,940
take a bitmap that was 18 bytes and

1298
00:48:46,940 --> 00:48:49,940
compress it down to 5 bytes that's

1299
00:48:49,940 --> 00:48:52,820
pretty impressive right so again I'm I'm

1300
00:48:52,820 --> 00:48:55,310
instead of now storing the the if I'm

1301
00:48:55,310 --> 00:48:57,020
doing bitmap encoding instead of storing

1302
00:48:57,020 --> 00:48:58,910
the the bitmap for every single value in

1303
00:48:58,910 --> 00:49:00,980
its uncompressed form I can take that

1304
00:49:00,980 --> 00:49:04,180
bitmap and compress each one separately

1305
00:49:04,180 --> 00:49:07,180
yes

1306
00:49:15,470 --> 00:49:18,620
[Music]

1307
00:49:23,220 --> 00:49:26,090
and one comprehension having you know

1308
00:49:26,090 --> 00:49:28,680
sighs all right so you're it so you're

1309
00:49:28,680 --> 00:49:29,970
saying rather than throwing these four

1310
00:49:29,970 --> 00:49:37,220
bits here for this sorry do what I

1311
00:49:40,040 --> 00:49:44,990
connect say here's the position that bit

1312
00:49:49,400 --> 00:49:52,500
by logic lying to have a larger gap size

1313
00:49:52,500 --> 00:49:55,740
yeah I think that would that would

1314
00:49:55,740 --> 00:49:59,000
potentially work yes um

1315
00:49:59,300 --> 00:50:05,070
they didn't do that way all right yeah

1316
00:50:05,070 --> 00:50:09,780
I again like I Isis I suspect media for

1317
00:50:09,780 --> 00:50:10,950
a reason rather they looked at they look

1318
00:50:10,950 --> 00:50:12,570
at their bitmaps for their databases

1319
00:50:12,570 --> 00:50:14,010
this is Oracle right so they have real

1320
00:50:14,010 --> 00:50:16,530
customers they have real data and they

1321
00:50:16,530 --> 00:50:18,210
made that decision that this for the

1322
00:50:18,210 --> 00:50:19,590
most cases this is the right way to do

1323
00:50:19,590 --> 00:50:23,880
this I would assume that right alright

1324
00:50:23,880 --> 00:50:26,280
so as I said Oracle doesn't do this

1325
00:50:26,280 --> 00:50:27,780
anymore this is considered an obsolete

1326
00:50:27,780 --> 00:50:30,720
format so the reason is because although

1327
00:50:30,720 --> 00:50:31,589
we're gonna get a great compression

1328
00:50:31,589 --> 00:50:35,070
ratio the it's actually gonna be

1329
00:50:35,070 --> 00:50:37,619
terrible for modern CPUs because there's

1330
00:50:37,619 --> 00:50:40,020
all this branching right as I'm going

1331
00:50:40,020 --> 00:50:42,030
through like in order for me to go

1332
00:50:42,030 --> 00:50:44,099
figure out like you know I'm operating

1333
00:50:44,099 --> 00:50:46,859
on these compressed bytes I had to go

1334
00:50:46,859 --> 00:50:48,570
jump around like oh is this thing all

1335
00:50:48,570 --> 00:50:50,760
ones if so jump over here or this thing

1336
00:50:50,760 --> 00:50:53,040
is is a smaller than that then I no need

1337
00:50:53,040 --> 00:50:54,690
look at this but this is one jump here

1338
00:50:54,690 --> 00:50:56,580
if this is zero do something else right

1339
00:50:56,580 --> 00:50:58,470
that's terrible for modern CPUs because

1340
00:50:58,470 --> 00:51:00,330
it's all those conditionals are going to

1341
00:51:00,330 --> 00:51:01,980
be hard for the the branch predictor on

1342
00:51:01,980 --> 00:51:03,450
the CPU to predict because it's gonna be

1343
00:51:03,450 --> 00:51:04,619
different for every single you know

1344
00:51:04,619 --> 00:51:07,589
chunk sequence and we're gonna have a

1345
00:51:07,589 --> 00:51:08,640
lot of miss prediction and that's going

1346
00:51:08,640 --> 00:51:10,260
to have to cause us to flush our

1347
00:51:10,260 --> 00:51:12,420
extraction pipeline and that would have

1348
00:51:12,420 --> 00:51:15,270
CPU stalls so for this reason they then

1349
00:51:15,270 --> 00:51:17,430
came up with another version of an

1350
00:51:17,430 --> 00:51:18,660
improved version called right-aligned

1351
00:51:18,660 --> 00:51:21,480
hybrid bitmap encoding that's designed

1352
00:51:21,480 --> 00:51:24,690
for modern CPUs the BBC stuff was

1353
00:51:24,690 --> 00:51:26,580
patented but it was like the 90s - it

1354
00:51:26,580 --> 00:51:28,320
might be gone it might be patent expired

1355
00:51:28,320 --> 00:51:30,210
right now this thing is still I think

1356
00:51:30,210 --> 00:51:33,570
patented I've never read it

1357
00:51:33,570 --> 00:51:35,940
database patents and and II don't mix so

1358
00:51:35,940 --> 00:51:36,930
we

1359
00:51:36,930 --> 00:51:38,820
we don't read anything like that but it

1360
00:51:38,820 --> 00:51:40,230
they do have something out there that

1361
00:51:40,230 --> 00:51:42,690
does this again the other thing to point

1362
00:51:42,690 --> 00:51:45,290
out too is that in all these cases the

1363
00:51:45,290 --> 00:51:47,640
this bitmap encoding scheme it doesn't

1364
00:51:47,640 --> 00:51:49,530
support random access that's okay if

1365
00:51:49,530 --> 00:51:51,660
we're doing OLAP queries because we're

1366
00:51:51,660 --> 00:51:52,890
gonna scan from beginning to end but if

1367
00:51:52,890 --> 00:51:54,540
we had we want to say find at this

1368
00:51:54,540 --> 00:51:56,550
position whether the bitmap is set to 1

1369
00:51:56,550 --> 00:51:58,650
or 0 we have to start from the beginning

1370
00:51:58,650 --> 00:52:02,310
and decompress everything and so this is

1371
00:52:02,310 --> 00:52:04,200
not gonna work well for OLTP for OLAP

1372
00:52:04,200 --> 00:52:07,530
this is fine alright so let's go to some

1373
00:52:07,530 --> 00:52:09,480
other crushing schemes so Delta encoding

1374
00:52:09,480 --> 00:52:11,880
the idea here is that if we know that

1375
00:52:11,880 --> 00:52:15,930
values within the same column are going

1376
00:52:15,930 --> 00:52:17,880
to be have a small difference in between

1377
00:52:17,880 --> 00:52:20,010
them then instead of storing again a

1378
00:52:20,010 --> 00:52:22,110
complete copy of that value for every

1379
00:52:22,110 --> 00:52:23,970
for every single tuple we'll restore a

1380
00:52:23,970 --> 00:52:27,210
Delta between the between consecutive

1381
00:52:27,210 --> 00:52:31,170
values right so the idea here say this

1382
00:52:31,170 --> 00:52:33,360
is my table and I'm recording the

1383
00:52:33,360 --> 00:52:35,400
temperature of this room it's 99 degrees

1384
00:52:35,400 --> 00:52:37,110
it's super hot but I'm taking a

1385
00:52:37,110 --> 00:52:38,580
measurement in my sensor every minute

1386
00:52:38,580 --> 00:52:40,590
and then the the difference between

1387
00:52:40,590 --> 00:52:41,700
these temperatures is not gonna be that

1388
00:52:41,700 --> 00:52:43,860
much because you know if it's think of

1389
00:52:43,860 --> 00:52:44,640
the physical world

1390
00:52:44,640 --> 00:52:47,160
this room's not gonna go from 99 degrees

1391
00:52:47,160 --> 00:52:49,110
to a zero degrees in a single in a

1392
00:52:49,110 --> 00:52:51,090
single minute so the most of time these

1393
00:52:51,090 --> 00:52:52,230
deltas are gonna be overly small

1394
00:52:52,230 --> 00:52:54,150
increments so what I'll do is I'll take

1395
00:52:54,150 --> 00:52:57,810
the first value in my two columns and I

1396
00:52:57,810 --> 00:53:00,990
will record them in their entirety but

1397
00:53:00,990 --> 00:53:02,370
then everything that comes after that is

1398
00:53:02,370 --> 00:53:04,680
just a delta between the previous one

1399
00:53:04,680 --> 00:53:06,810
alright so here I'm storing plus one

1400
00:53:06,810 --> 00:53:08,640
plus one to represent that I'm you know

1401
00:53:08,640 --> 00:53:11,930
I'm going one minute ahead and so forth

1402
00:53:11,930 --> 00:53:13,680
what's another technique way to get

1403
00:53:13,680 --> 00:53:18,540
further compression on this RLE right

1404
00:53:18,540 --> 00:53:21,900
exactly what it says that guy so in here

1405
00:53:21,900 --> 00:53:23,820
again I'm I'm quoting it's measuring

1406
00:53:23,820 --> 00:53:25,500
every minute I'm gonna have a bunch of

1407
00:53:25,500 --> 00:53:26,940
plus ones over and over again so I could

1408
00:53:26,940 --> 00:53:29,760
take this now and run orally on that and

1409
00:53:29,760 --> 00:53:31,680
now store that as a single you know

1410
00:53:31,680 --> 00:53:34,410
single double here plus one followed by

1411
00:53:34,410 --> 00:53:38,640
the size of the rock alright so in this

1412
00:53:38,640 --> 00:53:40,680
looking with my toy example here the

1413
00:53:40,680 --> 00:53:42,120
original data assuming that we're

1414
00:53:42,120 --> 00:53:43,260
storing the timestamp and the

1415
00:53:43,260 --> 00:53:48,150
temperature as 32 bits that's wrong it's

1416
00:53:48,150 --> 00:53:51,240
read 5 times 2 or did you just take the

1417
00:53:51,240 --> 00:53:53,070
so we have five values each one's thirty

1418
00:53:53,070 --> 00:53:55,440
bits that's 160 bits in this case here

1419
00:53:55,440 --> 00:53:56,640
assuming that we can then store maybe

1420
00:53:56,640 --> 00:53:59,400
this as 16 bits deltas so now I'll get

1421
00:53:59,400 --> 00:54:01,890
down to 96 bits but if I can do RLE on

1422
00:54:01,890 --> 00:54:04,230
this say I store the original 32 bits

1423
00:54:04,230 --> 00:54:07,830
followed by just two 16-bit fields like

1424
00:54:07,830 --> 00:54:10,109
the the Delta and then the the lengths

1425
00:54:10,109 --> 00:54:13,100
are wrong I can get it down to 64 bits

1426
00:54:13,100 --> 00:54:15,150
so that's that's pretty good

1427
00:54:15,150 --> 00:54:16,950
but again same thing like the all the

1428
00:54:16,950 --> 00:54:18,990
other seams I said before if I want to

1429
00:54:18,990 --> 00:54:20,910
know the exact value is for for this

1430
00:54:20,910 --> 00:54:23,119
unless I'm storing run-length encoding I

1431
00:54:23,119 --> 00:54:25,710
basically at the recompute this thing to

1432
00:54:25,710 --> 00:54:28,800
get down to what get the correct time so

1433
00:54:28,800 --> 00:54:30,330
this is where our le could actually get

1434
00:54:30,330 --> 00:54:33,000
us better computational and performance

1435
00:54:33,000 --> 00:54:35,130
improvements because I don't have to

1436
00:54:35,130 --> 00:54:36,420
reapply this I can just look at this and

1437
00:54:36,420 --> 00:54:37,920
say oh I know that's plus 4 and there's

1438
00:54:37,920 --> 00:54:43,140
do simple math to get there okay so

1439
00:54:43,140 --> 00:54:44,760
variant of Delta encoding is called

1440
00:54:44,760 --> 00:54:47,369
incremental encoding and the idea here

1441
00:54:47,369 --> 00:54:49,260
is that for Strings if we recognize that

1442
00:54:49,260 --> 00:54:52,040
we have repeated prefixes and suffixes

1443
00:54:52,040 --> 00:54:54,210
we just stored the difference between

1444
00:54:54,210 --> 00:54:57,270
between the consecutive values so say it

1445
00:54:57,270 --> 00:54:58,890
is my original data I have four strings

1446
00:54:58,890 --> 00:55:02,340
Rob Rob robbing and robot so the first

1447
00:55:02,340 --> 00:55:03,450
thing I need to do is figure out what is

1448
00:55:03,450 --> 00:55:04,770
the common prefix between consecutive

1449
00:55:04,770 --> 00:55:07,320
values so in the case of the first one

1450
00:55:07,320 --> 00:55:10,050
Rob well this is there's nothing before

1451
00:55:10,050 --> 00:55:11,700
it so we have to store that in its

1452
00:55:11,700 --> 00:55:13,680
entirety there is no common prefix but

1453
00:55:13,680 --> 00:55:15,359
when we look at the next one here robbed

1454
00:55:15,359 --> 00:55:17,130
well this shares the first three three

1455
00:55:17,130 --> 00:55:19,109
characters as this one appear so we know

1456
00:55:19,109 --> 00:55:21,180
that this is the common prefix from this

1457
00:55:21,180 --> 00:55:23,250
attribute to the next attribute all

1458
00:55:23,250 --> 00:55:24,240
right this bag to the next time we do

1459
00:55:24,240 --> 00:55:25,500
the same thing for all these other ones

1460
00:55:25,500 --> 00:55:27,930
here so now once we have this common

1461
00:55:27,930 --> 00:55:30,510
prefix the compressed version could just

1462
00:55:30,510 --> 00:55:34,650
store the the length of the prefix that

1463
00:55:34,650 --> 00:55:36,150
we're sharing and then the remaining

1464
00:55:36,150 --> 00:55:39,119
suffix that's actually different all

1465
00:55:39,119 --> 00:55:40,350
right so this absent only works if

1466
00:55:40,350 --> 00:55:44,550
things are sorted order it actually

1467
00:55:44,550 --> 00:55:46,109
would also work in inside of B plus

1468
00:55:46,109 --> 00:55:47,700
trees we'll see you later on but just

1469
00:55:47,700 --> 00:55:49,590
doing some quick math saying now I'm

1470
00:55:49,590 --> 00:55:52,050
storing all these characters is a bits

1471
00:55:52,050 --> 00:55:54,240
the total size of this data set here is

1472
00:55:54,240 --> 00:55:57,030
168 bits and then for this one it's 88

1473
00:55:57,030 --> 00:55:59,190
bits to store the suffixes and then say

1474
00:55:59,190 --> 00:56:01,500
four times eight bits to store the the

1475
00:56:01,500 --> 00:56:03,690
length of the the prefix

1476
00:56:03,690 --> 00:56:08,720
so in total that's what this 100 and 120

1477
00:56:08,720 --> 00:56:11,160
okay I'm also not storing the the length

1478
00:56:11,160 --> 00:56:13,170
of the the string I have to do that for

1479
00:56:13,170 --> 00:56:15,390
this as well and then same thing for the

1480
00:56:15,390 --> 00:56:16,740
suffix like I need of the length of

1481
00:56:16,740 --> 00:56:23,700
suffix yes this question is this yes if

1482
00:56:23,700 --> 00:56:25,049
it's sorted electric graphically I mean

1483
00:56:25,049 --> 00:56:27,630
this is saying let's abacus defining

1484
00:56:27,630 --> 00:56:28,890
what the collation of the sort order is

1485
00:56:28,890 --> 00:56:30,809
going to be so yes in my example its

1486
00:56:30,809 --> 00:56:33,089
ASCII sorting so yes alphabetical

1487
00:56:33,089 --> 00:56:33,960
sorting through this worked out great

1488
00:56:33,960 --> 00:56:39,599
for that okay so then the next scheme

1489
00:56:39,599 --> 00:56:42,089
that Myrtle that is not really a

1490
00:56:42,089 --> 00:56:45,000
compression scheme but it's a way to

1491
00:56:45,000 --> 00:56:48,329
encode data in in such a way you'd less

1492
00:56:48,329 --> 00:56:50,430
space so as far as they know the only

1493
00:56:50,430 --> 00:56:51,930
system that she does this is Amazon's

1494
00:56:51,930 --> 00:56:54,150
redshift and so they call this mostly in

1495
00:56:54,150 --> 00:56:55,530
coding so the idea is that if you have

1496
00:56:55,530 --> 00:56:58,230
an attribute where most of the time it's

1497
00:56:58,230 --> 00:56:59,880
not going to be the full size that you

1498
00:56:59,880 --> 00:57:02,430
define for the attribute then instead of

1499
00:57:02,430 --> 00:57:03,900
storing all the bits for that attribute

1500
00:57:03,900 --> 00:57:05,549
you just store it as a smaller data type

1501
00:57:05,549 --> 00:57:08,520
so this is oftentimes very common you

1502
00:57:08,520 --> 00:57:10,470
see this in people you know application

1503
00:57:10,470 --> 00:57:12,390
developers when they define their schema

1504
00:57:12,390 --> 00:57:13,559
they just say oh I want everything to be

1505
00:57:13,559 --> 00:57:15,809
in 64 but most the time you're only

1506
00:57:15,809 --> 00:57:17,130
storing you know something it could be

1507
00:57:17,130 --> 00:57:19,589
stored in 8 bits so if you know that you

1508
00:57:19,589 --> 00:57:21,000
can tell it isn't I want to do courts

1509
00:57:21,000 --> 00:57:22,349
called you know the mostly encoding to

1510
00:57:22,349 --> 00:57:24,990
say what used to be a 64 bit column now

1511
00:57:24,990 --> 00:57:26,849
I can is most of the times it's gonna be

1512
00:57:26,849 --> 00:57:30,210
8 bits all right but anytime that I have

1513
00:57:30,210 --> 00:57:32,880
a value that doesn't fit within 8 bits

1514
00:57:32,880 --> 00:57:36,809
then I just have a special flag to say

1515
00:57:36,809 --> 00:57:38,970
the original vector the full value

1516
00:57:38,970 --> 00:57:40,920
looking for it this offset is in this

1517
00:57:40,920 --> 00:57:43,770
lookup table here so now if I'm scanning

1518
00:57:43,770 --> 00:57:45,119
along and trying to find all values

1519
00:57:45,119 --> 00:57:47,819
greater than you know greater than 10 or

1520
00:57:47,819 --> 00:57:49,260
something like that if I come across

1521
00:57:49,260 --> 00:57:51,029
this I would have to know to jump over

1522
00:57:51,029 --> 00:57:55,680
here right so in this example here the

1523
00:57:55,680 --> 00:57:57,420
original size would be 5 times 64 bits

1524
00:57:57,420 --> 00:58:00,180
would be 33 and 20 bits assuming I again

1525
00:58:00,180 --> 00:58:02,700
I can store the this lookup thing as

1526
00:58:02,700 --> 00:58:05,970
just 16 bits plus a 64-bit value then I

1527
00:58:05,970 --> 00:58:08,180
get this down to be hundred 20 bits

1528
00:58:08,180 --> 00:58:13,950
right okay so I always we ran out of

1529
00:58:13,950 --> 00:58:15,809
time with compression but the thing I

1530
00:58:15,809 --> 00:58:17,609
want to spend time to finish up with is

1531
00:58:17,609 --> 00:58:19,980
the dictionary compression and the idea

1532
00:58:19,980 --> 00:58:22,819
here is that this is we're gonna find

1533
00:58:22,819 --> 00:58:28,200
frequently repeated values in the in the

1534
00:58:28,200 --> 00:58:30,900
values of the attributes and instead of

1535
00:58:30,900 --> 00:58:32,460
storing them in their entirety over and

1536
00:58:32,460 --> 00:58:34,019
over again we're just gonna replace them

1537
00:58:34,019 --> 00:58:36,779
with some kind of smaller code and that

1538
00:58:36,779 --> 00:58:40,230
anytime we want to do a look up to see

1539
00:58:40,230 --> 00:58:41,460
what was the original value we'd look in

1540
00:58:41,460 --> 00:58:43,289
this dictionary it can be a hash table

1541
00:58:43,289 --> 00:58:44,640
the B plus tree could be an array

1542
00:58:44,640 --> 00:58:46,529
different ways to organize it we can

1543
00:58:46,529 --> 00:58:48,509
then convert that encoded value back to

1544
00:58:48,509 --> 00:58:51,690
the original value so we obviously want

1545
00:58:51,690 --> 00:58:53,249
to support fast encoding and decoding if

1546
00:58:53,249 --> 00:58:55,380
it's really really expensive to encode

1547
00:58:55,380 --> 00:58:57,569
our block of data then this is gonna be

1548
00:58:57,569 --> 00:59:00,239
a bad trade off decoding can be gonna

1549
00:59:00,239 --> 00:59:01,470
gonna you know it's gonna be actually

1550
00:59:01,470 --> 00:59:02,700
matter more

1551
00:59:02,700 --> 00:59:04,410
but when I'll tell my bad V fast as well

1552
00:59:04,410 --> 00:59:06,150
and they were also one of us support

1553
00:59:06,150 --> 00:59:08,430
range queries so that we can do you know

1554
00:59:08,430 --> 00:59:09,749
find all attributes that are greater

1555
00:59:09,749 --> 00:59:11,789
than sum or sums follow tuples that were

1556
00:59:11,789 --> 00:59:13,049
the values great and some some given

1557
00:59:13,049 --> 00:59:15,239
value i don't want to be compressed that

1558
00:59:15,239 --> 00:59:16,559
to do that evaluation so if my

1559
00:59:16,559 --> 00:59:18,390
dictionary encoding follows the same

1560
00:59:18,390 --> 00:59:21,749
order of the original values then i can

1561
00:59:21,749 --> 00:59:24,930
operate directly on compress data so the

1562
00:59:24,930 --> 00:59:26,220
decisions we're gonna make for

1563
00:59:26,220 --> 00:59:27,779
dictionary compression is when to

1564
00:59:27,779 --> 00:59:29,220
construct the dictionary will should be

1565
00:59:29,220 --> 00:59:30,989
the scope of the dictionary like what

1566
00:59:30,989 --> 00:59:32,789
you know what portion of the data are we

1567
00:59:32,789 --> 00:59:34,739
looking at what data structure we want

1568
00:59:34,739 --> 00:59:35,970
to use and then what should be the

1569
00:59:35,970 --> 00:59:38,190
encoding scheme for the dictionary so

1570
00:59:38,190 --> 00:59:41,099
for the paper i have you guys read they

1571
00:59:41,099 --> 00:59:42,960
talk about you could use Huffman

1572
00:59:42,960 --> 00:59:45,359
encoding or who tucker codes who tougher

1573
00:59:45,359 --> 00:59:46,890
codes are actually gonna want to use

1574
00:59:46,890 --> 00:59:48,450
because their or their order preserving

1575
00:59:48,450 --> 00:59:50,099
we're not going to talk about what those

1576
00:59:50,099 --> 00:59:51,539
albums actually are because we don't

1577
00:59:51,539 --> 00:59:53,160
really care we just use whatever you

1578
00:59:53,160 --> 00:59:54,029
know we're database people will use

1579
00:59:54,029 --> 00:59:57,539
whatever just works but in general most

1580
00:59:57,539 --> 00:59:59,339
people mostly dictionaries are going to

1581
00:59:59,339 --> 01:00:01,559
use use the who tucker codes so we're

1582
01:00:01,559 --> 01:00:04,099
gonna focus on on the other ones up here

1583
01:00:04,099 --> 01:00:06,450
so the first one is we when do we

1584
01:00:06,450 --> 01:00:08,609
actually construct the dictionary so we

1585
01:00:08,609 --> 01:00:10,799
can do this all at once meaning we have

1586
01:00:10,799 --> 01:00:14,849
some some some some data that and we

1587
01:00:14,849 --> 01:00:15,930
know that there's nothing else going to

1588
01:00:15,930 --> 01:00:17,849
be added to it or delete it from it so

1589
01:00:17,849 --> 01:00:19,470
when you take that data and computer it

1590
01:00:19,470 --> 01:00:21,989
right then to them and this time you

1591
01:00:21,989 --> 01:00:23,400
know we're actually gonna possibly get

1592
01:00:23,400 --> 01:00:25,049
the best compression ratio because we

1593
01:00:25,049 --> 01:00:26,759
can assume that everything you know we

1594
01:00:26,759 --> 01:00:28,319
know exactly how to pack in the

1595
01:00:28,319 --> 01:00:29,730
dictionary codes in the smallest number

1596
01:00:29,730 --> 01:00:31,200
bikes that were ever gonna need

1597
01:00:31,200 --> 01:00:33,150
whereas now feminie comes along and and

1598
01:00:33,150 --> 01:00:35,760
wants to modify this we may have to

1599
01:00:35,760 --> 01:00:37,470
change depending or adding things or

1600
01:00:37,470 --> 01:00:38,609
deleting things we may have to recompute

1601
01:00:38,609 --> 01:00:40,559
the dictionary but if the data is cold

1602
01:00:40,559 --> 01:00:43,130
and this is not gonna happen that often

1603
01:00:43,130 --> 01:00:45,500
incremental encoding is where we would

1604
01:00:45,500 --> 01:00:48,329
we would have a dictionary where we add

1605
01:00:48,329 --> 01:00:50,940
some additional placeholders where we

1606
01:00:50,940 --> 01:00:52,950
know values could occur we don't know

1607
01:00:52,950 --> 01:00:54,390
what they are but we know in our and our

1608
01:00:54,390 --> 01:00:56,309
order preserving or the order of their

1609
01:00:56,309 --> 01:00:57,869
dictionary goes something could go there

1610
01:00:57,869 --> 01:00:59,760
so weasel is leave some extra space and

1611
01:00:59,760 --> 01:01:01,410
when these things show up we can

1612
01:01:01,410 --> 01:01:03,089
accommodate them and at some point we'll

1613
01:01:03,089 --> 01:01:04,559
still get to full and then that case we

1614
01:01:04,559 --> 01:01:06,119
have to recompute the the dictionary all

1615
01:01:06,119 --> 01:01:09,750
over again for the dictionary scope

1616
01:01:09,750 --> 01:01:11,130
again we talk to this a little bit

1617
01:01:11,130 --> 01:01:13,079
before but we'd have on a block level we

1618
01:01:13,079 --> 01:01:14,730
just take all the tuples on the same

1619
01:01:14,730 --> 01:01:17,880
value and compress them this is going to

1620
01:01:17,880 --> 01:01:19,619
be potentially lower compression ratio

1621
01:01:19,619 --> 01:01:20,940
and then these other ones because the

1622
01:01:20,940 --> 01:01:22,920
more unique the more repeated values we

1623
01:01:22,920 --> 01:01:24,270
see then the better compression ratio

1624
01:01:24,270 --> 01:01:26,670
we're going to get and the the fewer

1625
01:01:26,670 --> 01:01:28,109
copies of the same dictionary over and

1626
01:01:28,109 --> 01:01:30,450
over again we get the storm table level

1627
01:01:30,450 --> 01:01:32,099
would be we talked a dictionary for the

1628
01:01:32,099 --> 01:01:33,690
entire table this is gonna be amazing

1629
01:01:33,690 --> 01:01:35,220
compression ratio but of course this is

1630
01:01:35,220 --> 01:01:36,390
gonna be expensive to update because I

1631
01:01:36,390 --> 01:01:38,490
need to I'm inserting a lot of things

1632
01:01:38,490 --> 01:01:39,450
and I may have to recompute the

1633
01:01:39,450 --> 01:01:40,710
compression ratio or compression

1634
01:01:40,710 --> 01:01:42,420
dictionary all over again and then

1635
01:01:42,420 --> 01:01:44,880
multiple table would be we can record or

1636
01:01:44,880 --> 01:01:46,530
maintain a dictionary that can be span

1637
01:01:46,530 --> 01:01:49,619
multiple multiple tables and you would

1638
01:01:49,619 --> 01:01:51,150
do this if you have like foreign keys

1639
01:01:51,150 --> 01:01:52,829
where you know that you're gonna join

1640
01:01:52,829 --> 01:01:54,990
the the parent relation and the chav

1641
01:01:54,990 --> 01:01:56,369
elation through the spark and over again

1642
01:01:56,369 --> 01:01:58,140
so if you can compress them with the

1643
01:01:58,140 --> 01:01:59,160
same dictionary then they'll have the

1644
01:01:59,160 --> 01:02:00,210
same codes and you don't have to

1645
01:02:00,210 --> 01:02:01,680
decompress them in order to do the

1646
01:02:01,680 --> 01:02:04,650
comparison so let me skip multi I tree

1647
01:02:04,650 --> 01:02:06,240
coding in the sake of time but let's

1648
01:02:06,240 --> 01:02:10,290
focus on the order preservation stuff so

1649
01:02:10,290 --> 01:02:11,579
we already talked through this we want

1650
01:02:11,579 --> 01:02:13,319
to we need a dictionary that can support

1651
01:02:13,319 --> 01:02:16,049
encode and decode right encode basically

1652
01:02:16,049 --> 01:02:19,140
says for a given for given key its

1653
01:02:19,140 --> 01:02:20,730
uncompressed convert it to its

1654
01:02:20,730 --> 01:02:22,530
compressed form and then this is just

1655
01:02:22,530 --> 01:02:24,990
the reverse of this alright so the main

1656
01:02:24,990 --> 01:02:26,190
thing to point out is that there's not

1657
01:02:26,190 --> 01:02:27,750
gonna be any magic hash function we can

1658
01:02:27,750 --> 01:02:29,160
use to do this for us we're not that

1659
01:02:29,160 --> 01:02:30,869
have an additional data structure that

1660
01:02:30,869 --> 01:02:32,730
we're gonna have to compute in our datum

1661
01:02:32,730 --> 01:02:35,819
to do this for us right if you had a

1662
01:02:35,819 --> 01:02:37,440
hash function that could use this and

1663
01:02:37,440 --> 01:02:39,990
still be order preserving underneath the

1664
01:02:39,990 --> 01:02:41,280
covers the way it's actually can be

1665
01:02:41,280 --> 01:02:43,650
implemented right is probably gonna be a

1666
01:02:43,650 --> 01:02:44,760
hash table

1667
01:02:44,760 --> 01:02:47,490
just a dictionary so and we avoid all

1668
01:02:47,490 --> 01:02:49,140
that and to build the dictionary

1669
01:02:49,140 --> 01:02:51,810
ourselves so the reason why we won't be

1670
01:02:51,810 --> 01:02:54,240
order preserving as I already said if

1671
01:02:54,240 --> 01:02:56,580
now I have a query shows up like find

1672
01:02:56,580 --> 01:02:58,320
all the users where the name is like a

1673
01:02:58,320 --> 01:03:01,860
nd so for the app to execute this query

1674
01:03:01,860 --> 01:03:03,960
instead of having to scan through and

1675
01:03:03,960 --> 01:03:05,580
decompress the data and apply this

1676
01:03:05,580 --> 01:03:08,630
predicate I can just scan the dictionary

1677
01:03:08,630 --> 01:03:11,040
right if I can Rhian code this to say

1678
01:03:11,040 --> 01:03:13,530
now between 10 and 20 I can just scan

1679
01:03:13,530 --> 01:03:17,400
the compressed data and then I can

1680
01:03:17,400 --> 01:03:18,570
figure out what I actually want to look

1681
01:03:18,570 --> 01:03:19,620
at so the way you actually keep you at

1682
01:03:19,620 --> 01:03:22,470
this you do a and D you you like on the

1683
01:03:22,470 --> 01:03:24,000
dictionary first you figure out what the

1684
01:03:24,000 --> 01:03:26,790
the the between range is and then you

1685
01:03:26,790 --> 01:03:29,100
run that scan on this so this dictionary

1686
01:03:29,100 --> 01:03:31,050
is exposed or is maintained by the data

1687
01:03:31,050 --> 01:03:33,120
system itself so it can run like

1688
01:03:33,120 --> 01:03:35,100
preliminary queries on this to figure

1689
01:03:35,100 --> 01:03:37,530
out what is actually available in the

1690
01:03:37,530 --> 01:03:40,740
compressed data you do other things too

1691
01:03:40,740 --> 01:03:42,000
like if you want to say give me all the

1692
01:03:42,000 --> 01:03:44,490
unique count the number distinct names

1693
01:03:44,490 --> 01:03:47,130
in this column I don't even touch the

1694
01:03:47,130 --> 01:03:49,440
name field I just go to the dictionary

1695
01:03:49,440 --> 01:03:51,360
rip through this portion count all of

1696
01:03:51,360 --> 01:03:53,220
those or actually to know how many

1697
01:03:53,220 --> 01:03:54,390
dictionary entries that have that would

1698
01:03:54,390 --> 01:03:55,290
actually good enough to because these

1699
01:03:55,290 --> 01:04:01,110
are guaranteed to be unique right all

1700
01:04:01,110 --> 01:04:02,760
right so let's see now if we have a

1701
01:04:02,760 --> 01:04:04,230
query like this right we can already

1702
01:04:04,230 --> 01:04:05,820
talk about this the question is do we

1703
01:04:05,820 --> 01:04:07,500
have the performance control scan in

1704
01:04:07,500 --> 01:04:11,550
this case yes because I'll match on the

1705
01:04:11,550 --> 01:04:13,770
dictionary but I need to find what the

1706
01:04:13,770 --> 01:04:16,800
actual the tuples are but in this other

1707
01:04:16,800 --> 01:04:18,240
example here I just want to get the the

1708
01:04:18,240 --> 01:04:19,890
distinct name I don't want the

1709
01:04:19,890 --> 01:04:21,450
decompressor I only have to access the

1710
01:04:21,450 --> 01:04:26,820
dictionary yes good you keep bringing

1711
01:04:26,820 --> 01:04:28,470
this up some people's might get deleted

1712
01:04:28,470 --> 01:04:33,030
yes so if my dictionary is on a block of

1713
01:04:33,030 --> 01:04:34,890
data and I say that block is immutable

1714
01:04:34,890 --> 01:04:43,350
then that can't happen right sometimes

1715
01:04:43,350 --> 01:04:45,390
actually also too is in addition to the

1716
01:04:45,390 --> 01:04:49,050
code you also can store the count with

1717
01:04:49,050 --> 01:04:50,850
this right and that's cheap to maintain

1718
01:04:50,850 --> 01:04:52,860
every time I insert right yeah so then

1719
01:04:52,860 --> 01:04:54,360
you can just do more things on like that

1720
01:04:54,360 --> 01:04:55,740
it's sort of like again the pre-computer

1721
01:04:55,740 --> 01:04:57,120
aggregation stuff we talked about bazoom

1722
01:04:57,120 --> 01:04:58,040
at something beginning so I

1723
01:04:58,040 --> 01:05:00,200
for this for this value here's another

1724
01:05:00,200 --> 01:05:02,390
occurrences that I have for some queries

1725
01:05:02,390 --> 01:05:05,840
I just operate on the dictionary all

1726
01:05:05,840 --> 01:05:06,920
right so the different data structures

1727
01:05:06,920 --> 01:05:10,130
you can use for this the most simplest

1728
01:05:10,130 --> 01:05:12,410
one is just being an array so you have

1729
01:05:12,410 --> 01:05:14,030
all these very building strings and you

1730
01:05:14,030 --> 01:05:16,820
sort them in an array and the dictionary

1731
01:05:16,820 --> 01:05:19,270
code is just the offset into that array

1732
01:05:19,270 --> 01:05:21,590
again this works if the data is not

1733
01:05:21,590 --> 01:05:23,210
being modified over and over again right

1734
01:05:23,210 --> 01:05:25,040
as he's gonna ask about because if I

1735
01:05:25,040 --> 01:05:26,540
modify things and I add a new string

1736
01:05:26,540 --> 01:05:28,640
then now all the all sets change and I

1737
01:05:28,640 --> 01:05:31,610
gotta go recode everything so this is

1738
01:05:31,610 --> 01:05:33,710
actually what we do in our system today

1739
01:05:33,710 --> 01:05:35,330
because we followed the Apache arrow

1740
01:05:35,330 --> 01:05:37,060
format and the Apache arrow

1741
01:05:37,060 --> 01:05:39,320
specification Apache arrow is like a

1742
01:05:39,320 --> 01:05:42,710
open source universal file format for

1743
01:05:42,710 --> 01:05:45,380
in-memory columnstore x' and our data is

1744
01:05:45,380 --> 01:05:48,020
essentially follows this and so we they

1745
01:05:48,020 --> 01:05:49,460
do dictionary encoding this way even

1746
01:05:49,460 --> 01:05:50,660
though other encoding schemes might be

1747
01:05:50,660 --> 01:05:52,520
better Aero only does dictionary

1748
01:05:52,520 --> 01:05:55,040
encoding another approach did you use a

1749
01:05:55,040 --> 01:05:58,070
hash table so this would be fast and

1750
01:05:58,070 --> 01:05:59,990
compact to be with lookups the problems

1751
01:05:59,990 --> 01:06:00,980
going to be though you're not going to

1752
01:06:00,980 --> 01:06:02,210
support range queries or prefix queries

1753
01:06:02,210 --> 01:06:03,830
because you're not going to easily be

1754
01:06:03,830 --> 01:06:05,330
able to easily scan through the values

1755
01:06:05,330 --> 01:06:06,710
all right because everything everything

1756
01:06:06,710 --> 01:06:09,650
to be hashed and then the another

1757
01:06:09,650 --> 01:06:11,990
approach is use a B+ tree so this will

1758
01:06:11,990 --> 01:06:13,340
be slower than hash table in the arrays

1759
01:06:13,340 --> 01:06:15,170
and potentially take more memory but

1760
01:06:15,170 --> 01:06:17,030
this is going to support the the range

1761
01:06:17,030 --> 01:06:20,120
of prefix queries you may want to do as

1762
01:06:20,120 --> 01:06:23,110
far as I know most a lot of systems do

1763
01:06:23,110 --> 01:06:28,000
do this first one here I only know of

1764
01:06:28,000 --> 01:06:30,500
there's a patent on this particular idea

1765
01:06:30,500 --> 01:06:31,670
so I don't know who else actually does

1766
01:06:31,670 --> 01:06:34,340
this approach right yeah most of the

1767
01:06:34,340 --> 01:06:36,140
times you're compressing cold data so

1768
01:06:36,140 --> 01:06:38,840
this one is good enough let's see

1769
01:06:38,840 --> 01:06:40,730
actually how you want to do this so the

1770
01:06:40,730 --> 01:06:43,250
idea is basically you have to be plus

1771
01:06:43,250 --> 01:06:45,890
trees you have 1/2 encoding and that's

1772
01:06:45,890 --> 01:06:47,330
right side up and then you have 1 to do

1773
01:06:47,330 --> 01:06:50,000
decode and that's upside-down so you can

1774
01:06:50,000 --> 01:06:51,410
sort of go in both directions and then

1775
01:06:51,410 --> 01:06:53,570
in the middle they're gonna share leaves

1776
01:06:53,570 --> 01:06:56,060
that are gonna map values to codes or

1777
01:06:56,060 --> 01:06:57,770
they can use that to do reverse map code

1778
01:06:57,770 --> 01:07:00,290
to values all right so this is me the

1779
01:07:00,290 --> 01:07:01,970
shortest share leaf if I have a version

1780
01:07:01,970 --> 01:07:03,680
of value like Aboriginal string I want

1781
01:07:03,680 --> 01:07:05,300
to I want to encode it then I go through

1782
01:07:05,300 --> 01:07:08,720
this this index I land in my leaf node

1783
01:07:08,720 --> 01:07:10,820
find the entry that I want and then I

1784
01:07:10,820 --> 01:07:11,609
would produce

1785
01:07:11,609 --> 01:07:13,589
value but then if I have an encoded

1786
01:07:13,589 --> 01:07:15,660
value I use the bottom one do the same

1787
01:07:15,660 --> 01:07:16,650
thing but in Reverse

1788
01:07:16,650 --> 01:07:17,729
and then I can spit out what the

1789
01:07:17,729 --> 01:07:20,640
original value was and internally what

1790
01:07:20,640 --> 01:07:21,630
they're gonna do is they're gonna use

1791
01:07:21,630 --> 01:07:22,739
incremental encoding as we talked about

1792
01:07:22,739 --> 01:07:25,079
before because again within the node

1793
01:07:25,079 --> 01:07:26,729
there are gonna be sorted values and

1794
01:07:26,729 --> 01:07:28,499
they're oftentimes going to be repeating

1795
01:07:28,499 --> 01:07:30,239
the same prefix over and over again so I

1796
01:07:30,239 --> 01:07:33,119
can compress these down even further the

1797
01:07:33,119 --> 01:07:34,709
other thing they're gonna do also to in

1798
01:07:34,709 --> 01:07:36,539
my example here the codes are 10 20 30

1799
01:07:36,539 --> 01:07:38,930
40 so they're gonna leave gaps in here

1800
01:07:38,930 --> 01:07:41,400
between the code range so that if a new

1801
01:07:41,400 --> 01:07:43,529
tuple or new entry shows up that's in

1802
01:07:43,529 --> 01:07:46,979
between a a a a a B and a e I can insert

1803
01:07:46,979 --> 01:07:49,259
a new guy here give it code 11 without

1804
01:07:49,259 --> 01:07:51,809
having to recode everything as soon as I

1805
01:07:51,809 --> 01:07:53,999
run out of space in between these then I

1806
01:07:53,999 --> 01:07:56,989
the game's over and I've every start

1807
01:07:56,989 --> 01:07:59,729
okay so any questions about dictionary

1808
01:07:59,729 --> 01:08:13,680
encoding that was a bit fast yes yes

1809
01:08:13,680 --> 01:08:17,189
yeah so this example is and this one

1810
01:08:17,189 --> 01:08:20,250
here I have a be an AE so I know the

1811
01:08:20,250 --> 01:08:22,770
number of keys could exist in between

1812
01:08:22,770 --> 01:08:24,420
these can therefore I can Pat it out

1813
01:08:24,420 --> 01:08:26,939
that way you could do that I don't think

1814
01:08:26,939 --> 01:08:31,109
they do that though yeah like if you

1815
01:08:31,109 --> 01:08:32,549
know it can only be three characters

1816
01:08:32,549 --> 01:08:35,790
then that's okay right if you have if

1817
01:08:35,790 --> 01:08:37,529
it's longer than three then I like that

1818
01:08:37,529 --> 01:08:43,710
works it can be infinite okay so a quick

1819
01:08:43,710 --> 01:08:45,060
I'm gonna talk about how to do index

1820
01:08:45,060 --> 01:08:49,198
compression so as I said everything we

1821
01:08:49,198 --> 01:08:50,429
talked about here today has been about

1822
01:08:50,429 --> 01:08:52,290
OLAP common expressions you ever see

1823
01:08:52,290 --> 01:08:53,790
that mostly in the wild that that's the

1824
01:08:53,790 --> 01:08:56,609
most common thing but for OHP workloads

1825
01:08:56,609 --> 01:08:58,738
the the indexes themselves can actually

1826
01:08:58,738 --> 01:09:01,439
take up a lot of space as well right so

1827
01:09:01,439 --> 01:09:03,540
this is a study we did for a paper we

1828
01:09:03,540 --> 01:09:05,069
wrote it four years ago we took some

1829
01:09:05,069 --> 01:09:07,679
open source benchmarks TPCC articles

1830
01:09:07,679 --> 01:09:08,819
think of this is like hacker news or

1831
01:09:08,819 --> 01:09:10,529
reddit it's like people pushing articles

1832
01:09:10,529 --> 01:09:12,750
and then people write in comments and

1833
01:09:12,750 --> 01:09:15,149
then voter it came from both DB this is

1834
01:09:15,149 --> 01:09:18,599
a say the Japanese version of American

1835
01:09:18,599 --> 01:09:19,889
Idol it's like when people call them a

1836
01:09:19,889 --> 01:09:21,540
phone to vote when someone's like

1837
01:09:21,540 --> 01:09:23,939
performing this benchmark mimics that

1838
01:09:23,939 --> 01:09:25,380
you snicker but they sold

1839
01:09:25,380 --> 01:09:27,750
both to be a lot of to a lot of like it

1840
01:09:27,750 --> 01:09:29,460
ran runs the Japanese version American

1841
01:09:29,460 --> 01:09:30,960
Idol it runs the Canadian version of

1842
01:09:30,960 --> 01:09:33,479
American Idol I think there's no one on

1843
01:09:33,479 --> 01:09:36,510
Malaysia it runs like it's like it was

1844
01:09:36,510 --> 01:09:39,689
used a lot um I got hired based on this

1845
01:09:39,689 --> 01:09:43,710
right so uh so the original tuba size

1846
01:09:43,710 --> 01:09:46,529
for these benchmarks I for all of these

1847
01:09:46,529 --> 01:09:48,330
the indexes themselves are purp rising a

1848
01:09:48,330 --> 01:09:51,810
large portion of the data so again we

1849
01:09:51,810 --> 01:09:53,609
can't use the database compression stuff

1850
01:09:53,609 --> 01:09:54,780
we talked about so far we can't compress

1851
01:09:54,780 --> 01:09:57,300
the tuples because it's OTP we can

1852
01:09:57,300 --> 01:09:59,640
modify anything at any time so we can't

1853
01:09:59,640 --> 01:10:01,380
compress this part but here's all the

1854
01:10:01,380 --> 01:10:02,760
other data that we can potentially also

1855
01:10:02,760 --> 01:10:06,690
compress as well so the two most common

1856
01:10:06,690 --> 01:10:09,720
techniques to do in X compression are to

1857
01:10:09,720 --> 01:10:11,160
do prefix compression and suffix

1858
01:10:11,160 --> 01:10:13,620
truncation and this looks a lot like the

1859
01:10:13,620 --> 01:10:14,550
stuff that we already talked about

1860
01:10:14,550 --> 01:10:16,290
before we have bunch of sort of data and

1861
01:10:16,290 --> 01:10:19,650
in our nodes and then we repeat values

1862
01:10:19,650 --> 01:10:20,880
over and over again so instead of

1863
01:10:20,880 --> 01:10:23,670
storing the entire value repeatedly we

1864
01:10:23,670 --> 01:10:25,920
destroy the prefix once and then just

1865
01:10:25,920 --> 01:10:27,060
the suffix part that is actually

1866
01:10:27,060 --> 01:10:30,930
different right this one is actually the

1867
01:10:30,930 --> 01:10:32,340
most comments any and you'll see this

1868
01:10:32,340 --> 01:10:33,600
all the time in B plus trees that

1869
01:10:33,600 --> 01:10:35,250
support PBX compression they're gonna do

1870
01:10:35,250 --> 01:10:37,170
something like this now whether or not

1871
01:10:37,170 --> 01:10:39,300
the leaf nodes are compressed or just

1872
01:10:39,300 --> 01:10:40,980
the inter nodes or depends on what kind

1873
01:10:40,980 --> 01:10:42,180
of workload Turney think and a half like

1874
01:10:42,180 --> 01:10:44,310
most of the times you know most of the

1875
01:10:44,310 --> 01:10:45,750
updates are doing to it to a B+ tree

1876
01:10:45,750 --> 01:10:47,370
you're always gonna be the leaf nodes so

1877
01:10:47,370 --> 01:10:48,450
maybe you don't compress that part

1878
01:10:48,450 --> 01:10:49,440
because this thing's gonna change all

1879
01:10:49,440 --> 01:10:51,150
the time but for the inner nodes and up

1880
01:10:51,150 --> 01:10:52,530
above you could you could you could

1881
01:10:52,530 --> 01:10:57,570
oppress those yes this question is is

1882
01:10:57,570 --> 01:10:59,610
this a B+ tree approximation of a radix

1883
01:10:59,610 --> 01:11:05,550
tree yes that's one way to think about

1884
01:11:05,550 --> 01:11:08,280
it but again except the difference is

1885
01:11:08,280 --> 01:11:11,340
that the these key this is in this if

1886
01:11:11,340 --> 01:11:12,450
this is an inner node in the people s

1887
01:11:12,450 --> 01:11:13,890
tree these keys may not actually exist

1888
01:11:13,890 --> 01:11:18,570
right suffix truncation and the idea

1889
01:11:18,570 --> 01:11:20,610
here is that for the internodes we don't

1890
01:11:20,610 --> 01:11:23,730
maybe to store the entire key right we

1891
01:11:23,730 --> 01:11:25,650
just store that the we just throw away

1892
01:11:25,650 --> 01:11:27,630
the suffix word that doesn't help us and

1893
01:11:27,630 --> 01:11:29,760
only store the prefix in these case here

1894
01:11:29,760 --> 01:11:33,450
the the these two keys need for these

1895
01:11:33,450 --> 01:11:35,520
two keys they're unique immediately

1896
01:11:35,520 --> 01:11:37,170
after the first character a and B so I'm

1897
01:11:37,170 --> 01:11:38,580
looking for something that's that

1898
01:11:38,580 --> 01:11:40,680
greater than a then I know I want to go

1899
01:11:40,680 --> 01:11:41,820
one side I'm looking for something

1900
01:11:41,820 --> 01:11:43,410
that's less than a or I go to the other

1901
01:11:43,410 --> 01:11:46,620
side alright so I could destroy a b c

1902
01:11:46,620 --> 01:11:48,930
and l MN and that's enough for me to

1903
01:11:48,930 --> 01:11:50,730
distinguish which path i want to go down

1904
01:11:50,730 --> 01:11:54,950
and that can cut down the space alright

1905
01:11:54,950 --> 01:11:57,420
going back to this one real quickly I'm

1906
01:11:57,420 --> 01:11:58,950
shorting compressing keys you'd also

1907
01:11:58,950 --> 01:12:01,020
compress values the pointers to tuples

1908
01:12:01,020 --> 01:12:02,250
if they're look if you know that two

1909
01:12:02,250 --> 01:12:03,960
bows are being the same block then

1910
01:12:03,960 --> 01:12:05,250
instead of sorting the block ID maybe

1911
01:12:05,250 --> 01:12:07,500
destroy the offset like that that one's

1912
01:12:07,500 --> 01:12:08,700
more common in the displaced isms I

1913
01:12:08,700 --> 01:12:09,900
don't know about any in memory system

1914
01:12:09,900 --> 01:12:12,000
does this and I don't know of any a MIDI

1915
01:12:12,000 --> 01:12:13,500
Mr Justice a well prefix compression is

1916
01:12:13,500 --> 01:12:16,890
the more common one all right the last

1917
01:12:16,890 --> 01:12:18,300
thing we'll talk about is a paper we

1918
01:12:18,300 --> 01:12:20,430
wrote for hybrid indexes and again the

1919
01:12:20,430 --> 01:12:23,040
idea here is just to expose you to other

1920
01:12:23,040 --> 01:12:24,960
techniques of what other things we could

1921
01:12:24,960 --> 01:12:27,870
compress other than just data so we can

1922
01:12:27,870 --> 01:12:29,670
compress the keys in the index we can

1923
01:12:29,670 --> 01:12:31,710
press the values in the index but what

1924
01:12:31,710 --> 01:12:32,940
if we actually could also compress the

1925
01:12:32,940 --> 01:12:35,730
data structure itself but the internal

1926
01:12:35,730 --> 01:12:38,940
metadata we're maintaining to say this

1927
01:12:38,940 --> 01:12:43,050
actually a B+ stream so what we're gonna

1928
01:12:43,050 --> 01:12:44,700
do now is it's gonna call it hybrid

1929
01:12:44,700 --> 01:12:47,100
index we're gonna have a uncompressed

1930
01:12:47,100 --> 01:12:48,690
index and a compress index again this

1931
01:12:48,690 --> 01:12:49,860
looks a lot like the fraction mirror

1932
01:12:49,860 --> 01:12:50,820
stuff that we talked about for the

1933
01:12:50,820 --> 01:12:54,000
column store last class so the dynamic

1934
01:12:54,000 --> 01:12:55,560
index we fast update it's uncompressed

1935
01:12:55,560 --> 01:12:58,350
and then as data gets old we'll convert

1936
01:12:58,350 --> 01:13:00,360
it over to this compressed index that'll

1937
01:13:00,360 --> 01:13:02,250
be read-only and then every so often

1938
01:13:02,250 --> 01:13:04,500
we're gonna refresh the static stage by

1939
01:13:04,500 --> 01:13:06,660
merging in that the changes from the

1940
01:13:06,660 --> 01:13:08,730
front end the front end index to the

1941
01:13:08,730 --> 01:13:11,250
back end so it looks like this so we all

1942
01:13:11,250 --> 01:13:13,170
our inserts update leak go to a dynamic

1943
01:13:13,170 --> 01:13:15,930
dynamic index and then over time will

1944
01:13:15,930 --> 01:13:17,730
then merges with a static index that's

1945
01:13:17,730 --> 01:13:20,040
compressed but then we'll also maintain

1946
01:13:20,040 --> 01:13:21,450
this bloom filters so that when we do a

1947
01:13:21,450 --> 01:13:23,610
lookup we check the bloom filter to see

1948
01:13:23,610 --> 01:13:25,170
whether it the thing we're looking for

1949
01:13:25,170 --> 01:13:27,870
it could be in our dynamic index if yes

1950
01:13:27,870 --> 01:13:30,060
then we go check it out if not the blue

1951
01:13:30,060 --> 01:13:32,880
filter return tells us false then we

1952
01:13:32,880 --> 01:13:34,320
always go check the static index and

1953
01:13:34,320 --> 01:13:36,300
there's a voice that having any excuse

1954
01:13:36,300 --> 01:13:38,490
me any false negatives so everyone would

1955
01:13:38,490 --> 01:13:41,310
have bloom put there is who here doesn't

1956
01:13:41,310 --> 01:13:44,180
know what a bloom filter is

1957
01:13:45,250 --> 01:13:50,260
yes surely did this like yeah you don't

1958
01:13:50,260 --> 01:13:53,080
know what others okay I'll give you a

1959
01:13:53,080 --> 01:13:59,770
crash course in 30 sec okay so a bloom

1960
01:13:59,770 --> 01:14:01,120
filter is an approximate data structure

1961
01:14:01,120 --> 01:14:03,220
that answers set membership queries

1962
01:14:03,220 --> 01:14:04,810
it'll tell you whether does a key exist

1963
01:14:04,810 --> 01:14:07,570
in my set but it may give you false

1964
01:14:07,570 --> 01:14:09,340
positives it'll never tell you something

1965
01:14:09,340 --> 01:14:11,260
doesn't exist when it does exist but it

1966
01:14:11,260 --> 01:14:13,150
may tell you something does exist when

1967
01:14:13,150 --> 01:14:15,580
it doesn't actually exist okay and think

1968
01:14:15,580 --> 01:14:18,190
it's just a bitmap and we're gonna have

1969
01:14:18,190 --> 01:14:20,890
to get a hash function on our key or

1970
01:14:20,890 --> 01:14:22,690
take a key hash it multiple times and

1971
01:14:22,690 --> 01:14:24,220
update different bits and that's how we

1972
01:14:24,220 --> 01:14:26,800
record whether something exists right so

1973
01:14:26,800 --> 01:14:28,510
the only thing we can do is insert and

1974
01:14:28,510 --> 01:14:29,800
lookup we can't actually delete things

1975
01:14:29,800 --> 01:14:31,600
we can't do any range queries so this is

1976
01:14:31,600 --> 01:14:34,210
my super simple bloom filter so say I

1977
01:14:34,210 --> 01:14:36,190
want to insert RZA from the Mouton clan

1978
01:14:36,190 --> 01:14:38,650
I have two hash functions I take the key

1979
01:14:38,650 --> 01:14:40,360
I hash it I'm modded by the number of

1980
01:14:40,360 --> 01:14:41,920
bits that I have and I go flip these

1981
01:14:41,920 --> 01:14:44,290
bits to be one now I do this for the GZA

1982
01:14:44,290 --> 01:14:47,320
same thing i modded by number bits i

1983
01:14:47,320 --> 01:14:49,330
update that played this so now if i do a

1984
01:14:49,330 --> 01:14:51,100
lookup does this bloom filter contain

1985
01:14:51,100 --> 01:14:54,160
the key RZA then i den do the same thing

1986
01:14:54,160 --> 01:14:56,770
take the key pass it then I do my lookup

1987
01:14:56,770 --> 01:14:59,530
if all the ones of these positions all

1988
01:14:59,530 --> 01:15:01,120
the positions are these bits are set to

1989
01:15:01,120 --> 01:15:03,990
1 then I know the thing could exist

1990
01:15:03,990 --> 01:15:07,750
right and true in case of Raekwon a hash

1991
01:15:07,750 --> 01:15:11,290
him now one of the bits are set to 0 so

1992
01:15:11,290 --> 01:15:12,700
I know this thing could not definitely

1993
01:15:12,700 --> 01:15:14,710
does not exist but again you can get

1994
01:15:14,710 --> 01:15:16,000
false negatives if I do a lookup with

1995
01:15:16,000 --> 01:15:19,060
ODB then the hash will take me to bits

1996
01:15:19,060 --> 01:15:20,410
that are worth set but I never actually

1997
01:15:20,410 --> 01:15:22,510
inserted this so it's telling me this

1998
01:15:22,510 --> 01:15:24,790
key exists when it doesn't actually so

1999
01:15:24,790 --> 01:15:26,920
in my going back to my example that you

2000
01:15:26,920 --> 01:15:29,980
got it now ok so going back to my

2001
01:15:29,980 --> 01:15:33,160
example from before if I with this bloom

2002
01:15:33,160 --> 01:15:36,130
filter it could tell me something exists

2003
01:15:36,130 --> 01:15:37,570
in this dynamic index but it doesn't

2004
01:15:37,570 --> 01:15:38,620
actually exist because I've already

2005
01:15:38,620 --> 01:15:40,540
merged it back to the static stage but

2006
01:15:40,540 --> 01:15:42,370
that's ok because you know I maybe I do

2007
01:15:42,370 --> 01:15:44,290
a look up here and it's wasted but I'll

2008
01:15:44,290 --> 01:15:46,060
never say this thing doesn't exist I go

2009
01:15:46,060 --> 01:15:48,070
look in here and I don't see it when it

2010
01:15:48,070 --> 01:15:50,440
actually existed there so how can you

2011
01:15:50,440 --> 01:15:52,030
compact a B+ tree other than is doing

2012
01:15:52,030 --> 01:15:54,040
the compressing the keys well the first

2013
01:15:54,040 --> 01:15:56,410
thing is that because we have this rule

2014
01:15:56,410 --> 01:15:57,580
for B plus trees they say they have to

2015
01:15:57,580 --> 01:15:59,020
be at least half full we

2016
01:15:59,020 --> 01:16:00,040
no we're gonna have a bunch of empty

2017
01:16:00,040 --> 01:16:03,070
slots so if our index is never gonna be

2018
01:16:03,070 --> 01:16:03,670
updated

2019
01:16:03,670 --> 01:16:05,050
we don't actually never need these empty

2020
01:16:05,050 --> 01:16:07,030
slots so we now we can compress every

2021
01:16:07,030 --> 01:16:08,860
node down to be just storing exactly the

2022
01:16:08,860 --> 01:16:11,560
keys that exist but then the next thing

2023
01:16:11,560 --> 01:16:12,850
is that we have these pointers now

2024
01:16:12,850 --> 01:16:14,350
because these are just random nodes in

2025
01:16:14,350 --> 01:16:16,300
the heap so these are may 64-bit

2026
01:16:16,300 --> 01:16:18,010
pointers to some location in memory

2027
01:16:18,010 --> 01:16:20,140
where the next note is so instead of

2028
01:16:20,140 --> 01:16:21,820
storing these pointers in their entirety

2029
01:16:21,820 --> 01:16:25,450
if we just had a offset to some starting

2030
01:16:25,450 --> 01:16:28,000
location in an array then that could

2031
01:16:28,000 --> 01:16:29,560
then we could jump down to exactly where

2032
01:16:29,560 --> 01:16:31,270
where we should have been if these were

2033
01:16:31,270 --> 01:16:31,930
separate nodes

2034
01:16:31,930 --> 01:16:33,580
even though it's one can check you know

2035
01:16:33,580 --> 01:16:37,360
containers piece of memory so that's how

2036
01:16:37,360 --> 01:16:38,860
you can compress a B+ tree you can do

2037
01:16:38,860 --> 01:16:41,080
the same thing right for a skip list Mew

2038
01:16:41,080 --> 01:16:43,390
the same thing for an art index right

2039
01:16:43,390 --> 01:16:45,520
again recognizing that we're never going

2040
01:16:45,520 --> 01:16:46,960
to update the index all these extra

2041
01:16:46,960 --> 01:16:48,640
stuff we have in our system or in the

2042
01:16:48,640 --> 01:16:50,740
index so that we can accommodate updates

2043
01:16:50,740 --> 01:16:52,600
are unnecessary and we can throw them

2044
01:16:52,600 --> 01:16:55,690
all out and compress it down so in terms

2045
01:16:55,690 --> 01:16:58,780
of its size you can get this throughput

2046
01:16:58,780 --> 01:17:00,640
member formants with the hybrid index it

2047
01:17:00,640 --> 01:17:02,440
actually turns out being faster because

2048
01:17:02,440 --> 01:17:03,970
there's a product it's smaller and

2049
01:17:03,970 --> 01:17:05,400
therefore you have fewer cache misses

2050
01:17:05,400 --> 01:17:08,200
and then the overall size is reduced

2051
01:17:08,200 --> 01:17:11,380
quite a bit as well so again this is a

2052
01:17:11,380 --> 01:17:13,420
paper that we wrote a publishing sigmod

2053
01:17:13,420 --> 01:17:15,400
a few years ago no system actually does

2054
01:17:15,400 --> 01:17:17,590
this but I think these techniques again

2055
01:17:17,590 --> 01:17:20,620
could be applied for in a column store

2056
01:17:20,620 --> 01:17:22,390
system when you know tuples are not

2057
01:17:22,390 --> 01:17:26,830
gonna be modified yes question do you

2058
01:17:26,830 --> 01:17:28,000
have a separate bloom filter for each

2059
01:17:28,000 --> 01:17:31,360
stage if you have multiple stages like

2060
01:17:31,360 --> 01:17:33,100
you could say like here's a here's a hot

2061
01:17:33,100 --> 01:17:35,830
index here's a great cooler index and

2062
01:17:35,830 --> 01:17:37,510
here's the cold index you'd have a bloom

2063
01:17:37,510 --> 01:17:39,910
cluster of each one there you don't need

2064
01:17:39,910 --> 01:17:41,620
for the static one right because because

2065
01:17:41,620 --> 01:17:43,270
this is saying this is a query shows up

2066
01:17:43,270 --> 01:17:46,360
do I need to read this yes or no and the

2067
01:17:46,360 --> 01:17:47,560
blue meter tells me whether I need to do

2068
01:17:47,560 --> 01:17:49,480
that and the idea is that because I say

2069
01:17:49,480 --> 01:17:51,370
I do much inserts into my dynamic index

2070
01:17:51,370 --> 01:17:53,320
over time I want to then merge it all

2071
01:17:53,320 --> 01:17:55,270
those keys now exist in the static side

2072
01:17:55,270 --> 01:17:56,470
they're no longer exist here this thing

2073
01:17:56,470 --> 01:17:58,950
becomes empty

2074
01:17:58,950 --> 01:18:00,790
questions why don't we you just interior

2075
01:18:00,790 --> 01:18:06,370
cuz we have other problems yeah I say

2076
01:18:06,370 --> 01:18:10,780
also too like we don't have a we don't

2077
01:18:10,780 --> 01:18:13,390
have a multi-threaded thread-safe

2078
01:18:13,390 --> 01:18:16,180
merge algorithm we did this in HD or HD

2079
01:18:16,180 --> 01:18:18,190
or single-threaded engines so like we

2080
01:18:18,190 --> 01:18:19,930
we'd stop all transactions while we

2081
01:18:19,930 --> 01:18:22,060
merged it cuz otherwise you have to like

2082
01:18:22,060 --> 01:18:23,410
merge this and worry about people

2083
01:18:23,410 --> 01:18:25,180
reading the same time or make me make

2084
01:18:25,180 --> 01:18:26,800
two copies and that gets expensive

2085
01:18:26,800 --> 01:18:28,560
because they ex can get kind of large

2086
01:18:28,560 --> 01:18:31,270
okay so that was rush to the end I

2087
01:18:31,270 --> 01:18:33,730
apologize but beginning the hope of the

2088
01:18:33,730 --> 01:18:35,520
main takeaway you got from this is that

2089
01:18:35,520 --> 01:18:37,870
hey the database system knows what the

2090
01:18:37,870 --> 01:18:39,100
data looks like it knows what the types

2091
01:18:39,100 --> 01:18:40,780
are it knows how the query is gonna be

2092
01:18:40,780 --> 01:18:42,490
accessing them it can now make better

2093
01:18:42,490 --> 01:18:43,930
decisions on how to compress the data

2094
01:18:43,930 --> 01:18:45,700
beyond what I can ieave compression

2095
01:18:45,700 --> 01:18:47,950
scheme can do right and we showed how we

2096
01:18:47,950 --> 01:18:50,830
can do we can apply we can apply

2097
01:18:50,830 --> 01:18:52,750
compression on already compressed data

2098
01:18:52,750 --> 01:18:55,720
like doing RLE on delta encoding and get

2099
01:18:55,720 --> 01:18:57,730
even better compression ratios alright

2100
01:18:57,730 --> 01:19:00,730
and then we talked to how about we can

2101
01:19:00,730 --> 01:19:02,110
wait as long as possible during query

2102
01:19:02,110 --> 01:19:03,760
execution before we have to decompress

2103
01:19:03,760 --> 01:19:05,290
the data only when we tell the outside

2104
01:19:05,290 --> 01:19:07,060
world here's the answer the query then

2105
01:19:07,060 --> 01:19:09,250
ideally we have to decompress it for

2106
01:19:09,250 --> 01:19:12,460
joins we'll see later in the semester if

2107
01:19:12,460 --> 01:19:13,510
you're joining two things that are

2108
01:19:13,510 --> 01:19:14,890
compressed and their own a different

2109
01:19:14,890 --> 01:19:16,510
dictionary you have to be compressed one

2110
01:19:16,510 --> 01:19:17,740
and potentially re-encode the other one

2111
01:19:17,740 --> 01:19:19,180
and that gets expensive but that's

2112
01:19:19,180 --> 01:19:22,660
unavoidable okay alright so next week

2113
01:19:22,660 --> 01:19:24,580
we'll talk about our started one

2114
01:19:24,580 --> 01:19:27,970
Wednesday today's Wednesday thank you

2115
01:19:27,970 --> 01:19:32,280
yeah all right it's been rough alright

2116
01:19:32,280 --> 01:19:34,510
okay baby all right

2117
01:19:34,510 --> 01:19:36,550
so Monday thank you Monday we'll talk

2118
01:19:36,550 --> 01:19:37,930
about logging checkpoints so now even

2119
01:19:37,930 --> 01:19:39,070
though we're in memory database we still

2120
01:19:39,070 --> 01:19:40,360
need to be able to recover after a crash

2121
01:19:40,360 --> 01:19:42,430
so I had to write out things to disks

2122
01:19:42,430 --> 01:19:43,840
and take checkpoints up quickly as

2123
01:19:43,840 --> 01:19:44,850
possible okay

2124
01:19:44,850 --> 01:19:48,250
alright have a good weekend see ya Bank

2125
01:19:48,250 --> 01:19:50,440
it in the side park what is this

2126
01:19:50,440 --> 01:19:53,280
some fools hey yo

2127
01:19:53,280 --> 01:19:55,949
spica's ain't with that here called the

2128
01:19:55,949 --> 01:19:58,829
okay cuz I'm all Chi I stood down with

2129
01:19:58,829 --> 01:20:02,159
the testy hi you look and it was go grab

2130
01:20:02,159 --> 01:20:04,949
me a 40 just to get my buzz song cuz I

2131
01:20:04,949 --> 01:20:07,199
needed just a little more kick like a

2132
01:20:07,199 --> 01:20:09,750
fish after just one slip put it to my

2133
01:20:09,750 --> 01:20:12,510
lips and rip the top off the truck the

2134
01:20:12,510 --> 01:20:15,690
same as hot and my hood wants me to say

2135
01:20:15,690 --> 01:20:20,389
I'm nice to take a seat to pray

