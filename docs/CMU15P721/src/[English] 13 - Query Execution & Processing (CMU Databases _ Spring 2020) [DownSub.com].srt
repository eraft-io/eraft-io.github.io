1
00:00:01,300 --> 00:00:05,129
[Music]

2
00:00:05,200 --> 00:00:05,930
[Applause]

3
00:00:05,930 --> 00:00:08,630
[Music]

4
00:00:08,630 --> 00:00:10,020
[Applause]

5
00:00:10,020 --> 00:00:12,170
[Music]

6
00:00:12,170 --> 00:00:14,000
so today we're gonna get we're

7
00:00:14,000 --> 00:00:16,260
continuing our discussion on now

8
00:00:16,260 --> 00:00:17,460
actually how to start executing queries

9
00:00:17,460 --> 00:00:20,220
so this is what I showed a few few

10
00:00:20,220 --> 00:00:23,310
lectures ago just the overview of what a

11
00:00:23,310 --> 00:00:24,900
hypothetical database system will look

12
00:00:24,900 --> 00:00:26,340
like a high-performance modern database

13
00:00:26,340 --> 00:00:30,000
system and so we've covered you know

14
00:00:30,000 --> 00:00:31,590
some of the parts down here and the

15
00:00:31,590 --> 00:00:33,059
networking layer and now we're at this

16
00:00:33,059 --> 00:00:36,149
point here and we're going up in this

17
00:00:36,149 --> 00:00:38,160
direction we're start you know talk

18
00:00:38,160 --> 00:00:39,270
about execution

19
00:00:39,270 --> 00:00:41,370
Cori execution today next week we talked

20
00:00:41,370 --> 00:00:42,870
about compilation then we'll come back

21
00:00:42,870 --> 00:00:45,469
and do more query execution and then

22
00:00:45,469 --> 00:00:48,809
after midterm or after the spring break

23
00:00:48,809 --> 00:00:50,520
will then talk about code optimization

24
00:00:50,520 --> 00:00:55,440
query planning okay so the idea what we

25
00:00:55,440 --> 00:00:57,180
talked about today is and going forward

26
00:00:57,180 --> 00:01:01,410
for the semester is how to do a build an

27
00:01:01,410 --> 00:01:03,390
efficient query execution engine and

28
00:01:03,390 --> 00:01:04,739
we're differ this between some of the

29
00:01:04,739 --> 00:01:06,180
techniques we talked about in a

30
00:01:06,180 --> 00:01:08,340
discounted system because if we're

31
00:01:08,340 --> 00:01:11,820
entirely in memory then we don't have to

32
00:01:11,820 --> 00:01:13,170
worry about the main bottleneck that the

33
00:01:13,170 --> 00:01:14,430
disk oriented system had to worry about

34
00:01:14,430 --> 00:01:17,460
right in a disk learning system the goal

35
00:01:17,460 --> 00:01:19,500
was always to produce disk i/o because

36
00:01:19,500 --> 00:01:21,200
that was always the most expensive thing

37
00:01:21,200 --> 00:01:24,330
and so now if we don't have stalls

38
00:01:24,330 --> 00:01:26,400
because we're going to fetch disk to run

39
00:01:26,400 --> 00:01:28,049
our queries now we have on to other

40
00:01:28,049 --> 00:01:29,310
things we got to worry about and those

41
00:01:29,310 --> 00:01:31,740
that there's the the the bottlenecks

42
00:01:31,740 --> 00:01:33,689
we'll discuss throughout the semester

43
00:01:33,689 --> 00:01:35,460
and then the techniques I'll be

44
00:01:35,460 --> 00:01:37,049
describing will show you how to actually

45
00:01:37,049 --> 00:01:39,299
overcome them or mitigate their effects

46
00:01:39,299 --> 00:01:41,430
on on the performance of the system and

47
00:01:41,430 --> 00:01:44,729
so the we obviously still need disk for

48
00:01:44,729 --> 00:01:46,560
logging so that doesn't go away but it's

49
00:01:46,560 --> 00:01:48,329
really like when I actually do a query I

50
00:01:48,329 --> 00:01:51,210
can assume that I'm gonna read a tuple

51
00:01:51,210 --> 00:01:52,860
or read a column we need a block of data

52
00:01:52,860 --> 00:01:58,020
and that's in memory alright so at the

53
00:01:58,020 --> 00:01:59,040
way it essentially think about what

54
00:01:59,040 --> 00:02:01,259
we're talking about here is it's gonna

55
00:02:01,259 --> 00:02:03,420
be there's not any one technique we're

56
00:02:03,420 --> 00:02:05,820
gonna do that's gonna be there's make

57
00:02:05,820 --> 00:02:07,290
everything go better it's going to be

58
00:02:07,290 --> 00:02:11,640
sort of an orchestration or coordination

59
00:02:11,640 --> 00:02:13,440
across multiple optimizations

60
00:02:13,440 --> 00:02:16,560
and the by combining them together then

61
00:02:16,560 --> 00:02:17,550
we'll get the efficient execution

62
00:02:17,550 --> 00:02:20,700
performance that we need so the spoiler

63
00:02:20,700 --> 00:02:23,370
would be just as a heads up compilation

64
00:02:23,370 --> 00:02:25,200
parallelization and vectorization those

65
00:02:25,200 --> 00:02:26,760
are going to be the big three but

66
00:02:26,760 --> 00:02:28,890
there's a penalty about other things we

67
00:02:28,890 --> 00:02:30,620
could talk about as we go along

68
00:02:30,620 --> 00:02:33,540
so what are our optimization goals in

69
00:02:33,540 --> 00:02:35,640
our system so the disk goes away what do

70
00:02:35,640 --> 00:02:36,660
we actually want to care about now to

71
00:02:36,660 --> 00:02:39,870
get a query actuation performance to be

72
00:02:39,870 --> 00:02:43,050
to be good so the the first one is

73
00:02:43,050 --> 00:02:46,620
obvious right we are going to be we want

74
00:02:46,620 --> 00:02:49,140
to just reduce what we do when we

75
00:02:49,140 --> 00:02:50,970
execute queries and we can do this I

76
00:02:50,970 --> 00:02:55,230
predict count that the davison prosecute

77
00:02:55,230 --> 00:02:58,080
in order to to process the query so we

78
00:02:58,080 --> 00:02:59,970
want to execute fewer instructions to do

79
00:02:59,970 --> 00:03:03,210
the same amount of work and that's you

80
00:03:03,210 --> 00:03:04,170
know then we'll get better performance

81
00:03:04,170 --> 00:03:07,770
that way right so the compiler will help

82
00:03:07,770 --> 00:03:10,170
us a little bit like we can pass in Oh

83
00:03:10,170 --> 00:03:12,720
to typically don't ship database systems

84
00:03:12,720 --> 00:03:15,390
with oh three compiled binaries because

85
00:03:15,390 --> 00:03:18,990
that it it's not that none that it's

86
00:03:18,990 --> 00:03:20,640
experimental it just there might be some

87
00:03:20,640 --> 00:03:24,540
anomalies that you may not be prepared

88
00:03:24,540 --> 00:03:27,150
for or can consider Oh two is typically

89
00:03:27,150 --> 00:03:29,790
people ship software with so the

90
00:03:29,790 --> 00:03:31,110
compiler doesn't help a little bit but

91
00:03:31,110 --> 00:03:33,180
what we're gonna end up wanting to do is

92
00:03:33,180 --> 00:03:35,520
specialize our database system for the

93
00:03:35,520 --> 00:03:37,650
cific queries that we're executing again

94
00:03:37,650 --> 00:03:39,540
that'll be query compilation on Monday

95
00:03:39,540 --> 00:03:41,520
next week but we'll see a flavor of what

96
00:03:41,520 --> 00:03:44,730
it looks like today the next one is to

97
00:03:44,730 --> 00:03:48,120
reduce the cycles we have to incur when

98
00:03:48,120 --> 00:03:50,880
we execute these instructions so we've

99
00:03:50,880 --> 00:03:52,350
already done this we've already reduced

100
00:03:52,350 --> 00:03:53,700
our instruction count and so for the

101
00:03:53,700 --> 00:03:55,050
remaining instructions we do have to

102
00:03:55,050 --> 00:03:57,209
execute we want to reduce the number of

103
00:03:57,209 --> 00:04:00,209
cycles it takes to process them then I

104
00:04:00,209 --> 00:04:07,739
know how we actually want to do that yes

105
00:04:07,739 --> 00:04:13,620
she said parallelization not quite

106
00:04:13,620 --> 00:04:16,000
that's the same thing as well that's

107
00:04:16,000 --> 00:04:17,910
vectorization that's paralyzation but

108
00:04:17,910 --> 00:04:19,470
mmm

109
00:04:19,470 --> 00:04:23,680
cindy will give us this it won't

110
00:04:23,680 --> 00:04:31,990
necessarily give us this Yassir branch

111
00:04:31,990 --> 00:04:34,870
misprediction is one but also cat

112
00:04:34,870 --> 00:04:37,570
reducing them our cache misses right we

113
00:04:37,570 --> 00:04:39,280
if we have an instruction that needs to

114
00:04:39,280 --> 00:04:40,539
touch a piece of data and that piece of

115
00:04:40,539 --> 00:04:42,880
data is not in our CPU caches it has to

116
00:04:42,880 --> 00:04:46,300
stall and in spend more cycles to go up

117
00:04:46,300 --> 00:04:47,949
to DRAM to bring it into our CPU caches

118
00:04:47,949 --> 00:04:50,440
so we want to maximize the locality of

119
00:04:50,440 --> 00:04:51,789
the data as we're processing to them as

120
00:04:51,789 --> 00:04:54,250
well as being intelligent about how we

121
00:04:54,250 --> 00:04:58,630
do our how we have branches in the four

122
00:04:58,630 --> 00:05:00,910
loops as you process tuples to reduce

123
00:05:00,910 --> 00:05:03,550
the amount of misprediction and then the

124
00:05:03,550 --> 00:05:04,990
last one is sort of what you guys you

125
00:05:04,990 --> 00:05:07,210
two said here the parallel is 8 and the

126
00:05:07,210 --> 00:05:09,370
rector ization this is just gonna allow

127
00:05:09,370 --> 00:05:11,440
us to now use the additional cores that

128
00:05:11,440 --> 00:05:14,770
we're getting on our own our CPUs to to

129
00:05:14,770 --> 00:05:17,139
process queries in parallel right and

130
00:05:17,139 --> 00:05:18,190
we'll talk about those different schemes

131
00:05:18,190 --> 00:05:20,470
look like today like Moore's law is

132
00:05:20,470 --> 00:05:23,020
essentially ending a Intel really can't

133
00:05:23,020 --> 00:05:27,270
crank up the clock speed anymore thing

134
00:05:27,270 --> 00:05:30,370
there are alternative materials we could

135
00:05:30,370 --> 00:05:31,960
use to build our processors laughing

136
00:05:31,960 --> 00:05:34,840
that don't have that melting point but

137
00:05:34,840 --> 00:05:36,910
we're that's way way in the future

138
00:05:36,910 --> 00:05:39,880
so what Intel and ambi are giving us

139
00:05:39,880 --> 00:05:41,440
it's just Boris

140
00:05:41,440 --> 00:05:43,810
we wanna execute our query is on on as

141
00:05:43,810 --> 00:05:45,130
many scores as possible and that's

142
00:05:45,130 --> 00:05:46,030
really tricky because now when you

143
00:05:46,030 --> 00:05:49,330
potentially coordinator cross done so

144
00:05:49,330 --> 00:05:51,820
paper I have you guys read um with a bit

145
00:05:51,820 --> 00:05:54,400
more I'm gonna say analytical then what

146
00:05:54,400 --> 00:05:56,410
we'll talk about today but it was what I

147
00:05:56,410 --> 00:05:57,910
reason why I picked it is because at

148
00:05:57,910 --> 00:06:00,010
least it's an eruption portion of the

149
00:06:00,010 --> 00:06:00,340
FIR

150
00:06:00,340 --> 00:06:01,960
they went through a lot of the

151
00:06:01,960 --> 00:06:03,449
techniques that we'll talk about today

152
00:06:03,449 --> 00:06:05,680
and just showing you that there's a

153
00:06:05,680 --> 00:06:08,139
bunch of difference you could do to you

154
00:06:08,139 --> 00:06:10,120
know how to design the system to execute

155
00:06:10,120 --> 00:06:12,520
queries that was still an OLAP and

156
00:06:12,520 --> 00:06:14,169
that'll primarily be what we'll focus on

157
00:06:14,169 --> 00:06:16,930
for today's lecture but again they

158
00:06:16,930 --> 00:06:18,219
talked about this difference between

159
00:06:18,219 --> 00:06:19,750
should I do an in X probe which is

160
00:06:19,750 --> 00:06:21,220
random i/o or

161
00:06:21,220 --> 00:06:23,950
special scan and there's no one answer

162
00:06:23,950 --> 00:06:25,360
to say when you want to use one versus

163
00:06:25,360 --> 00:06:27,640
the other the main thing that I want to

164
00:06:27,640 --> 00:06:30,090
get you to get out of it was that

165
00:06:30,090 --> 00:06:32,350
typically in a Discordian system it was

166
00:06:32,350 --> 00:06:35,250
always the it would always make this

167
00:06:35,250 --> 00:06:37,660
decision about whether dude Enix can

168
00:06:37,660 --> 00:06:39,370
respond to scan based on the selectivity

169
00:06:39,370 --> 00:06:41,410
of the predicate if I haven't index and

170
00:06:41,410 --> 00:06:42,610
I have a predicate that could be used on

171
00:06:42,610 --> 00:06:43,180
the index

172
00:06:43,180 --> 00:06:44,770
how many tuples do I think I'm gonna get

173
00:06:44,770 --> 00:06:47,770
back from the index in an in-memory

174
00:06:47,770 --> 00:06:49,990
world we actually do care about what the

175
00:06:49,990 --> 00:06:51,190
performance of the harbor is gonna look

176
00:06:51,190 --> 00:06:53,440
like like what the CPU will actually do

177
00:06:53,440 --> 00:06:55,270
when they we execute our database system

178
00:06:55,270 --> 00:06:57,340
as well as what other queries are

179
00:06:57,340 --> 00:07:00,130
running at the same time and this one

180
00:07:00,130 --> 00:07:01,540
bit tricky because this one needs to be

181
00:07:01,540 --> 00:07:04,270
you know now we need have the optimizer

182
00:07:04,270 --> 00:07:06,400
be aware of what else is running at the

183
00:07:06,400 --> 00:07:09,490
same time and then you make decisions

184
00:07:09,490 --> 00:07:10,060
based on that

185
00:07:10,060 --> 00:07:13,180
so do you only see this applying this

186
00:07:13,180 --> 00:07:14,890
technique in the high-end commercial

187
00:07:14,890 --> 00:07:15,640
database systems

188
00:07:15,640 --> 00:07:17,320
like I don't Postgres doesn't look to

189
00:07:17,320 --> 00:07:18,400
see what other queries are running at

190
00:07:18,400 --> 00:07:19,930
the same time because it's hard to do

191
00:07:19,930 --> 00:07:21,550
this right because it's like here's my

192
00:07:21,550 --> 00:07:23,080
query I run through the optimizer I'm

193
00:07:23,080 --> 00:07:25,419
gonna pick what could be running the

194
00:07:25,419 --> 00:07:27,310
same by the time my query comes out of

195
00:07:27,310 --> 00:07:28,900
the optimizer and starts running those

196
00:07:28,900 --> 00:07:30,220
other queries might be wrong so my

197
00:07:30,220 --> 00:07:33,070
decisions might be incorrect so this one

198
00:07:33,070 --> 00:07:35,290
is hard to do this one you can you can

199
00:07:35,290 --> 00:07:37,030
compute in the beginning although it can

200
00:07:37,030 --> 00:07:38,680
vary if you're running on Amazon because

201
00:07:38,680 --> 00:07:41,050
and you know the even though you get the

202
00:07:41,050 --> 00:07:42,550
same instance type the performance can

203
00:07:42,550 --> 00:07:44,410
change by I think about up to like 20%

204
00:07:44,410 --> 00:07:45,640
because again somebody else might be

205
00:07:45,640 --> 00:07:47,550
running on the same box as you all right

206
00:07:47,550 --> 00:07:50,890
so the type of optimizations we're gonna

207
00:07:50,890 --> 00:07:52,270
apply that they talked about in that

208
00:07:52,270 --> 00:07:53,770
paper as well as some additional ones

209
00:07:53,770 --> 00:07:56,020
we'll talk about today are just how

210
00:07:56,020 --> 00:07:57,100
we're gonna actually go process the

211
00:07:57,100 --> 00:07:58,870
query how we're gonna move data from one

212
00:07:58,870 --> 00:08:00,850
operator to the next or where they're

213
00:08:00,850 --> 00:08:02,770
gonna put you a push or a pull scan

214
00:08:02,770 --> 00:08:04,650
sharing is a technique where you allow

215
00:08:04,650 --> 00:08:07,060
to two or more queries that are running

216
00:08:07,060 --> 00:08:09,430
the same accessing the same data to

217
00:08:09,430 --> 00:08:11,830
piggyback off the iterators and instead

218
00:08:11,830 --> 00:08:13,120
of having each of them read the same day

219
00:08:13,120 --> 00:08:15,000
at the same time you combine it together

220
00:08:15,000 --> 00:08:16,810
materialized views is a way to

221
00:08:16,810 --> 00:08:19,360
pre-compute some portion of a query

222
00:08:19,360 --> 00:08:21,070
ahead of time and can maintain it as the

223
00:08:21,070 --> 00:08:22,750
table gets updated so you can use that

224
00:08:22,750 --> 00:08:24,340
for query execution so for these two

225
00:08:24,340 --> 00:08:26,320
we're not gonna talk about much this

226
00:08:26,320 --> 00:08:27,760
semester I'll try to talk a little bit

227
00:08:27,760 --> 00:08:30,310
later on but it's these other ones here

228
00:08:30,310 --> 00:08:31,270
that we'll spend most of our time

229
00:08:31,270 --> 00:08:33,339
because as I said these three here are

230
00:08:33,339 --> 00:08:34,630
when you get the biggest bang for the

231
00:08:34,630 --> 00:08:35,479
buck

232
00:08:35,479 --> 00:08:37,760
most workloads obviously in some cases

233
00:08:37,760 --> 00:08:39,860
materialized using screen sharing could

234
00:08:39,860 --> 00:08:42,649
be very beneficial if you have queries

235
00:08:42,649 --> 00:08:44,240
that have a high opportunities to take

236
00:08:44,240 --> 00:08:46,100
advantage of these things but typically

237
00:08:46,100 --> 00:08:47,720
these things are general-purpose enough

238
00:08:47,720 --> 00:08:49,370
what we want to do again query

239
00:08:49,370 --> 00:08:51,260
compilation is it's Co specialization

240
00:08:51,260 --> 00:08:53,000
vectorization what he talked about using

241
00:08:53,000 --> 00:08:54,470
Cynthia instructions parallel algorithms

242
00:08:54,470 --> 00:08:56,269
Lucia she mentioned is running the

243
00:08:56,269 --> 00:08:57,980
operator in parallel and then we'll

244
00:08:57,980 --> 00:09:01,070
finish up the animist er talking about

245
00:09:01,070 --> 00:09:04,459
how to embed or how people use yes or

246
00:09:04,459 --> 00:09:06,440
user-defined functions in queries and

247
00:09:06,440 --> 00:09:09,430
there's big opportunities actually

248
00:09:09,430 --> 00:09:12,110
basically merge these into the query

249
00:09:12,110 --> 00:09:14,480
plan itself instead of treating as the

250
00:09:14,480 --> 00:09:16,370
UDF as a black box and you get way

251
00:09:16,370 --> 00:09:19,130
better performance as well so we'll cut

252
00:09:19,130 --> 00:09:20,089
that that'll be at the end of semester

253
00:09:20,089 --> 00:09:22,040
again these are the big 31 that we want

254
00:09:22,040 --> 00:09:25,399
to discuss alright so today's agenda

255
00:09:25,399 --> 00:09:26,570
that we're going to first talk about

256
00:09:26,570 --> 00:09:30,320
what sir modern CPUs look like in the

257
00:09:30,320 --> 00:09:31,910
context of database systems that what

258
00:09:31,910 --> 00:09:33,560
aspects of databases what about a

259
00:09:33,560 --> 00:09:35,779
diffuse do we need to care about when we

260
00:09:35,779 --> 00:09:37,610
build our database system then we'll

261
00:09:37,610 --> 00:09:38,839
talk about different processing models

262
00:09:38,839 --> 00:09:40,160
to how to move data between between

263
00:09:40,160 --> 00:09:41,630
operators and then we'll finish up

264
00:09:41,630 --> 00:09:43,610
talking about different parallel

265
00:09:43,610 --> 00:09:45,649
execution models the idea here is

266
00:09:45,649 --> 00:09:46,940
basically how we're gonna architect the

267
00:09:46,940 --> 00:09:49,220
system to support pale execution of

268
00:09:49,220 --> 00:09:51,319
multiple operators at the same time all

269
00:09:51,319 --> 00:09:59,990
right all right so the over 15 years old

270
00:09:59,990 --> 00:10:05,779
and 2005 proposing a improved version of

271
00:10:05,779 --> 00:10:07,670
Monet TV which is one of the original

272
00:10:07,670 --> 00:10:09,829
open-source academic columnstore systems

273
00:10:09,829 --> 00:10:12,620
are in memory Thomas or systems it was

274
00:10:12,620 --> 00:10:15,170
an improved version of Monet DB where

275
00:10:15,170 --> 00:10:18,620
they showed how existing database system

276
00:10:18,620 --> 00:10:21,829
implementations were insufficient or not

277
00:10:21,829 --> 00:10:24,319
targeting what modern superscalar CPUs

278
00:10:24,319 --> 00:10:27,079
look like and that if you redesign the

279
00:10:27,079 --> 00:10:29,269
architecture of the database system you

280
00:10:29,269 --> 00:10:30,740
can get much better performance if you

281
00:10:30,740 --> 00:10:33,470
write it in such a way that is is ideal

282
00:10:33,470 --> 00:10:35,120
or amenable to how the CPU actually

283
00:10:35,120 --> 00:10:37,339
processes instructions so the way to

284
00:10:37,339 --> 00:10:40,579
think about this is most times when

285
00:10:40,579 --> 00:10:42,760
people tell database systems and

286
00:10:42,760 --> 00:10:46,010
certainly you know that's coming now but

287
00:10:46,010 --> 00:10:48,110
this is very common in this paper they

288
00:10:48,110 --> 00:10:49,190
look like my sequel and post

289
00:10:49,190 --> 00:10:51,230
showed an orbiter this was the case a

290
00:10:51,230 --> 00:10:53,380
lot of times when people build software

291
00:10:53,380 --> 00:10:55,970
they build it in such a way that makes

292
00:10:55,970 --> 00:10:57,890
it easier for humans to reason about

293
00:10:57,890 --> 00:11:00,170
that software of what the actual code is

294
00:11:00,170 --> 00:11:03,350
actually doing but it turns out the way

295
00:11:03,350 --> 00:11:05,270
that's easier for humans is actually bad

296
00:11:05,270 --> 00:11:08,360
for what CPUs actually want and so what

297
00:11:08,360 --> 00:11:09,860
they're proposing here is if you design

298
00:11:09,860 --> 00:11:11,630
the system in such a way that may be

299
00:11:11,630 --> 00:11:13,790
more complicated for mere mortal humans

300
00:11:13,790 --> 00:11:14,990
to reason about but it's actually better

301
00:11:14,990 --> 00:11:16,910
than for the CPU you already get much

302
00:11:16,910 --> 00:11:18,130
much better performance

303
00:11:18,130 --> 00:11:22,130
so Mon ADB X 100 was the name of their

304
00:11:22,130 --> 00:11:26,120
prototype this later got renamed in and

305
00:11:26,120 --> 00:11:28,130
and commercialized as vector wise and

306
00:11:28,130 --> 00:11:31,670
then act in it's a holding company for

307
00:11:31,670 --> 00:11:34,570
old software they bought vector wise

308
00:11:34,570 --> 00:11:38,150
renamed it to a vector

309
00:11:38,150 --> 00:11:39,980
they then like killed it off for a

310
00:11:39,980 --> 00:11:42,200
couple years or they hit it on the

311
00:11:42,200 --> 00:11:43,760
webpage and then every time I taught

312
00:11:43,760 --> 00:11:45,590
this class I would always say vector

313
00:11:45,590 --> 00:11:47,270
wise is great too bad acne and killed it

314
00:11:47,270 --> 00:11:49,040
off and then like some dude emailed me

315
00:11:49,040 --> 00:11:50,240
last year or two years ago and said hey

316
00:11:50,240 --> 00:11:52,130
no no it's still around here's the

317
00:11:52,130 --> 00:11:53,360
webpage where it actually was like they

318
00:11:53,360 --> 00:11:55,040
like the website went out of its way to

319
00:11:55,040 --> 00:11:56,840
hide where you could actually download

320
00:11:56,840 --> 00:11:59,780
the software it was really bizarre but

321
00:11:59,780 --> 00:12:01,460
then they sort of came out and rebranded

322
00:12:01,460 --> 00:12:03,110
it now as Avalanche but they're selling

323
00:12:03,110 --> 00:12:05,060
this as like a that oh that in-memory

324
00:12:05,060 --> 00:12:08,750
OLAP cloud system in the AI runs in you

325
00:12:08,750 --> 00:12:13,280
know AWS or Azure so previous years the

326
00:12:13,280 --> 00:12:15,380
heavy paper the first part is really

327
00:12:15,380 --> 00:12:17,150
good the second part is not so much and

328
00:12:17,150 --> 00:12:19,190
it's a bit dated right it's 15 years ago

329
00:12:19,190 --> 00:12:20,630
so they spend a lot of time talking

330
00:12:20,630 --> 00:12:23,960
about like Itanium CPUs we don't exist

331
00:12:23,960 --> 00:12:27,800
anymore and they talk about how you know

332
00:12:27,800 --> 00:12:31,100
on Pentium 4s pipelines with like 31

333
00:12:31,100 --> 00:12:33,740
stages but obviously in in modern CPUs

334
00:12:33,740 --> 00:12:36,320
the stages are much smaller like on like

335
00:12:36,320 --> 00:12:39,230
as of like 2015 or when husband

336
00:12:39,230 --> 00:12:41,839
Broadwell came out it was like 14 stages

337
00:12:41,839 --> 00:12:44,540
even up the latest one like copper lake

338
00:12:44,540 --> 00:12:46,490
from intel's like I think nineteen

339
00:12:46,490 --> 00:12:48,560
stages or no copper like it's 14 stages

340
00:12:48,560 --> 00:12:52,280
the AMD Rison is 19 stages so building

341
00:12:52,280 --> 00:12:53,390
your system such a way to deal with

342
00:12:53,390 --> 00:12:54,890
these really long pipelines and the way

343
00:12:54,890 --> 00:12:58,370
they describe is not it still matters

344
00:12:58,370 --> 00:12:59,930
but not to the same extent of it they

345
00:12:59,930 --> 00:13:01,850
did back before the other reason why

346
00:13:01,850 --> 00:13:03,080
this paper is also super interesting is

347
00:13:03,080 --> 00:13:06,350
that multi-year bonds and Marcin

348
00:13:06,350 --> 00:13:09,200
Szarkowski Peter bonds went back to CWI

349
00:13:09,200 --> 00:13:12,740
and he's working on duck TB now Marcin

350
00:13:12,740 --> 00:13:14,720
basically went off off and co-founded

351
00:13:14,720 --> 00:13:16,520
snowflake and a lot of snowflakes

352
00:13:16,520 --> 00:13:19,250
designed is based on what what how dr.

353
00:13:19,250 --> 00:13:22,670
Weis laid out the system right so it is

354
00:13:22,670 --> 00:13:24,230
although vector wise is not that

355
00:13:24,230 --> 00:13:26,480
well-known certainly snowflake is super

356
00:13:26,480 --> 00:13:28,100
well-known now and it's it's you know

357
00:13:28,100 --> 00:13:31,430
it's it's the rightful successor of dr.

358
00:13:31,430 --> 00:13:33,950
wise okay so this is gonna be a crash

359
00:13:33,950 --> 00:13:36,500
course on in two slides of like

360
00:13:36,500 --> 00:13:39,110
everything you know about CPUs that

361
00:13:39,110 --> 00:13:43,430
matter for for databases right so as I

362
00:13:43,430 --> 00:13:46,010
said a CPU as wheat is v is going to

363
00:13:46,010 --> 00:13:47,600
execute instructions in terms of these

364
00:13:47,600 --> 00:13:50,630
pipeline stages I and look as I said the

365
00:13:50,630 --> 00:13:52,760
Intel CPUs have 14 pipelines or 14

366
00:13:52,760 --> 00:13:55,730
stages and the AMD has has 19 right so

367
00:13:55,730 --> 00:13:58,010
they're not in the hundreds it's it's

368
00:13:58,010 --> 00:14:00,980
pretty short but the end the idea is of

369
00:14:00,980 --> 00:14:02,300
these pipelines is that it's gonna allow

370
00:14:02,300 --> 00:14:05,240
the CPU to try to be busy at all times

371
00:14:05,240 --> 00:14:10,580
but I type line on different parts of

372
00:14:10,580 --> 00:14:13,910
the CPU so that way if one instruction

373
00:14:13,910 --> 00:14:16,430
is it has a catchment it has to go out

374
00:14:16,430 --> 00:14:17,870
to D Ram to get some data that needs to

375
00:14:17,870 --> 00:14:20,090
process but at that same cycle to see

376
00:14:20,090 --> 00:14:21,740
you can actually get an instruction that

377
00:14:21,740 --> 00:14:23,450
maybe has data already in the registers

378
00:14:23,450 --> 00:14:26,750
that it can execute efficiently so it's

379
00:14:26,750 --> 00:14:31,040
gonna allow us to execute to hide all

380
00:14:31,040 --> 00:14:32,930
the delays from these from these cache

381
00:14:32,930 --> 00:14:36,080
misses and but it's and it's gonna do

382
00:14:36,080 --> 00:14:37,790
this by executing the instructions out

383
00:14:37,790 --> 00:14:39,920
of the order in which they were it into

384
00:14:39,920 --> 00:14:41,930
the pipeline so what that means is like

385
00:14:41,930 --> 00:14:44,000
when you write your code the compiler

386
00:14:44,000 --> 00:14:45,860
turns it into CPU instructions the

387
00:14:45,860 --> 00:14:49,160
machine code it processes that stream of

388
00:14:49,160 --> 00:14:50,600
instructions allows them to the pipeline

389
00:14:50,600 --> 00:14:53,210
the CPU may not actually execute them in

390
00:14:53,210 --> 00:14:55,910
the same order that they were defined in

391
00:14:55,910 --> 00:14:58,610
that stream and that they're gonna track

392
00:14:58,610 --> 00:15:00,470
unreal things like dependencies to know

393
00:15:00,470 --> 00:15:02,270
that the output of one instruction is

394
00:15:02,270 --> 00:15:03,350
used as the input for the next

395
00:15:03,350 --> 00:15:04,910
instruction it has to make sure it

396
00:15:04,910 --> 00:15:08,690
executes in the correct order so the

397
00:15:08,690 --> 00:15:10,070
again these are called super superscalar

398
00:15:10,070 --> 00:15:14,750
CPUs and so because we are aware what

399
00:15:14,750 --> 00:15:16,130
the heart is actually going to do we're

400
00:15:16,130 --> 00:15:16,460
gonna

401
00:15:16,460 --> 00:15:18,490
try to build our database system

402
00:15:18,490 --> 00:15:20,210
execution engine where we actually

403
00:15:20,210 --> 00:15:21,410
process tuples cuz that's the most

404
00:15:21,410 --> 00:15:24,020
expensive part in such a way to to

405
00:15:24,020 --> 00:15:26,270
mitigate problems that can occur when

406
00:15:26,270 --> 00:15:27,830
there's mistakes in visions that the

407
00:15:27,830 --> 00:15:31,070
that the harbors gonna make so the first

408
00:15:31,070 --> 00:15:32,540
problem is going to be dependencies and

409
00:15:32,540 --> 00:15:33,920
this is what I said like if you have a

410
00:15:33,920 --> 00:15:36,440
instruction that that the input of that

411
00:15:36,440 --> 00:15:37,580
instruction depends on the output of

412
00:15:37,580 --> 00:15:39,590
another instruction the CPU can't

413
00:15:39,590 --> 00:15:41,450
execute the second one first it has to

414
00:15:41,450 --> 00:15:42,770
wait to the first one finished it before

415
00:15:42,770 --> 00:15:45,020
it exits the next one so there really

416
00:15:45,020 --> 00:15:46,520
isn't that much we can do in our

417
00:15:46,520 --> 00:15:51,320
database system to avoid this right when

418
00:15:51,320 --> 00:15:52,550
you think about it like you know if I

419
00:15:52,550 --> 00:15:54,920
need to go scan a tuple and the output

420
00:15:54,920 --> 00:15:57,320
of that tuple is then fed and put into a

421
00:15:57,320 --> 00:15:59,870
buffer I can't put that data into the

422
00:15:59,870 --> 00:16:01,280
buffer until I actually do that scan on

423
00:16:01,280 --> 00:16:03,380
the two ball all right so can we there's

424
00:16:03,380 --> 00:16:05,660
not that much we can do to avoid this

425
00:16:05,660 --> 00:16:07,880
problem the one that we are gonna try to

426
00:16:07,880 --> 00:16:09,800
avoid is is when we have branch

427
00:16:09,800 --> 00:16:12,350
mispredictions so in addition to

428
00:16:12,350 --> 00:16:14,210
executing instructions in a part of

429
00:16:14,210 --> 00:16:15,560
executing and you know multiple

430
00:16:15,560 --> 00:16:17,380
structions in the same in a single cycle

431
00:16:17,380 --> 00:16:20,060
when it sees a jump statement rat like

432
00:16:20,060 --> 00:16:21,920
to jump to a branch based on some

433
00:16:21,920 --> 00:16:24,140
conditional rather than waiting to see

434
00:16:24,140 --> 00:16:25,160
what that condition will actually

435
00:16:25,160 --> 00:16:26,510
evaluates to and then determine whether

436
00:16:26,510 --> 00:16:28,580
you go you know inside the if clause or

437
00:16:28,580 --> 00:16:30,920
go go around it it will actually try to

438
00:16:30,920 --> 00:16:32,990
predict what what path you're gonna take

439
00:16:32,990 --> 00:16:35,090
and start executing those instructions

440
00:16:35,090 --> 00:16:36,950
that that are followed in that and that

441
00:16:36,950 --> 00:16:39,920
conditional and then if it gets it wrong

442
00:16:39,920 --> 00:16:42,350
then it has to throw back throw away

443
00:16:42,350 --> 00:16:43,820
everything it's already done and then go

444
00:16:43,820 --> 00:16:45,620
back and refill the pipeline with the

445
00:16:45,620 --> 00:16:46,700
correct path that it should have gone

446
00:16:46,700 --> 00:16:48,830
down if it gets it right and this is

447
00:16:48,830 --> 00:16:50,900
fantastic right we basically did

448
00:16:50,900 --> 00:16:52,880
speculative execution and assume that

449
00:16:52,880 --> 00:16:54,080
you know we were to go down this branch

450
00:16:54,080 --> 00:16:55,700
and we got it right and all the work we

451
00:16:55,700 --> 00:16:57,560
did ahead of time is actually useful for

452
00:16:57,560 --> 00:17:00,260
us all right and again this is law it's

453
00:17:00,260 --> 00:17:01,520
gonna reduce the number of cycles that

454
00:17:01,520 --> 00:17:06,619
take to as we process queries so how the

455
00:17:06,619 --> 00:17:08,660
CPU actually does branch misprediction

456
00:17:08,660 --> 00:17:10,819
from our perspective if people build in

457
00:17:10,819 --> 00:17:12,829
the day to day system we don't know we

458
00:17:12,829 --> 00:17:14,329
don't care and actually unless you're

459
00:17:14,329 --> 00:17:16,160
like working at Intel AMD you're also

460
00:17:16,160 --> 00:17:18,230
not gonna know because this is like one

461
00:17:18,230 --> 00:17:19,490
of those secretive parts of the database

462
00:17:19,490 --> 00:17:22,880
system or the CPU right the simplest

463
00:17:22,880 --> 00:17:24,050
thing you could think of is like if you

464
00:17:24,050 --> 00:17:26,990
recognize you see a branch and you've

465
00:17:26,990 --> 00:17:28,339
been down that branch before you just

466
00:17:28,339 --> 00:17:29,740
take the path at the last

467
00:17:29,740 --> 00:17:31,510
you ever took right that's the easiest

468
00:17:31,510 --> 00:17:33,370
dumbest thing to do but obviously in

469
00:17:33,370 --> 00:17:36,850
these in Intel AMD AMD they're doing way

470
00:17:36,850 --> 00:17:38,200
more complicated things and what we're

471
00:17:38,200 --> 00:17:39,970
actually doing is is is a trade secret

472
00:17:39,970 --> 00:17:42,790
all right but we don't care what

473
00:17:42,790 --> 00:17:44,410
actually is we just we know that there

474
00:17:44,410 --> 00:17:46,179
is something that's doing branch

475
00:17:46,179 --> 00:17:48,100
prediction and we can design our code

476
00:17:48,100 --> 00:17:56,890
around it yes yeah so this question is

477
00:17:56,890 --> 00:17:59,110
like is this part of the problem people

478
00:17:59,110 --> 00:18:00,400
were hitting with like that in the

479
00:18:00,400 --> 00:18:02,080
specter or the meltdown stuff this is

480
00:18:02,080 --> 00:18:09,280
part of it yeah yeah okay as they said

481
00:18:09,280 --> 00:18:11,110
because they have long pipelines the

482
00:18:11,110 --> 00:18:13,300
CPUs can start executing specular

483
00:18:13,300 --> 00:18:15,280
branches for us and again the reason why

484
00:18:15,280 --> 00:18:17,200
we want to do this is gonna hide the

485
00:18:17,200 --> 00:18:21,940
stall okay we think we're gonna execute

486
00:18:21,940 --> 00:18:24,490
this let's go ahead if I get it then

487
00:18:24,490 --> 00:18:26,350
things are fantastic and that'll reduce

488
00:18:26,350 --> 00:18:30,429
my cycle count per instruction so we're

489
00:18:30,429 --> 00:18:32,740
we're gonna hit this the most is for

490
00:18:32,740 --> 00:18:34,960
analytical queries is as we start

491
00:18:34,960 --> 00:18:37,300
scanning tuples we have to evaluate

492
00:18:37,300 --> 00:18:38,920
predicates and our where clause to

493
00:18:38,920 --> 00:18:40,420
decide whether to put something in our

494
00:18:40,420 --> 00:18:42,820
buffer right as the output for this

495
00:18:42,820 --> 00:18:44,950
operator and that's just an if clause

496
00:18:44,950 --> 00:18:48,100
right so now if I have a billion tuples

497
00:18:48,100 --> 00:18:49,330
in my table and I'm doing a complete

498
00:18:49,330 --> 00:18:51,070
sequential scan on that table I'm gonna

499
00:18:51,070 --> 00:18:54,550
have a billion branches I have to

500
00:18:54,550 --> 00:18:57,330
protect potentially predict on in my CPU

501
00:18:57,330 --> 00:18:59,890
and now when you think about it too

502
00:18:59,890 --> 00:19:01,750
unless I'm pre sorting my data like

503
00:19:01,750 --> 00:19:04,210
Vertica the data could be completely

504
00:19:04,210 --> 00:19:07,330
random as I do my predicate you know for

505
00:19:07,330 --> 00:19:09,370
all my attributes when I do my predicate

506
00:19:09,370 --> 00:19:12,760
evaluation so the c2 is happening have

507
00:19:12,760 --> 00:19:14,980
no way to actually predict what branch

508
00:19:14,980 --> 00:19:16,690
are actually going to take because the

509
00:19:16,690 --> 00:19:18,070
the probability that you're gonna take

510
00:19:18,070 --> 00:19:21,040
the same branch as you did the last time

511
00:19:21,040 --> 00:19:23,230
depends on what the data actually looks

512
00:19:23,230 --> 00:19:24,429
like and the selectivity of your

513
00:19:24,429 --> 00:19:29,380
predicate so in a modern c++ you can

514
00:19:29,380 --> 00:19:33,010
actually so we'll see in the next slide

515
00:19:33,010 --> 00:19:34,630
how to design our data system to to

516
00:19:34,630 --> 00:19:39,220
avoid this in super those twenty you can

517
00:19:39,220 --> 00:19:41,980
pass a hint to the compiler called

518
00:19:41,980 --> 00:19:43,660
likely and unlikely when you have like

519
00:19:43,660 --> 00:19:46,030
if calls are a case statement we even

520
00:19:46,030 --> 00:19:48,220
say that I'm not likely to go down this

521
00:19:48,220 --> 00:19:50,350
branch or I am likely to go down this

522
00:19:50,350 --> 00:19:52,090
branch so you could imagine if you're

523
00:19:52,090 --> 00:19:53,560
doing koujun which again we'll talk

524
00:19:53,560 --> 00:19:54,280
about it on Monday

525
00:19:54,280 --> 00:19:55,840
you could start inserting these things

526
00:19:55,840 --> 00:19:57,730
if you know your predicate is not going

527
00:19:57,730 --> 00:20:00,400
to be selective and that's what most two

528
00:20:00,400 --> 00:20:01,600
boys are I'm not gonna get put you know

529
00:20:01,600 --> 00:20:03,790
I get satisfy the predicate and put me

530
00:20:03,790 --> 00:20:05,560
up a buffer you can inject these things

531
00:20:05,560 --> 00:20:07,030
to give hints to the cpu to say this is

532
00:20:07,030 --> 00:20:07,630
not gonna happen

533
00:20:07,630 --> 00:20:10,960
but even then the cpu actually can do a

534
00:20:10,960 --> 00:20:12,790
pretty good job in that case if you're

535
00:20:12,790 --> 00:20:14,140
not very selective log it can still do

536
00:20:14,140 --> 00:20:16,930
really well just do some of this this

537
00:20:16,930 --> 00:20:19,360
came up with project project one

538
00:20:19,360 --> 00:20:20,620
somebody was asking about this about

539
00:20:20,620 --> 00:20:22,210
branch mispredictions I just want to say

540
00:20:22,210 --> 00:20:23,560
there's a way to do this in school those

541
00:20:23,560 --> 00:20:25,450
20 to make this happen I think modern

542
00:20:25,450 --> 00:20:30,760
GCC and there's a intrinsic we make this

543
00:20:30,760 --> 00:20:32,740
happen in modern C not in GCC and clang

544
00:20:32,740 --> 00:20:34,810
they don't have exact keyword but you

545
00:20:34,810 --> 00:20:37,480
can force it to do this as well all

546
00:20:37,480 --> 00:20:41,520
right so let's look an example yes sorry

547
00:20:41,970 --> 00:20:44,620
compiler then I think gives there's an

548
00:20:44,620 --> 00:20:46,450
instruction I think in x86 to tell

549
00:20:46,450 --> 00:20:48,580
before the conditional in the actual

550
00:20:48,580 --> 00:20:50,950
assembly or the instruction stream to

551
00:20:50,950 --> 00:20:53,380
say like I'm not gonna I'm not like any

552
00:20:53,380 --> 00:20:55,800
great down this path it's a hint right

553
00:20:55,800 --> 00:20:58,150
all I said was likely I'm not saying

554
00:20:58,150 --> 00:21:00,310
like what for some probability the CPU

555
00:21:00,310 --> 00:21:01,840
may or may not take that into

556
00:21:01,840 --> 00:21:04,000
consideration and what it actually does

557
00:21:04,000 --> 00:21:07,680
when it sees this hint only in tonot

558
00:21:07,680 --> 00:21:12,940
right all right let's say yes let query

559
00:21:12,940 --> 00:21:15,730
us when we have two predicates where key

560
00:21:15,730 --> 00:21:17,170
greater than equal some low value and

561
00:21:17,170 --> 00:21:18,870
key less than equal to some high value

562
00:21:18,870 --> 00:21:22,210
so normally I tell my PhD students don't

563
00:21:22,210 --> 00:21:24,580
show code and slides but for this for

564
00:21:24,580 --> 00:21:25,810
this example we have to see some code

565
00:21:25,810 --> 00:21:27,820
but it should be pretty simple to to

566
00:21:27,820 --> 00:21:30,100
understand so a really simple way to

567
00:21:30,100 --> 00:21:32,620
implement this that select query is just

568
00:21:32,620 --> 00:21:34,180
a for loop on every single tuple in the

569
00:21:34,180 --> 00:21:36,520
table go grab the key you want to bow it

570
00:21:36,520 --> 00:21:38,770
over we're calls and then apply our

571
00:21:38,770 --> 00:21:40,810
predicate right if he's graduating too

572
00:21:40,810 --> 00:21:42,730
low and key less than equal to high then

573
00:21:42,730 --> 00:21:44,710
we're going to copy that tuple into our

574
00:21:44,710 --> 00:21:47,140
output buffer increment are all set in

575
00:21:47,140 --> 00:21:48,730
the output buffer so that was we come

576
00:21:48,730 --> 00:21:50,350
back around you know we can sort the

577
00:21:50,350 --> 00:21:55,120
next table right so the the bad part of

578
00:21:55,120 --> 00:21:56,470
this about this query is obviously this

579
00:21:56,470 --> 00:21:57,159
if clause

580
00:21:57,159 --> 00:22:01,690
because as I said the unless the key is

581
00:22:01,690 --> 00:22:04,239
you know gonna be almost always the same

582
00:22:04,239 --> 00:22:09,129
value then then like it could be

583
00:22:09,129 --> 00:22:11,049
completely random whether I'm gonna go

584
00:22:11,049 --> 00:22:12,609
down this path or not and and the CPU is

585
00:22:12,609 --> 00:22:13,359
gonna have a hard time actually

586
00:22:13,359 --> 00:22:15,599
predicting whether to do this or not

587
00:22:15,599 --> 00:22:18,970
alright and if I you know I could put

588
00:22:18,970 --> 00:22:21,190
likely or unlikely but again that's just

589
00:22:21,190 --> 00:22:22,509
a hint and it's and it may not be

590
00:22:22,509 --> 00:22:24,909
represented what the actual data is so

591
00:22:24,909 --> 00:22:26,799
this is how to do this scan using

592
00:22:26,799 --> 00:22:29,440
branching we can rewrite it though to do

593
00:22:29,440 --> 00:22:31,929
a branchless scan where we're still

594
00:22:31,929 --> 00:22:33,309
going to iterate over every single tuple

595
00:22:33,309 --> 00:22:35,979
but the very first thing we do is copy

596
00:22:35,979 --> 00:22:38,559
the tuple into the output buffer we

597
00:22:38,559 --> 00:22:41,349
don't evaluate the keys we just say we

598
00:22:41,349 --> 00:22:44,039
just copy it and then we have a little

599
00:22:44,039 --> 00:22:47,470
two ternary operations here where we go

600
00:22:47,470 --> 00:22:49,539
do a comparison of the keys

601
00:22:49,539 --> 00:22:51,639
based on our where clause predicate and

602
00:22:51,639 --> 00:22:53,799
what's happening is if the predicate

603
00:22:53,799 --> 00:22:55,960
matches then we at we have a one if it

604
00:22:55,960 --> 00:22:57,159
doesn't matter zero

605
00:22:57,159 --> 00:23:00,549
and the two things together and that

606
00:23:00,549 --> 00:23:01,989
then tells us whether it's a one or

607
00:23:01,989 --> 00:23:04,239
based on that that's gonna tell us

608
00:23:04,239 --> 00:23:05,759
whether we increment our counter or not

609
00:23:05,759 --> 00:23:10,210
so that if these both therefore zero

610
00:23:10,210 --> 00:23:14,859
then the offset will be zero when I loop

611
00:23:14,859 --> 00:23:18,669
back around when I come I'm just watchin

612
00:23:18,669 --> 00:23:21,580
it and then I need some code down here

613
00:23:21,580 --> 00:23:23,139
to make sure that goodness that last one

614
00:23:23,139 --> 00:23:24,759
didn't match that I don't include that

615
00:23:24,759 --> 00:23:27,159
my offer right but for simplicity I'm

616
00:23:27,159 --> 00:23:28,989
not doing that here so what's gonna

617
00:23:28,989 --> 00:23:32,470
happen is even those look like these

618
00:23:32,470 --> 00:23:36,700
conditionals pause the beam operating on

619
00:23:36,700 --> 00:23:40,090
energy directly the time and write the

620
00:23:40,090 --> 00:23:44,590
arithmetic operation right what is that

621
00:23:44,590 --> 00:23:51,009
I turned this into like keep my you

622
00:23:51,009 --> 00:23:52,600
check but it's

623
00:23:52,600 --> 00:23:55,240
check the actual bits using bit

624
00:23:55,240 --> 00:23:56,470
operators to see whether it's zero or

625
00:23:56,470 --> 00:23:59,710
not like we can rewrite all the more

626
00:23:59,710 --> 00:24:01,240
instructions that do just just do the

627
00:24:01,240 --> 00:24:07,300
math right so this seems like this would

628
00:24:07,300 --> 00:24:10,180
be terrible because for a billion t

629
00:24:10,180 --> 00:24:12,040
blows I'm copying all 1 billion of them

630
00:24:12,040 --> 00:24:15,370
and in this case here I'm only copying

631
00:24:15,370 --> 00:24:17,980
the ones that actually match but again

632
00:24:17,980 --> 00:24:19,980
depending on the selectivity of the

633
00:24:19,980 --> 00:24:21,940
predicate and what our data looks like

634
00:24:21,940 --> 00:24:24,070
this actually might be the better

635
00:24:24,070 --> 00:24:29,880
approach the graph later perfect and a

636
00:24:29,880 --> 00:24:33,070
scan on a table with the branching and

637
00:24:33,070 --> 00:24:34,780
the branchless version of that scan as I

638
00:24:34,780 --> 00:24:37,090
showed before and then the x-axis is

639
00:24:37,090 --> 00:24:38,470
varying the selectivity of that

640
00:24:38,470 --> 00:24:40,930
predicate so over here when the

641
00:24:40,930 --> 00:24:42,700
selectivity is low meaning most tuples

642
00:24:42,700 --> 00:24:44,860
are not gonna match like when no tuples

643
00:24:44,860 --> 00:24:46,780
match the branching one actually

644
00:24:46,780 --> 00:24:48,370
performs the best because again I'm

645
00:24:48,370 --> 00:24:51,700
checked my conditional and it doesn't

646
00:24:51,700 --> 00:24:53,320
match so I'm not wasting instructions

647
00:24:53,320 --> 00:24:55,780
doing the copying furthermore the CPU is

648
00:24:55,780 --> 00:24:57,100
gonna predict like the last you know

649
00:24:57,100 --> 00:24:59,110
this almost every single tuple is never

650
00:24:59,110 --> 00:25:00,730
gonna match that if calls so it's gonna

651
00:25:00,730 --> 00:25:03,310
predict to go around it and not do the

652
00:25:03,310 --> 00:25:07,270
copy as well but opt around 5% then it

653
00:25:07,270 --> 00:25:08,470
actually starts doing worse than the

654
00:25:08,470 --> 00:25:10,180
branch list case so the branchless one

655
00:25:10,180 --> 00:25:12,760
is a flat line because no matter what

656
00:25:12,760 --> 00:25:14,350
what whether the predicate evaluates are

657
00:25:14,350 --> 00:25:16,450
true or not I'm always paying that

658
00:25:16,450 --> 00:25:17,920
penalty to do the copying right so

659
00:25:17,920 --> 00:25:19,600
that's why it's almost a plateau

660
00:25:19,600 --> 00:25:21,640
straight across and as you can see up in

661
00:25:21,640 --> 00:25:24,550
here when it's like 50% selectivity 50%

662
00:25:24,550 --> 00:25:25,930
of the tuples are matching 50% of them

663
00:25:25,930 --> 00:25:27,430
aren't matching that's the worst case

664
00:25:27,430 --> 00:25:29,080
scenario because that's the CPU just

665
00:25:29,080 --> 00:25:31,240
can't predict this at all and only when

666
00:25:31,240 --> 00:25:34,240
you get down here 100 matches that does

667
00:25:34,240 --> 00:25:38,100
the CPU catch up yes

668
00:25:43,560 --> 00:25:48,750
the question is so you're saying this

669
00:25:48,750 --> 00:25:50,860
this graph would look different if you

670
00:25:50,860 --> 00:25:52,510
change the memory bandwidth speed or

671
00:25:52,510 --> 00:26:00,550
that well I mean like leave the memory

672
00:26:00,550 --> 00:26:05,920
of what for the scan yeah so like I have

673
00:26:05,920 --> 00:26:09,930
to do that scan for both cases right

674
00:26:10,380 --> 00:26:12,310
yeah but think about it like coming back

675
00:26:12,310 --> 00:26:17,020
here like I do this copy right

676
00:26:17,020 --> 00:26:19,600
if I come back around if this does I do

677
00:26:19,600 --> 00:26:22,360
a copy the tuple doesn't match I come

678
00:26:22,360 --> 00:26:23,890
back around when I do the next copy I'm

679
00:26:23,890 --> 00:26:25,690
overriding the last place I copied into

680
00:26:25,690 --> 00:26:27,310
memory and that's gonna be sitting my

681
00:26:27,310 --> 00:26:35,050
CPU caches anyway the transfer what do

682
00:26:35,050 --> 00:26:38,260
this copy is this is give me a CPU

683
00:26:38,260 --> 00:26:39,160
caches right

684
00:26:39,160 --> 00:26:40,360
I've read the two but I got to bring in

685
00:26:40,360 --> 00:26:42,670
my cache right I got to do that for both

686
00:26:42,670 --> 00:26:44,620
of them now I do this copy that's

687
00:26:44,620 --> 00:26:45,970
running from one cache location to

688
00:26:45,970 --> 00:26:47,980
another cache location probably gonna be

689
00:26:47,980 --> 00:26:54,880
an l1 right I mean it's it's the next

690
00:26:54,880 --> 00:26:56,590
instruction after this I bring the tuple

691
00:26:56,590 --> 00:26:59,590
my cache then I copy it and that's

692
00:26:59,590 --> 00:27:02,050
running to another cache location then I

693
00:27:02,050 --> 00:27:05,020
loop back around and the the yes by the

694
00:27:05,020 --> 00:27:06,760
time I come I do this and come back

695
00:27:06,760 --> 00:27:10,240
around and I do another copy it may be

696
00:27:10,240 --> 00:27:12,040
the case that the like if I'm

697
00:27:12,040 --> 00:27:13,750
overwriting the last one I copied that

698
00:27:13,750 --> 00:27:17,470
location got flushed my CPU cache maybe

699
00:27:17,470 --> 00:27:19,030
gets flushed from l1 but I think I'm

700
00:27:19,030 --> 00:27:22,150
lucky to get flushed from l2 l3 right

701
00:27:22,150 --> 00:27:23,260
this is a tight loop there's nothing

702
00:27:23,260 --> 00:27:24,670
else we're doing we just evaluate this

703
00:27:24,670 --> 00:27:28,150
one predicate there's no guarantee

704
00:27:28,150 --> 00:27:30,250
because we can't control how the caching

705
00:27:30,250 --> 00:27:35,950
policy of the CPU x86 you can give it

706
00:27:35,950 --> 00:27:39,630
hints but it doesn't not quite follow it

707
00:27:48,580 --> 00:27:51,230
yes this point which I agree with him is

708
00:27:51,230 --> 00:27:53,750
that I I don't know what other

709
00:27:53,750 --> 00:27:55,309
optimizations the CPU could be doing

710
00:27:55,309 --> 00:27:57,500
here sorry the compiler could be doing

711
00:27:57,500 --> 00:27:59,149
like it could unroll the loop so it's

712
00:27:59,149 --> 00:28:01,940
like four copies and then a bunch of

713
00:28:01,940 --> 00:28:07,100
these things and come back around but so

714
00:28:07,100 --> 00:28:09,080
again so this is what sisters research

715
00:28:09,080 --> 00:28:16,600
is right the the exact view

716
00:28:23,730 --> 00:28:30,490
and we see so the hypothesis is that

717
00:28:30,490 --> 00:28:34,750
when branching when most tuples don't

718
00:28:34,750 --> 00:28:37,630
match the penalty of paying that copying

719
00:28:37,630 --> 00:28:39,160
is it's not worth it because you're

720
00:28:39,160 --> 00:28:40,300
you're doing the second instruction

721
00:28:40,300 --> 00:28:44,950
maybe didn't need to write so I so I

722
00:28:44,950 --> 00:28:46,120
agree with you that there's a lot

723
00:28:46,120 --> 00:28:47,410
there's a lot of other things that could

724
00:28:47,410 --> 00:28:49,180
be happening here that the compiler

725
00:28:49,180 --> 00:28:53,770
could be doing but we leave like we can

726
00:28:53,770 --> 00:28:55,750
verify that this is actually working out

727
00:28:55,750 --> 00:28:57,580
correctly we think it is based on these

728
00:28:57,580 --> 00:29:18,010
results yes yeah so his question is this

729
00:29:18,010 --> 00:29:20,080
works fine if if you're doing a table

730
00:29:20,080 --> 00:29:22,090
scan because you've just been a rip

731
00:29:22,090 --> 00:29:23,520
through continuous regions of memory

732
00:29:23,520 --> 00:29:26,140
there's a memory prefetcher that if he

733
00:29:26,140 --> 00:29:27,220
knows were accessing things that are

734
00:29:27,220 --> 00:29:28,450
squinch alyttle bring those things and

735
00:29:28,450 --> 00:29:29,770
reach out cache misses so we get a lot

736
00:29:29,770 --> 00:29:31,570
of benefits from scanning columns if

737
00:29:31,570 --> 00:29:33,910
it's an index scan there's not much I

738
00:29:33,910 --> 00:29:35,950
can do as I probe down other than maybe

739
00:29:35,950 --> 00:29:38,920
prefetch in memory but once I get down

740
00:29:38,920 --> 00:29:40,960
the leaf nodes and now I start scanning

741
00:29:40,960 --> 00:29:43,660
along the leaf nodes am I gonna get the

742
00:29:43,660 --> 00:29:45,100
same kind of benefit if I do something

743
00:29:45,100 --> 00:29:50,500
like this for data in the same node like

744
00:29:50,500 --> 00:29:51,850
if I'm very cheap was in the same node

745
00:29:51,850 --> 00:29:53,890
and I don't have to go maybe look at the

746
00:29:53,890 --> 00:29:56,710
at the the the remaining parts of the

747
00:29:56,710 --> 00:30:00,070
tuple like if so if I have my key my

748
00:30:00,070 --> 00:30:03,010
index is based on this key but then

749
00:30:03,010 --> 00:30:04,510
there's also another part in the where

750
00:30:04,510 --> 00:30:05,890
clause that touches another attribute

751
00:30:05,890 --> 00:30:07,140
where I gotta then go look at the tuple

752
00:30:07,140 --> 00:30:09,910
then this probably is a bad idea or this

753
00:30:09,910 --> 00:30:11,260
probably was not gonna be as a big of a

754
00:30:11,260 --> 00:30:12,670
win but if I'm just looking at what I

755
00:30:12,670 --> 00:30:15,490
need exactly in that node then then I

756
00:30:15,490 --> 00:30:18,600
think that's what still work yes

757
00:30:20,820 --> 00:30:37,770
for what Claus yes your question is if

758
00:30:37,770 --> 00:30:40,440
you use the likely in this case here so

759
00:30:40,440 --> 00:30:44,940
like if our selectivity is 50% so likely

760
00:30:44,940 --> 00:30:47,130
and unlike he doesn't help us here so in

761
00:30:47,130 --> 00:30:49,070
that case could the compiler then

762
00:30:49,070 --> 00:31:17,340
rewrite the code or do what it's

763
00:31:17,340 --> 00:31:21,500
basically like if I had if then else

764
00:31:21,500 --> 00:31:24,150
you're saying like do I get any benefit

765
00:31:24,150 --> 00:31:27,120
from say the else clause is the one

766
00:31:27,120 --> 00:31:30,150
that's most likely to happen should I

767
00:31:30,150 --> 00:31:33,120
should I flip them I do whatever the the

768
00:31:33,120 --> 00:31:36,090
the boolean logic to reverse this so

769
00:31:36,090 --> 00:31:38,130
that the first thing that comes out of

770
00:31:38,130 --> 00:31:40,400
the if Clause is the likely one I

771
00:31:40,400 --> 00:31:45,020
actually don't know what they do yeah

772
00:31:47,420 --> 00:31:51,390
yeah yeah I see your point yeah yeah III

773
00:31:51,390 --> 00:31:56,730
actually don't know yeah actually a good

774
00:31:56,730 --> 00:31:59,090
point actually

775
00:32:02,550 --> 00:32:04,530
wellyes applies to a statement to allow

776
00:32:04,530 --> 00:32:05,760
the compiler to optimize with the case

777
00:32:05,760 --> 00:32:06,990
were passed that execution including a

778
00:32:06,990 --> 00:32:08,190
statement I'm more looking to more

779
00:32:08,190 --> 00:32:09,540
likely an alternate path action that is

780
00:32:09,540 --> 00:32:15,870
not in such a statement so yeah maybe I

781
00:32:15,870 --> 00:32:17,790
mean this is not very documented but

782
00:32:17,790 --> 00:32:19,530
maybe just maybe this is how they this

783
00:32:19,530 --> 00:32:28,620
is what is actually doing correct yeah

784
00:32:28,620 --> 00:32:30,660
yeah so his point is if you know whether

785
00:32:30,660 --> 00:32:32,250
you're gonna go down one branch or not

786
00:32:32,250 --> 00:32:34,380
you can make a decision about whether in

787
00:32:34,380 --> 00:32:35,760
line or not yeah I mean compilers are

788
00:32:35,760 --> 00:32:38,460
hold another beast it's not yeah what

789
00:32:38,460 --> 00:32:41,790
about a magazine article pilot expert

790
00:32:41,790 --> 00:32:43,380
will talk about compilers in the context

791
00:32:43,380 --> 00:32:45,030
of databases in the next class like

792
00:32:45,030 --> 00:32:48,800
things that we that's a good point

793
00:32:48,800 --> 00:32:54,900
okay so the other thing we want to try

794
00:32:54,900 --> 00:32:56,400
to also avoid it which is the first part

795
00:32:56,400 --> 00:32:57,810
we talked about of having a lot of

796
00:32:57,810 --> 00:33:02,220
instructions you you know use more

797
00:33:02,220 --> 00:33:03,570
instructions maybe that mean necessary

798
00:33:03,570 --> 00:33:07,620
the x-rays again if the if we design the

799
00:33:07,620 --> 00:33:10,580
system to be sort of general purpose a

800
00:33:10,580 --> 00:33:12,810
lot of times what you'll see in these

801
00:33:12,810 --> 00:33:14,580
databases are these giant switch clauses

802
00:33:14,580 --> 00:33:17,280
that that as you start evaluating

803
00:33:17,280 --> 00:33:20,160
predicates or evaluating tuples for

804
00:33:20,160 --> 00:33:22,110
every single type of data your your

805
00:33:22,110 --> 00:33:24,270
dailies can support you have these which

806
00:33:24,270 --> 00:33:26,370
call switch calls listen I have an

807
00:33:26,370 --> 00:33:27,810
integer and I'm adding it with integer

808
00:33:27,810 --> 00:33:29,700
do this if I'm adding it to a float and

809
00:33:29,700 --> 00:33:33,270
do that all that at inches all that adds

810
00:33:33,270 --> 00:33:34,830
you know misprediction that's gonna slow

811
00:33:34,830 --> 00:33:37,590
us down and it's just you know it's more

812
00:33:37,590 --> 00:33:38,970
instructions to actually you know

813
00:33:38,970 --> 00:33:41,130
evaluate the conditionals and decide

814
00:33:41,130 --> 00:33:43,620
whether what our type of data is and so

815
00:33:43,620 --> 00:33:45,630
I we need to share this a table before

816
00:33:45,630 --> 00:33:48,180
but this was an intro class if you go

817
00:33:48,180 --> 00:33:49,470
look at the Postgres source code of how

818
00:33:49,470 --> 00:33:50,850
they handle numerics like the fixed

819
00:33:50,850 --> 00:33:52,980
point decimals this is just the function

820
00:33:52,980 --> 00:33:55,650
to do add write to add to numerix

821
00:33:55,650 --> 00:33:57,480
together and you see there's all these

822
00:33:57,480 --> 00:33:59,460
if causes like if it's if it's negative

823
00:33:59,460 --> 00:34:00,390
it's positive

824
00:34:00,390 --> 00:34:01,740
if one you know if we're taking the

825
00:34:01,740 --> 00:34:03,270
absolute value of something break all

826
00:34:03,270 --> 00:34:05,340
this is problematic because this is a

827
00:34:05,340 --> 00:34:07,320
lot of instructions and a lot of chances

828
00:34:07,320 --> 00:34:11,219
for the CPU to get this wrong so we'll

829
00:34:11,219 --> 00:34:13,139
see this again on on Monday next week of

830
00:34:13,139 --> 00:34:16,199
how to specialize our specialized code

831
00:34:16,199 --> 00:34:20,329
Oh for what you need for that one quick

832
00:34:20,329 --> 00:34:22,649
and this is called cogeneration or

833
00:34:22,649 --> 00:34:25,949
just-in-time compilation all right so

834
00:34:25,949 --> 00:34:28,500
now we said we know how to we know what

835
00:34:28,500 --> 00:34:30,480
the CP is going to look like and we want

836
00:34:30,480 --> 00:34:32,129
to know how we want to design our

837
00:34:32,129 --> 00:34:35,250
operator implementations to be mindful

838
00:34:35,250 --> 00:34:36,899
of it now we're going to talk about how

839
00:34:36,899 --> 00:34:38,399
we're going to organize the system to

840
00:34:38,399 --> 00:34:40,199
process the queries which comprise on

841
00:34:40,199 --> 00:34:42,210
multiple operators so last class was

842
00:34:42,210 --> 00:34:43,770
talking about how we actually scheduled

843
00:34:43,770 --> 00:34:46,440
but the tasks that that X P of these

844
00:34:46,440 --> 00:34:48,480
operators but now we're just talk about

845
00:34:48,480 --> 00:34:53,040
a sort of a higher level con so they're

846
00:34:53,040 --> 00:34:53,940
gonna be different trade-offs we're

847
00:34:53,940 --> 00:34:55,889
gonna make depending on what type of

848
00:34:55,889 --> 00:34:57,329
workload we're gonna want to want to

849
00:34:57,329 --> 00:34:59,819
support like OLTP transactions there's a

850
00:34:59,819 --> 00:35:01,950
left analytical queries so there's the

851
00:35:01,950 --> 00:35:03,180
most common one the general-purpose one

852
00:35:03,180 --> 00:35:05,220
the iterator model materialisation model

853
00:35:05,220 --> 00:35:07,020
oh and a vectorized model so again the

854
00:35:07,020 --> 00:35:09,780
spoiler would be that the in a modern

855
00:35:09,780 --> 00:35:11,069
analytical system we're gonna want to

856
00:35:11,069 --> 00:35:13,109
use the vectorized model because it in

857
00:35:13,109 --> 00:35:16,559
pass comment being we can pass you know

858
00:35:16,559 --> 00:35:18,630
chunks of columns between operators and

859
00:35:18,630 --> 00:35:20,970
then use cindy instructions inside the

860
00:35:20,970 --> 00:35:24,710
operators to execute them efficiently

861
00:35:26,990 --> 00:35:29,700
this is some of the volcano model or the

862
00:35:29,700 --> 00:35:35,180
pipeline model bog volcano way-hey

863
00:35:35,720 --> 00:35:38,220
that came out in like the late 1980s

864
00:35:38,220 --> 00:35:40,980
early 1990s in addition to defining the

865
00:35:40,980 --> 00:35:42,089
exchange operator which I'll talk about

866
00:35:42,089 --> 00:35:44,460
in a second there's also the volcano

867
00:35:44,460 --> 00:35:45,410
query optimizer

868
00:35:45,410 --> 00:35:48,450
which we'll cover in a later in the

869
00:35:48,450 --> 00:35:50,069
semester which then was the the

870
00:35:50,069 --> 00:35:51,690
precursor to Cascades which is another

871
00:35:51,690 --> 00:35:53,369
optimizer optimization scheme and I'll

872
00:35:53,369 --> 00:35:56,549
talk about like this the the Gertz graph

873
00:35:56,549 --> 00:35:58,200
II was also the guy that that did the

874
00:35:58,200 --> 00:36:01,020
the wrote the the modern be B plus tree

875
00:36:01,020 --> 00:36:02,819
book that I sent you guys for the first

876
00:36:02,819 --> 00:36:04,500
project that's the guy's does amaze me

877
00:36:04,500 --> 00:36:07,319
stuff it's very influential so in the

878
00:36:07,319 --> 00:36:08,309
iterator model the way we're gonna

879
00:36:08,309 --> 00:36:09,740
implement it is that every single

880
00:36:09,740 --> 00:36:12,089
operator is gonna implement this this

881
00:36:12,089 --> 00:36:15,359
next function and what happened when

882
00:36:15,359 --> 00:36:17,339
someone calls neck on that operator it

883
00:36:17,339 --> 00:36:21,900
has to then return back one tuple and it

884
00:36:21,900 --> 00:36:26,670
could and since we organized tree that

885
00:36:26,670 --> 00:36:29,190
that operator needs data from its

886
00:36:29,190 --> 00:36:30,120
children it

887
00:36:30,120 --> 00:36:31,830
next on children and that sort of

888
00:36:31,830 --> 00:36:33,270
cascades to the bottom or you have the

889
00:36:33,270 --> 00:36:34,890
access methods where you're retrieving

890
00:36:34,890 --> 00:36:37,080
the data from the table or an index and

891
00:36:37,080 --> 00:36:40,050
then we we move the tuples up all right

892
00:36:40,050 --> 00:36:42,420
and essentially just we're gonna keep

893
00:36:42,420 --> 00:36:43,980
calling next next next next at the at

894
00:36:43,980 --> 00:36:45,270
the root and keep getting tuples and we

895
00:36:45,270 --> 00:36:47,130
get output for it for for our cursor

896
00:36:47,130 --> 00:36:49,800
until at some point we get it a good

897
00:36:49,800 --> 00:36:51,120
response and say there's no multiples

898
00:36:51,120 --> 00:36:53,520
available to us so at a high level it

899
00:36:53,520 --> 00:36:56,040
looks like this so say we have a two-way

900
00:36:56,040 --> 00:36:58,500
joint on table R and s and so for every

901
00:36:58,500 --> 00:37:00,090
single operator we're going to have this

902
00:37:00,090 --> 00:37:02,460
little for loop that typically it's

903
00:37:02,460 --> 00:37:05,100
gonna iterate over some some input data

904
00:37:05,100 --> 00:37:06,750
that it's getting either from the table

905
00:37:06,750 --> 00:37:09,510
or from its children and then admit them

906
00:37:09,510 --> 00:37:12,270
up so again think of these is just the

907
00:37:12,270 --> 00:37:14,790
next function so will my child up here

908
00:37:14,790 --> 00:37:16,680
the first the route we call next on our

909
00:37:16,680 --> 00:37:18,690
own a child we come down to this next

910
00:37:18,690 --> 00:37:20,640
block and it wants to do the join so

911
00:37:20,640 --> 00:37:22,530
it's gonna iterate over its left child

912
00:37:22,530 --> 00:37:25,620
come down here and say give me you mean

913
00:37:25,620 --> 00:37:26,790
the next two but you have and this will

914
00:37:26,790 --> 00:37:29,130
send back up a single tuple and we keep

915
00:37:29,130 --> 00:37:30,720
doing this until this is finished till

916
00:37:30,720 --> 00:37:31,890
we get all the tuples and we build our

917
00:37:31,890 --> 00:37:33,780
hash table then we fall down to the next

918
00:37:33,780 --> 00:37:34,380
for loop

919
00:37:34,380 --> 00:37:36,600
call next on this child and then it's

920
00:37:36,600 --> 00:37:37,740
just the same thing it starts moving

921
00:37:37,740 --> 00:37:40,830
tuples up to from the s table then do

922
00:37:40,830 --> 00:37:42,060
our probe in the hash table and then

923
00:37:42,060 --> 00:37:43,320
anything that matches gets shoved up

924
00:37:43,320 --> 00:37:51,020
here alright so this approach yes sir

925
00:38:01,280 --> 00:38:07,370
okay yes

926
00:38:07,370 --> 00:38:10,550
so so we're not that's we're not there

927
00:38:10,550 --> 00:38:11,930
yet so this is like that would be like

928
00:38:11,930 --> 00:38:14,660
how do we one of the CPUs doing when the

929
00:38:14,660 --> 00:38:16,910
processing think of this is like there's

930
00:38:16,910 --> 00:38:18,620
some place that you think a one thread

931
00:38:18,620 --> 00:38:20,990
calls next on the route that calls next

932
00:38:20,990 --> 00:38:22,970
on this child right and there's one

933
00:38:22,970 --> 00:38:24,080
instance that's iterating with this

934
00:38:24,080 --> 00:38:25,820
tuple but I said it's a blocking call so

935
00:38:25,820 --> 00:38:27,830
when I call next this is not processing

936
00:38:27,830 --> 00:38:28,760
this is waiting

937
00:38:28,760 --> 00:38:33,560
okay so fusion almost every single debut

938
00:38:33,560 --> 00:38:36,440
system you've ever heard about the

939
00:38:36,440 --> 00:38:37,580
advantage of this is that we're gonna be

940
00:38:37,580 --> 00:38:39,920
able to do pipelining meaning like in

941
00:38:39,920 --> 00:38:43,660
this case here for as we emit a to best

942
00:38:43,660 --> 00:38:50,200
ride that way all the way because

943
00:38:50,200 --> 00:38:52,760
everything we need to nor the process a

944
00:38:52,760 --> 00:38:55,010
single tuple is available to us at that

945
00:38:55,010 --> 00:38:56,270
point of time when we do it we're

946
00:38:56,270 --> 00:38:57,980
calling next right I can take this to

947
00:38:57,980 --> 00:39:00,770
chuck here or the predicate put the

948
00:39:00,770 --> 00:39:02,930
probe for the hash table and that that

949
00:39:02,930 --> 00:39:04,730
matches then I show up here and do the

950
00:39:04,730 --> 00:39:06,590
projection and then I can produce that

951
00:39:06,590 --> 00:39:09,260
two boys my output so that's considered

952
00:39:09,260 --> 00:39:10,730
a pipeline the fact we take one tuple

953
00:39:10,730 --> 00:39:13,430
and ride it all the way up into in this

954
00:39:13,430 --> 00:39:15,920
case here the output of the query or if

955
00:39:15,920 --> 00:39:17,720
there's a pipeline breaker or we can't

956
00:39:17,720 --> 00:39:19,010
go any farther then it gets buffered up

957
00:39:19,010 --> 00:39:24,170
there so that's the rest area so some

958
00:39:24,170 --> 00:39:25,610
operators are out the block until they

959
00:39:25,610 --> 00:39:27,370
get all the Cho to emit all their tuples

960
00:39:27,370 --> 00:39:30,800
and the other finish you get from the

961
00:39:30,800 --> 00:39:32,300
iterator model is that output control is

962
00:39:32,300 --> 00:39:34,340
really easy because we don't the push

963
00:39:34,340 --> 00:39:36,500
down any logic to do limits necessarily

964
00:39:36,500 --> 00:39:38,720
because if I know I only want to get 10

965
00:39:38,720 --> 00:39:40,970
to poles as for my query then I just

966
00:39:40,970 --> 00:39:43,460
call stop calling next on the route once

967
00:39:43,460 --> 00:39:46,190
I get 10 to poles I've seen everything

968
00:39:46,190 --> 00:39:49,460
and so down here this is just this is

969
00:39:49,460 --> 00:39:53,060
just a sample of the of the of the

970
00:39:53,060 --> 00:39:54,890
databases that use this approach like

971
00:39:54,890 --> 00:39:56,960
these are ones I can confirm by looking

972
00:39:56,960 --> 00:39:58,460
at the documentation or looking at the

973
00:39:58,460 --> 00:39:59,540
source code but

974
00:39:59,540 --> 00:40:06,349
[Laughter]

975
00:40:06,349 --> 00:40:08,329
the iterator model it's the canonical

976
00:40:08,329 --> 00:40:15,079
way yes compilation this is again this

977
00:40:15,079 --> 00:40:17,239
is different than compilation this is

978
00:40:17,239 --> 00:40:18,979
orthogonal this is like you just do do

979
00:40:18,979 --> 00:40:21,529
compilation right I could still take all

980
00:40:21,529 --> 00:40:22,309
these four loops

981
00:40:22,309 --> 00:40:25,309
pop and run it and that would still be

982
00:40:25,309 --> 00:40:27,439
using the iterator model in the case of

983
00:40:27,439 --> 00:40:30,979
hyper they're doing they're still doing

984
00:40:30,979 --> 00:40:32,390
this and that's another thing we need to

985
00:40:32,390 --> 00:40:34,339
talk about as well like this is a this

986
00:40:34,339 --> 00:40:36,049
is a top-down approach meaning I start

987
00:40:36,049 --> 00:40:38,709
at the top and I sort of pull tuples up

988
00:40:38,709 --> 00:40:41,089
the reverse of this would be a push

989
00:40:41,089 --> 00:40:43,489
approach we start at the bottom and you

990
00:40:43,489 --> 00:40:45,319
have this for loop and start here rather

991
00:40:45,319 --> 00:40:47,329
than here and you start emitting tuples

992
00:40:47,329 --> 00:40:53,150
up hyper does that most systems do the

993
00:40:53,150 --> 00:40:55,369
top-down iterator model that's in those

994
00:40:55,369 --> 00:41:04,640
common one yet that's next next awesome

995
00:41:04,640 --> 00:41:05,359
yeah

996
00:41:05,359 --> 00:41:08,779
I realize it's like it's like kind of

997
00:41:08,779 --> 00:41:09,859
lame for me to give this lecture and

998
00:41:09,859 --> 00:41:11,659
like oh yeah here's something cool next

999
00:41:11,659 --> 00:41:15,909
class right it's like people's first and

1000
00:41:16,269 --> 00:41:20,989
alright so the other one here is that

1001
00:41:20,989 --> 00:41:23,449
rather than having an X where that only

1002
00:41:23,449 --> 00:41:27,349
gives back a single tuple I'm gonna have

1003
00:41:27,349 --> 00:41:31,069
every operator all the tuples that is

1004
00:41:31,069 --> 00:41:32,630
ever going to produce all at once and

1005
00:41:32,630 --> 00:41:35,900
then shove that to the next operator and

1006
00:41:35,900 --> 00:41:38,150
again I could be either doing this down

1007
00:41:38,150 --> 00:41:40,819
like you call you know get nax or next

1008
00:41:40,819 --> 00:41:41,839
and then instead of getting one tuple

1009
00:41:41,839 --> 00:41:43,669
you get a you get everything all once or

1010
00:41:43,669 --> 00:41:45,859
I could push it from the bottom up like

1011
00:41:45,859 --> 00:41:48,199
run run the operator get the output and

1012
00:41:48,199 --> 00:41:49,789
then put shove it up to the next guy

1013
00:41:49,789 --> 00:41:53,569
alright so in the materialization model

1014
00:41:53,569 --> 00:41:55,819
you can either do material as an entire

1015
00:41:55,819 --> 00:41:57,799
row or a single column like if you're

1016
00:41:57,799 --> 00:41:59,479
doing analytics it doesn't make sense to

1017
00:41:59,479 --> 00:42:01,339
materialize the entire all the

1018
00:42:01,339 --> 00:42:03,019
attributes of a tuple in your output

1019
00:42:03,019 --> 00:42:05,089
buffer if you know that you know most of

1020
00:42:05,089 --> 00:42:06,589
the most of the table most of the query

1021
00:42:06,589 --> 00:42:08,269
is not gonna need those at columns at

1022
00:42:08,269 --> 00:42:09,739
all or all the rest of the queries that

1023
00:42:09,739 --> 00:42:11,329
means the comments at all

1024
00:42:11,329 --> 00:42:13,849
and so this big same thing you need to

1025
00:42:13,849 --> 00:42:15,949
be a comm store or a row store and do

1026
00:42:15,949 --> 00:42:18,690
this so going

1027
00:42:18,690 --> 00:42:20,680
instead of having the next function we

1028
00:42:20,680 --> 00:42:23,170
just have this in every single operator

1029
00:42:23,170 --> 00:42:24,730
now we're gonna allocate an output

1030
00:42:24,730 --> 00:42:26,650
buffer and then we just keep filling it

1031
00:42:26,650 --> 00:42:28,390
up with tuples that match and then when

1032
00:42:28,390 --> 00:42:29,769
we're done we shove it up

1033
00:42:29,769 --> 00:42:32,259
so again super Mike from the top down I

1034
00:42:32,259 --> 00:42:34,509
call output function on this guy he

1035
00:42:34,509 --> 00:42:36,220
calls the epitope function this guy he

1036
00:42:36,220 --> 00:42:37,630
puts all the two plus if he has in a

1037
00:42:37,630 --> 00:42:39,609
buffer puts it on this thing now he can

1038
00:42:39,609 --> 00:42:42,609
iterate over this this output buffer and

1039
00:42:42,609 --> 00:42:44,769
build out his hash table and then we do

1040
00:42:44,769 --> 00:42:46,210
the same thing for this other one here

1041
00:42:46,210 --> 00:42:51,579
and we shove data up alright so in this

1042
00:42:51,579 --> 00:42:54,279
example here this is this is this is

1043
00:42:54,279 --> 00:42:56,319
like the this is a naive implementation

1044
00:42:56,319 --> 00:42:58,589
this is obviously really stupid because

1045
00:42:58,589 --> 00:43:00,819
for some of these things I actually can

1046
00:43:00,819 --> 00:43:03,009
combine together what the operator is

1047
00:43:03,009 --> 00:43:05,589
actually doing right so in this case

1048
00:43:05,589 --> 00:43:08,230
here what am i doing up I'm scanning the

1049
00:43:08,230 --> 00:43:10,630
table s putting all the tuples in table

1050
00:43:10,630 --> 00:43:12,849
s into my output buffer then passing

1051
00:43:12,849 --> 00:43:14,319
that output buffer now to this operator

1052
00:43:14,319 --> 00:43:15,910
which is just going to iterate over that

1053
00:43:15,910 --> 00:43:18,670
and if I with my predicate so a better

1054
00:43:18,670 --> 00:43:19,989
idea was obviously just combine these

1055
00:43:19,989 --> 00:43:21,670
two operators together so I do that one

1056
00:43:21,670 --> 00:43:24,279
scan as you just do scan the table

1057
00:43:24,279 --> 00:43:26,049
evaluate the predicate and if I see the

1058
00:43:26,049 --> 00:43:30,180
match then I put in my output buffer I

1059
00:43:30,180 --> 00:43:32,680
and you could do the branch this one or

1060
00:43:32,680 --> 00:43:33,880
you could do the the branching one and

1061
00:43:33,880 --> 00:43:36,339
it depends on the implementation so

1062
00:43:36,339 --> 00:43:38,680
although this seems like in the naive

1063
00:43:38,680 --> 00:43:39,910
example this seems really stupid you

1064
00:43:39,910 --> 00:43:41,829
don't want to do this like there are

1065
00:43:41,829 --> 00:43:44,650
optimizations you can do to make this go

1066
00:43:44,650 --> 00:43:46,119
faster and you do other things like if I

1067
00:43:46,119 --> 00:43:47,890
know I've have a limit clause of above

1068
00:43:47,890 --> 00:43:49,450
and I only want two n-tuples I could

1069
00:43:49,450 --> 00:43:52,779
push that down and eat it as well all

1070
00:43:52,779 --> 00:43:56,920
right so is my opinion that the

1071
00:43:56,920 --> 00:43:59,849
materialization model is the best for

1072
00:43:59,849 --> 00:44:04,869
for quality workloads because these

1073
00:44:04,869 --> 00:44:06,700
operators are these queries only want to

1074
00:44:06,700 --> 00:44:08,049
touch a small number of tuples at a time

1075
00:44:08,049 --> 00:44:10,269
so the size of the output buffer that

1076
00:44:10,269 --> 00:44:11,859
I'm shoving up to the next operator is

1077
00:44:11,859 --> 00:44:13,900
it not going to be that big go get an

1078
00:44:13,900 --> 00:44:16,089
ease account record on as Amazon you

1079
00:44:16,089 --> 00:44:17,829
know Amazon's website that's one tuple

1080
00:44:17,829 --> 00:44:19,269
that I mean need to move from one hopper

1081
00:44:19,269 --> 00:44:21,869
to the next right and we're gonna

1082
00:44:21,869 --> 00:44:25,029
benefits because we call the operator

1083
00:44:25,029 --> 00:44:27,609
once we call that output function once

1084
00:44:27,609 --> 00:44:29,259
we get all the tuples we're ever going

1085
00:44:29,259 --> 00:44:30,400
to need for the operator we never go

1086
00:44:30,400 --> 00:44:31,670
back to it again

1087
00:44:31,670 --> 00:44:33,740
and we shove the data long and that

1088
00:44:33,740 --> 00:44:35,270
reduces the number of function calls

1089
00:44:35,270 --> 00:44:37,099
which are going to be expensive because

1090
00:44:37,099 --> 00:44:39,230
those are jumps in our instruction

1091
00:44:39,230 --> 00:44:41,480
stream that the see us execute so for

1092
00:44:41,480 --> 00:44:43,400
only I think this is this is the right

1093
00:44:43,400 --> 00:44:45,710
way to go and then when we built H door

1094
00:44:45,710 --> 00:44:47,089
which was then commercialized volte B

1095
00:44:47,089 --> 00:44:50,390
this is how we how we did it Monay DB

1096
00:44:50,390 --> 00:44:52,369
did it as well although they're trying

1097
00:44:52,369 --> 00:44:54,890
to do this for analytics and so there

1098
00:44:54,890 --> 00:44:56,059
was a bunch of papers that they had to

1099
00:44:56,059 --> 00:44:57,890
come up with to actually overcome this

1100
00:44:57,890 --> 00:44:59,930
issue of like trying to materialize all

1101
00:44:59,930 --> 00:45:02,599
the data at once in memory high-rise

1102
00:45:02,599 --> 00:45:04,220
originally does as well there's a German

1103
00:45:04,220 --> 00:45:07,430
academic system the old version of this

1104
00:45:07,430 --> 00:45:09,049
approach the new version does not they

1105
00:45:09,049 --> 00:45:11,000
rewrote it to do the vectorize model and

1106
00:45:11,000 --> 00:45:13,390
then surprisingly I think this is true

1107
00:45:13,390 --> 00:45:16,250
Teradata does the same thing but they're

1108
00:45:16,250 --> 00:45:19,730
like a massive scale you know parallel

1109
00:45:19,730 --> 00:45:21,319
data warehouse so they're running OLAP

1110
00:45:21,319 --> 00:45:23,180
queries so they have a bunch of crap

1111
00:45:23,180 --> 00:45:24,680
they have to do to do bunch of push

1112
00:45:24,680 --> 00:45:26,990
downs as much as possible and basically

1113
00:45:26,990 --> 00:45:29,660
inlining operators within each other to

1114
00:45:29,660 --> 00:45:31,760
avoid having to you know move data in

1115
00:45:31,760 --> 00:45:33,650
wholesale from one you know one no to

1116
00:45:33,650 --> 00:45:35,720
the next I need to double check this but

1117
00:45:35,720 --> 00:45:37,160
I'm pretty sure this is how it works as

1118
00:45:37,160 --> 00:45:37,720
well

1119
00:45:37,720 --> 00:45:41,210
surprising because you know well tear

1120
00:45:41,210 --> 00:45:43,670
data was designed in 1979 so it's before

1121
00:45:43,670 --> 00:45:46,690
sort of the vectorized model came along

1122
00:45:46,690 --> 00:45:48,890
alright so the last one is this

1123
00:45:48,890 --> 00:45:50,329
vectorize model so this seems sort of

1124
00:45:50,329 --> 00:45:52,490
obvious to us now but like before the

1125
00:45:52,490 --> 00:45:55,670
x100 paper from from peter bouncing and

1126
00:45:55,670 --> 00:45:58,910
marcin marcin people just didn't build

1127
00:45:58,910 --> 00:46:00,140
database systems this way they either

1128
00:46:00,140 --> 00:46:02,480
did the materialization model or the or

1129
00:46:02,480 --> 00:46:05,510
the the iterator model so with the

1130
00:46:05,510 --> 00:46:07,280
vector prize model is basically like

1131
00:46:07,280 --> 00:46:08,390
iterator where you have this next

1132
00:46:08,390 --> 00:46:09,619
function and put obviously instead of

1133
00:46:09,619 --> 00:46:11,569
sitting sending a single tuple which

1134
00:46:11,569 --> 00:46:12,799
would be expense to do if we have to

1135
00:46:12,799 --> 00:46:14,660
scan a lot of tuples all at once we can

1136
00:46:14,660 --> 00:46:17,960
send a batch of tuples and the size of

1137
00:46:17,960 --> 00:46:19,520
the batch is going to depend on what the

1138
00:46:19,520 --> 00:46:21,170
harbor is gonna look like depending on

1139
00:46:21,170 --> 00:46:22,460
like whether we can be operator we're

1140
00:46:22,460 --> 00:46:24,950
gonna feed it into is gonna be able to

1141
00:46:24,950 --> 00:46:28,369
do vectorize execution right on sim DS

1142
00:46:28,369 --> 00:46:29,809
so if you know the size of our senior

1143
00:46:29,809 --> 00:46:31,250
editors we can then make decisions of

1144
00:46:31,250 --> 00:46:36,280
how big our batches shouldn't be so

1145
00:46:36,280 --> 00:46:38,599
going back to our example here now

1146
00:46:38,599 --> 00:46:39,799
slightly more complicated we still have

1147
00:46:39,799 --> 00:46:41,510
our next function but now when we call

1148
00:46:41,510 --> 00:46:43,820
next instead of getting back

1149
00:46:43,820 --> 00:46:46,550
a single tuple we're gonna get back a

1150
00:46:46,550 --> 00:46:48,740
batch of tuples and then inside the

1151
00:46:48,740 --> 00:46:51,200
kernel here when we do our for loop we

1152
00:46:51,200 --> 00:46:54,350
could do you know vectorize instructions

1153
00:46:54,350 --> 00:46:56,030
to execute those things and in parallel

1154
00:46:56,030 --> 00:46:58,040
efficiently take that batch apply all

1155
00:46:58,040 --> 00:46:59,180
the predicates with a single sim you

1156
00:46:59,180 --> 00:47:03,110
look at the same thing now for the other

1157
00:47:03,110 --> 00:47:06,770
side so this is ideal for OLAP queries

1158
00:47:06,770 --> 00:47:08,750
again because we reducing the number

1159
00:47:08,750 --> 00:47:10,640
invocations per operator for moving

1160
00:47:10,640 --> 00:47:12,680
tuples around from one Operator to the

1161
00:47:12,680 --> 00:47:14,150
next in such a way that we can execute

1162
00:47:14,150 --> 00:47:17,080
vectorize instructions very efficiently

1163
00:47:17,080 --> 00:47:19,550
most date and a little database systems

1164
00:47:19,550 --> 00:47:22,700
built in the last 10 years are gonna

1165
00:47:22,700 --> 00:47:25,520
follow follow this approach so see both

1166
00:47:25,520 --> 00:47:27,230
server and db2 and Oracle if you just

1167
00:47:27,230 --> 00:47:29,360
get the regular general-purpose row

1168
00:47:29,360 --> 00:47:30,680
store version of these database systems

1169
00:47:30,680 --> 00:47:32,540
it's all gonna be using the iterator

1170
00:47:32,540 --> 00:47:34,580
model but then they have the specialized

1171
00:47:34,580 --> 00:47:35,900
execution engine is like the fractured

1172
00:47:35,900 --> 00:47:39,610
mirror stuff we talked about for Oracle

1173
00:47:39,610 --> 00:47:42,140
db2 has this accelerator called blue

1174
00:47:42,140 --> 00:47:43,820
right these are all sort of these

1175
00:47:43,820 --> 00:47:45,890
standalone copies of data that have

1176
00:47:45,890 --> 00:47:48,230
vectorize execution models and they get

1177
00:47:48,230 --> 00:47:50,060
better performance cockroach DB is

1178
00:47:50,060 --> 00:47:52,010
actually not and surprisingly they have

1179
00:47:52,010 --> 00:47:53,630
a blog article that shows that they have

1180
00:47:53,630 --> 00:47:58,940
a vector engine and then in our new

1181
00:47:58,940 --> 00:48:00,230
system that we're working on here

1182
00:48:00,230 --> 00:48:03,160
everything's effect threads as well ok

1183
00:48:03,160 --> 00:48:09,830
yes question is is the size of the

1184
00:48:09,830 --> 00:48:15,110
vector aligned to the same knee size in

1185
00:48:15,110 --> 00:48:22,040
our system it's a line to the seamy size

1186
00:48:22,040 --> 00:48:23,750
but it's not going exactly the same knee

1187
00:48:23,750 --> 00:48:26,000
register size so like if you're if

1188
00:48:26,000 --> 00:48:28,520
you're simi registers are 512 512 bits

1189
00:48:28,520 --> 00:48:30,290
it's not like you want to pass around

1190
00:48:30,290 --> 00:48:32,270
only five and twelve bits you would pass

1191
00:48:32,270 --> 00:48:34,610
around maybe like like ten chunks that

1192
00:48:34,610 --> 00:48:35,930
are each five and twelve bits so they

1193
00:48:35,930 --> 00:48:38,420
like you can take a chunk within that

1194
00:48:38,420 --> 00:48:40,460
vector and then do the vectors execution

1195
00:48:40,460 --> 00:48:45,020
on it and typically the way that works

1196
00:48:45,020 --> 00:48:46,550
is and that's what we do in our system

1197
00:48:46,550 --> 00:48:48,740
when it when you turn on the database

1198
00:48:48,740 --> 00:48:50,900
system they go read information from

1199
00:48:50,900 --> 00:48:53,060
from the CPU like would your assembly

1200
00:48:53,060 --> 00:48:54,770
register sizes where your cache size and

1201
00:48:54,770 --> 00:48:55,930
then you can make a decision how

1202
00:48:55,930 --> 00:48:59,680
had a had a size things up that way it's

1203
00:48:59,680 --> 00:49:05,020
usually just heuristics okay so I

1204
00:49:05,020 --> 00:49:06,640
already said this before right there's

1205
00:49:06,640 --> 00:49:08,920
yeah I'm sure all my examples I was

1206
00:49:08,920 --> 00:49:11,170
showing top to bottom but you can

1207
00:49:11,170 --> 00:49:12,820
actually go bottom the top all right

1208
00:49:12,820 --> 00:49:14,530
this one is the most common because this

1209
00:49:14,530 --> 00:49:16,390
to people you know this is like the

1210
00:49:16,390 --> 00:49:18,400
textbook implementation of how people

1211
00:49:18,400 --> 00:49:20,710
vote database systems the paper you'll

1212
00:49:20,710 --> 00:49:23,200
read for next week from hyper shows that

1213
00:49:23,200 --> 00:49:25,360
you actually want to use a bottom-to-top

1214
00:49:25,360 --> 00:49:27,910
approach because now you can be very

1215
00:49:27,910 --> 00:49:29,950
careful about how you organize the code

1216
00:49:29,950 --> 00:49:32,740
for the extra queries so that within

1217
00:49:32,740 --> 00:49:35,040
your pipeline you're not just riding up

1218
00:49:35,040 --> 00:49:38,350
values from tuples within your CPU

1219
00:49:38,350 --> 00:49:40,570
caches they can go even more lower-level

1220
00:49:40,570 --> 00:49:42,250
and try to control things like so they

1221
00:49:42,250 --> 00:49:44,410
remain the CPU registers because that's

1222
00:49:44,410 --> 00:49:46,750
gonna be even faster than then the cache

1223
00:49:46,750 --> 00:49:50,980
l1 cache in our old system which was

1224
00:49:50,980 --> 00:49:52,300
that we have this technique called relax

1225
00:49:52,300 --> 00:49:54,970
operator fusion which I'll cover in

1226
00:49:54,970 --> 00:49:57,130
later lectures but this is actually now

1227
00:49:57,130 --> 00:50:00,280
in our new system as well the idea here

1228
00:50:00,280 --> 00:50:03,340
is that we're trying to combine the the

1229
00:50:03,340 --> 00:50:05,290
bottom-to-top approach with vector is

1230
00:50:05,290 --> 00:50:07,510
execution and we're trying to get the

1231
00:50:07,510 --> 00:50:09,100
best of both worlds as hyper in vector

1232
00:50:09,100 --> 00:50:11,200
wise so we're trying to pass vectors and

1233
00:50:11,200 --> 00:50:13,780
do bottom bottom to top and need to be

1234
00:50:13,780 --> 00:50:15,430
careful about where you sort of put your

1235
00:50:15,430 --> 00:50:17,830
your buffer boundaries so that you can

1236
00:50:17,830 --> 00:50:19,720
stage things in a certain size so that

1237
00:50:19,720 --> 00:50:21,520
you can pass things along and that can

1238
00:50:21,520 --> 00:50:23,020
fit in steeply registers sort of what he

1239
00:50:23,020 --> 00:50:25,240
was saying and it may not always be the

1240
00:50:25,240 --> 00:50:27,130
pipeline breaker point it may be points

1241
00:50:27,130 --> 00:50:30,280
within within a pipeline I think I will

1242
00:50:30,280 --> 00:50:33,030
cover that more later on this buster

1243
00:50:33,030 --> 00:50:35,940
all right so now let's talk about

1244
00:50:35,940 --> 00:50:37,630
regardless of whether we're doing a

1245
00:50:37,630 --> 00:50:39,520
materialization you know what processing

1246
00:50:39,520 --> 00:50:41,260
model were using we can talk about how

1247
00:50:41,260 --> 00:50:44,110
we actually run parallel queries so

1248
00:50:44,110 --> 00:50:45,160
we've already talked about how to do

1249
00:50:45,160 --> 00:50:46,840
inner query pedals in before but that

1250
00:50:46,840 --> 00:50:47,890
was the scheduling stuff we talked about

1251
00:50:47,890 --> 00:50:50,410
last class or we talked about well the

1252
00:50:50,410 --> 00:50:51,460
idea here is this we're gonna allow

1253
00:50:51,460 --> 00:50:53,560
multiple queries to run at the same time

1254
00:50:53,560 --> 00:50:57,430
and so you basically have a you know a

1255
00:50:57,430 --> 00:50:59,530
scheduling mechanism to decide you know

1256
00:50:59,530 --> 00:51:01,660
what tasks for what query runs hyper

1257
00:51:01,660 --> 00:51:03,280
original just had it we can only run one

1258
00:51:03,280 --> 00:51:04,360
query at a time

1259
00:51:04,360 --> 00:51:05,830
when that query is done then you switch

1260
00:51:05,830 --> 00:51:08,080
to the next one in the new system they

1261
00:51:08,080 --> 00:51:09,550
don't do that and most

1262
00:51:09,550 --> 00:51:12,280
you don't do that right because you you

1263
00:51:12,280 --> 00:51:14,170
want to have the system be responsible

1264
00:51:14,170 --> 00:51:15,940
as possible so sometimes you have like a

1265
00:51:15,940 --> 00:51:18,270
fast Q fast query Q and a slow query Q

1266
00:51:18,270 --> 00:51:20,440
so the different techniques and then we

1267
00:51:20,440 --> 00:51:23,110
use concurrent role to protect the the

1268
00:51:23,110 --> 00:51:26,970
data if queries are updating things so

1269
00:51:26,970 --> 00:51:29,260
this is a conjecture of mine I can't

1270
00:51:29,260 --> 00:51:30,910
prove it but it's after I thought about

1271
00:51:30,910 --> 00:51:33,310
for a little bit I don't think that

1272
00:51:33,310 --> 00:51:36,820
there is any difference in in the

1273
00:51:36,820 --> 00:51:39,010
complexity of implementing a different

1274
00:51:39,010 --> 00:51:42,820
query processing model the complexity

1275
00:51:42,820 --> 00:51:44,230
doesn't change if you're using different

1276
00:51:44,230 --> 00:51:45,670
control schemes meaning like if I'm

1277
00:51:45,670 --> 00:51:47,950
doing two-phase locking or OCC it

1278
00:51:47,950 --> 00:51:49,210
doesn't matter whether I'm using the

1279
00:51:49,210 --> 00:51:50,970
materialization model or the the

1280
00:51:50,970 --> 00:51:54,520
vectorize model I I think they're

1281
00:51:54,520 --> 00:51:57,220
they're they're isolated from me from

1282
00:51:57,220 --> 00:51:58,750
each other enough that it doesn't

1283
00:51:58,750 --> 00:52:01,210
actually really matter because a my

1284
00:52:01,210 --> 00:52:02,770
access method want to go that's tuple

1285
00:52:02,770 --> 00:52:04,090
that's when it goes and check to see

1286
00:52:04,090 --> 00:52:05,260
whether I can actually read something or

1287
00:52:05,260 --> 00:52:06,700
not whether it's being held you know

1288
00:52:06,700 --> 00:52:08,470
something else lock on it and all that

1289
00:52:08,470 --> 00:52:11,740
doesn't matter if it for this alright

1290
00:52:11,740 --> 00:52:12,880
well we care about though though is

1291
00:52:12,880 --> 00:52:14,770
intro query parallelism and that's how

1292
00:52:14,770 --> 00:52:16,900
we take a single query and now X key to

1293
00:52:16,900 --> 00:52:18,790
all its operators in parallel again the

1294
00:52:18,790 --> 00:52:19,960
scheduling stuff was last week this is

1295
00:52:19,960 --> 00:52:23,080
how we assign these tasks to cores now

1296
00:52:23,080 --> 00:52:25,900
it's basically how do we then organize

1297
00:52:25,900 --> 00:52:28,840
the flow of data within our query plan

1298
00:52:28,840 --> 00:52:31,360
so that we we can determine whether one

1299
00:52:31,360 --> 00:52:34,330
task is allowed to run yeah run yet or

1300
00:52:34,330 --> 00:52:45,330
not so the two approaches mean we're at

1301
00:52:46,050 --> 00:52:48,370
horizontally crossing the crepe and

1302
00:52:48,370 --> 00:52:49,460
vertical pair

1303
00:52:49,460 --> 00:52:51,440
I can have different parts of the query

1304
00:52:51,440 --> 00:52:53,420
plan execute at the same time again I

1305
00:52:53,420 --> 00:52:54,950
need to be mindful about what data

1306
00:52:54,950 --> 00:52:56,510
dependencies I have between these

1307
00:52:56,510 --> 00:52:58,760
different tasks to determine whether I'm

1308
00:52:58,760 --> 00:53:03,530
okay and again although I'm laying out

1309
00:53:03,530 --> 00:53:06,470
as he approaches it's not like either or

1310
00:53:06,470 --> 00:53:08,300
you can actually combine these these two

1311
00:53:08,300 --> 00:53:10,160
things together you can do parallelism

1312
00:53:10,160 --> 00:53:12,109
and vertical piles and together as

1313
00:53:12,109 --> 00:53:15,730
within the same query there also be

1314
00:53:15,730 --> 00:53:18,500
within every single operator there also

1315
00:53:18,500 --> 00:53:21,140
be a parallel version that we can use

1316
00:53:21,140 --> 00:53:22,280
execute the query

1317
00:53:22,280 --> 00:53:24,050
all right do that operator and that's

1318
00:53:24,050 --> 00:53:26,240
what books aren't covering next week or

1319
00:53:26,240 --> 00:53:33,650
they're my parallel joins in sorting but

1320
00:53:33,650 --> 00:53:36,080
Twitter's our operators then are

1321
00:53:36,080 --> 00:53:38,810
instantiated into or as separate

1322
00:53:38,810 --> 00:53:41,030
instances and they're all good they're

1323
00:53:41,030 --> 00:53:42,740
gonna perform all the instances of the

1324
00:53:42,740 --> 00:53:44,330
operate are gonna form the same function

1325
00:53:44,330 --> 00:53:47,060
just on different pieces of data this is

1326
00:53:47,060 --> 00:53:49,190
the more some of it last time break my

1327
00:53:49,190 --> 00:53:50,990
table up into horizontal motions and

1328
00:53:50,990 --> 00:53:52,820
then the different instances are gonna

1329
00:53:52,820 --> 00:53:54,200
run through different different

1330
00:53:54,200 --> 00:53:56,330
partitions at the same time so to

1331
00:53:56,330 --> 00:53:58,099
coordinate now these different instances

1332
00:53:58,099 --> 00:54:04,099
running the same time is they basically

1333
00:54:04,099 --> 00:54:07,339
another way a breakpoint in our query

1334
00:54:07,339 --> 00:54:09,170
plan so that we can recognize that we

1335
00:54:09,170 --> 00:54:11,030
can't proceed up into the query plan

1336
00:54:11,030 --> 00:54:13,490
until all the operator instances below

1337
00:54:13,490 --> 00:54:16,670
us produce all the tuples that they're

1338
00:54:16,670 --> 00:54:18,740
supposed to produce there's a wait again

1339
00:54:18,740 --> 00:54:20,150
for us to organize the query plan and

1340
00:54:20,150 --> 00:54:23,540
keep track of internally whether you

1341
00:54:23,540 --> 00:54:25,040
know whether one set of tasks of

1342
00:54:25,040 --> 00:54:27,859
operations are allowed to run or not so

1343
00:54:27,859 --> 00:54:31,280
say we have simple query on beam so we

1344
00:54:31,280 --> 00:54:34,910
can take this scan on a and we can break

1345
00:54:34,910 --> 00:54:35,900
that up into different operator

1346
00:54:35,900 --> 00:54:37,640
instances and each of these guys are

1347
00:54:37,640 --> 00:54:39,580
gonna run on us and in a separate worker

1348
00:54:39,580 --> 00:54:43,520
and then now because I want to do I want

1349
00:54:43,520 --> 00:54:45,440
to find other operators within my

1350
00:54:45,440 --> 00:54:48,770
pipeline I can also do the the filter as

1351
00:54:48,770 --> 00:54:51,320
well so take the output of each of these

1352
00:54:51,320 --> 00:54:52,820
scans and feed that immediately into

1353
00:54:52,820 --> 00:54:55,040
this filter operator to then remove

1354
00:54:55,040 --> 00:54:57,830
anything that shouldn't be there but now

1355
00:54:57,830 --> 00:54:59,810
also I can do other organizations like

1356
00:54:59,810 --> 00:55:01,910
if I have this projection up here that

1357
00:55:01,910 --> 00:55:03,170
shows me that i only need

1358
00:55:03,170 --> 00:55:05,960
I only need eight hid for my query and

1359
00:55:05,960 --> 00:55:09,589
say this a stable a has a thousand two

1360
00:55:09,589 --> 00:55:11,869
pools so rather than me copying a

1361
00:55:11,869 --> 00:55:13,940
thousand tuples up from one operative

1362
00:55:13,940 --> 00:55:16,490
annex I can actually push down now make

1363
00:55:16,490 --> 00:55:18,260
a copy of the projection here to filter

1364
00:55:18,260 --> 00:55:19,819
out everything just the except for a I

1365
00:55:19,819 --> 00:55:22,490
do I need here so then now they're gonna

1366
00:55:22,490 --> 00:55:23,869
do the for the join they need to build

1367
00:55:23,869 --> 00:55:25,579
the hash table and I'm not declaring

1368
00:55:25,579 --> 00:55:28,579
whether this hash table is a single hash

1369
00:55:28,579 --> 00:55:29,990
table or whether this partition it

1370
00:55:29,990 --> 00:55:33,040
doesn't matter but I know I can't now do

1371
00:55:33,040 --> 00:55:35,540
anything on this side I can't start

1372
00:55:35,540 --> 00:55:37,309
scanning B and doing the program in the

1373
00:55:37,309 --> 00:55:39,559
hash table until the hash table is

1374
00:55:39,559 --> 00:55:41,630
actually built otherwise I could get

1375
00:55:41,630 --> 00:55:43,280
false negatives right ID look up and say

1376
00:55:43,280 --> 00:55:45,440
it is my hash table contain this key it

1377
00:55:45,440 --> 00:55:49,900
should but I just haven't gotten yet I

1378
00:55:49,900 --> 00:55:53,119
wait these brought before proceeding

1379
00:55:53,119 --> 00:55:55,220
with the next one so this is what the

1380
00:55:55,220 --> 00:55:56,900
exchange operator is doing for us it's

1381
00:55:56,900 --> 00:55:58,430
basically now a way to coalesce the

1382
00:55:58,430 --> 00:55:59,869
results from running from these

1383
00:55:59,869 --> 00:56:01,880
different tasks running on different

1384
00:56:01,880 --> 00:56:03,980
workers and keep track of I can't

1385
00:56:03,980 --> 00:56:08,390
proceed until this is done then now for

1386
00:56:08,390 --> 00:56:10,819
for this on this side right I'm doing

1387
00:56:10,819 --> 00:56:12,740
the scan on B same thing also do the

1388
00:56:12,740 --> 00:56:15,650
filter do my push down my projection now

1389
00:56:15,650 --> 00:56:18,260
I probe my hash table in parallel right

1390
00:56:18,260 --> 00:56:20,270
and each of these guys are gonna produce

1391
00:56:20,270 --> 00:56:22,490
an output but to make sure that I don't

1392
00:56:22,490 --> 00:56:24,200
produce the final result of the query

1393
00:56:24,200 --> 00:56:26,780
until they've all finished I add another

1394
00:56:26,780 --> 00:56:29,000
exchange operator up here just knows

1395
00:56:29,000 --> 00:56:30,410
that I'm waiting for three threads to

1396
00:56:30,410 --> 00:56:31,760
give me all the results and then once I

1397
00:56:31,760 --> 00:56:32,869
have everything then I can show up the

1398
00:56:32,869 --> 00:56:36,829
output to the two up above right so now

1399
00:56:36,829 --> 00:56:38,510
my example here I put the exchange

1400
00:56:38,510 --> 00:56:41,270
operator up after we build the hash

1401
00:56:41,270 --> 00:56:42,859
table this is on a bottle we definitely

1402
00:56:42,859 --> 00:56:45,700
we definitely need this but I could have

1403
00:56:45,700 --> 00:56:50,410
set the query up such that I have a I

1404
00:56:50,410 --> 00:56:53,089
have an exchange operator here where I

1405
00:56:53,089 --> 00:56:54,740
do the scan do the filter do the

1406
00:56:54,740 --> 00:56:58,089
projection and then all my my my

1407
00:56:58,089 --> 00:57:00,650
operator instance tasks start filling in

1408
00:57:00,650 --> 00:57:02,540
an output buffer in this exchange

1409
00:57:02,540 --> 00:57:04,160
operator and then once they're all done

1410
00:57:04,160 --> 00:57:06,589
and that's all done then now I can flat

1411
00:57:06,589 --> 00:57:10,549
have the theory the queries blast the or

1412
00:57:10,549 --> 00:57:12,920
do the join and produce results so

1413
00:57:12,920 --> 00:57:14,180
there's different alternatives to doing

1414
00:57:14,180 --> 00:57:16,520
exiting this query and again it depends

1415
00:57:16,520 --> 00:57:16,850
on

1416
00:57:16,850 --> 00:57:18,950
what the dista tends on the selectivity

1417
00:57:18,950 --> 00:57:20,870
of the predicate attends the selectivity

1418
00:57:20,870 --> 00:57:23,390
of the join clause the datum could try

1419
00:57:23,390 --> 00:57:24,560
to figure out what the right way to do

1420
00:57:24,560 --> 00:57:26,870
this is or there's not one plan that

1421
00:57:26,870 --> 00:57:31,760
works for everyone all right so now for

1422
00:57:31,760 --> 00:57:34,310
inter operator parallelism the vertical

1423
00:57:34,310 --> 00:57:37,130
parallelism the idea here is that we can

1424
00:57:37,130 --> 00:57:40,190
overlap now different operators running

1425
00:57:40,190 --> 00:57:42,050
at the same time we can still have to

1426
00:57:42,050 --> 00:57:43,520
use an exchange operator to keep track

1427
00:57:43,520 --> 00:57:45,710
of whether you know a operator instance

1428
00:57:45,710 --> 00:57:48,260
a set of operator instances have

1429
00:57:48,260 --> 00:57:49,490
produced all the results or so to

1430
00:57:49,490 --> 00:57:52,280
produce but instead of you know doing

1431
00:57:52,280 --> 00:57:53,900
one and then you know a bunch of work

1432
00:57:53,900 --> 00:57:55,100
and then switching over to the next one

1433
00:57:55,100 --> 00:57:57,890
I could have a thread start processing

1434
00:57:57,890 --> 00:58:00,560
data as it arrives from the query plan

1435
00:58:00,560 --> 00:58:05,660
below it and do it in parallel right so

1436
00:58:05,660 --> 00:58:07,520
this is sometimes cut line parallel

1437
00:58:07,520 --> 00:58:09,020
meaning I'm running the pipeline's in

1438
00:58:09,020 --> 00:58:12,530
parallel so say I'm doing a Cartesian

1439
00:58:12,530 --> 00:58:15,410
product across four tables I join a B

1440
00:58:15,410 --> 00:58:17,780
and C and D with no no--don't calls

1441
00:58:17,780 --> 00:58:19,310
right but you wouldn't actually do this

1442
00:58:19,310 --> 00:58:24,230
but here it is right and so the query

1443
00:58:24,230 --> 00:58:25,970
would look like this where I can now run

1444
00:58:25,970 --> 00:58:28,790
the a and then B and do that in parallel

1445
00:58:28,790 --> 00:58:31,220
but it says I know I can't do the the

1446
00:58:31,220 --> 00:58:34,870
remaining join on on the scene D output

1447
00:58:34,870 --> 00:58:38,300
until that until this is all done then I

1448
00:58:38,300 --> 00:58:40,400
put my exchange operator so what will

1449
00:58:40,400 --> 00:58:42,980
happen is I could have one thread do the

1450
00:58:42,980 --> 00:58:45,170
join on a and B fill up some output

1451
00:58:45,170 --> 00:58:47,390
buffer here right and then which mean

1452
00:58:47,390 --> 00:58:49,670
building out the hash table as well then

1453
00:58:49,670 --> 00:58:51,080
another thing I can do that join and C

1454
00:58:51,080 --> 00:58:54,590
and D but then once and then as these

1455
00:58:54,590 --> 00:58:57,830
guys are and for the person the join

1456
00:58:57,830 --> 00:59:02,630
result got this thing nap time

1457
00:59:02,630 --> 00:59:03,790
right

1458
00:59:03,790 --> 00:59:05,380
because in this case here there's no

1459
00:59:05,380 --> 00:59:06,880
where clause

1460
00:59:06,880 --> 00:59:09,340
there's no join clause to determine

1461
00:59:09,340 --> 00:59:11,860
whether something to match or not so I'm

1462
00:59:11,860 --> 00:59:13,090
doing Cartesian product so I want to

1463
00:59:13,090 --> 00:59:15,310
take any tuple that comes out of C and D

1464
00:59:15,310 --> 00:59:17,230
and mashed-up fit any tuple that matches

1465
00:59:17,230 --> 00:59:18,940
with a and B so I don't need to wait for

1466
00:59:18,940 --> 00:59:20,380
these guys technically the fairness they

1467
00:59:20,380 --> 00:59:21,940
can search shoving all the data out in

1468
00:59:21,940 --> 00:59:24,190
parallel and have it you know certain

1469
00:59:24,190 --> 00:59:26,110
start computing the rest of the joint as

1470
00:59:26,110 --> 00:59:32,140
well right okay so to finish it real

1471
00:59:32,140 --> 00:59:36,130
quickly the theme we can really talk

1472
00:59:36,130 --> 00:59:37,930
about too is also like how to determine

1473
00:59:37,930 --> 00:59:39,580
the number of workers we're going to use

1474
00:59:39,580 --> 00:59:41,050
we talked a little bit of a little bit

1475
00:59:41,050 --> 00:59:43,870
about the last time of how we want to

1476
00:59:43,870 --> 00:59:45,520
organize the sort of the sketching

1477
00:59:45,520 --> 00:59:47,500
mechanism but we never really decided

1478
00:59:47,500 --> 00:59:49,560
okay well I had this number of cores

1479
00:59:49,560 --> 00:59:52,090
haven't and this number tasks how many

1480
00:59:52,090 --> 00:59:54,190
workers should actually use and as I

1481
00:59:54,190 --> 00:59:56,050
already sort of alluded to in that one

1482
00:59:56,050 --> 00:59:59,770
example with the with the horizontal

1483
00:59:59,770 --> 01:00:02,470
parallelism it depends on what the the

1484
01:00:02,470 --> 01:00:04,630
data looks like what the selectivity of

1485
01:00:04,630 --> 01:00:05,830
the predicate is and how much output

1486
01:00:05,830 --> 01:00:08,890
data I'm gonna generate so one simple

1487
01:00:08,890 --> 01:00:10,750
way to do this you can in the case of

1488
01:00:10,750 --> 01:00:12,700
hyper you just have one worker per core

1489
01:00:12,700 --> 01:00:14,740
and you just pin them to the core that

1490
01:00:14,740 --> 01:00:16,060
they're actually running and then

1491
01:00:16,060 --> 01:00:17,170
another approach is you have multiple

1492
01:00:17,170 --> 01:00:18,220
workers per core which is the Hana

1493
01:00:18,220 --> 01:00:20,920
approach and the idea here is that if

1494
01:00:20,920 --> 01:00:24,010
one one worker ever blocks then we just

1495
01:00:24,010 --> 01:00:26,170
you know we let other cores run run

1496
01:00:26,170 --> 01:00:27,280
threads run in the cores at the same

1497
01:00:27,280 --> 01:00:31,120
time right the last thing to talk about

1498
01:00:31,120 --> 01:00:33,850
again is this that is is I think we

1499
01:00:33,850 --> 01:00:34,720
already cover this last class actually

1500
01:00:34,720 --> 01:00:37,060
the push versus pull yeah actually we

1501
01:00:37,060 --> 01:00:38,320
ignore this this is just the sketching

1502
01:00:38,320 --> 01:00:39,160
stuff I don't know why this is here

1503
01:00:39,160 --> 01:00:42,040
sorry this is just saying that like in

1504
01:00:42,040 --> 01:00:44,580
the case of hyper I was pulling from a

1505
01:00:44,580 --> 01:00:48,040
from a global queue whereas in the case

1506
01:00:48,040 --> 01:00:50,200
of Hana I was pushing things into the

1507
01:00:50,200 --> 01:00:51,610
queue and then the threads had to take

1508
01:00:51,610 --> 01:00:56,250
down right all right so to finish up so

1509
01:00:56,250 --> 01:01:00,910
the as I said today it was seen as more

1510
01:01:00,910 --> 01:01:03,160
on Monday the easiest way for us as

1511
01:01:03,160 --> 01:01:04,270
humans to implement parts of our

1512
01:01:04,270 --> 01:01:05,830
database system may turn out be the

1513
01:01:05,830 --> 01:01:07,330
worst way for the CPU to actually

1514
01:01:07,330 --> 01:01:09,520
execute this and so if we're aware and

1515
01:01:09,520 --> 01:01:10,960
what the harbor looks like how it's

1516
01:01:10,960 --> 01:01:12,940
going to behave we may not know exactly

1517
01:01:12,940 --> 01:01:15,640
how that's gonna be but we at least be

1518
01:01:15,640 --> 01:01:17,230
mindful of it'll look

1519
01:01:17,230 --> 01:01:19,390
we can design the execution code for our

1520
01:01:19,390 --> 01:01:22,630
data system to be optimal for with the

1521
01:01:22,630 --> 01:01:24,250
Harvard with the CPU actually once and

1522
01:01:24,250 --> 01:01:27,880
then as we see in today throughout the

1523
01:01:27,880 --> 01:01:30,280
rest of the semester the it's my opinion

1524
01:01:30,280 --> 01:01:31,810
that vectorize the bottom of execution

1525
01:01:31,810 --> 01:01:32,859
approach will be the best way to always

1526
01:01:32,859 --> 01:01:36,480
exit olaf lab queries for OLTP it'll be

1527
01:01:36,480 --> 01:01:39,790
bottom-up materialization but most

1528
01:01:39,790 --> 01:01:41,290
systems that a lot of times like you

1529
01:01:41,290 --> 01:01:44,740
think like Postgres my siegel db2 oracle

1530
01:01:44,740 --> 01:01:46,240
all of these are sort of designed to be

1531
01:01:46,240 --> 01:01:47,950
general-purpose systems and that's what

1532
01:01:47,950 --> 01:01:51,190
the iterator tries to be like instead of

1533
01:01:51,190 --> 01:01:54,090
good for everyone yes

1534
01:01:54,570 --> 01:01:56,350
why is it matter of Bodmin for the

1535
01:01:56,350 --> 01:02:00,340
top-down fewer of function calls right

1536
01:02:00,340 --> 01:02:02,500
so like for bottom up in the

1537
01:02:02,500 --> 01:02:05,890
channelization model i I called the

1538
01:02:05,890 --> 01:02:08,470
operator the execution function for that

1539
01:02:08,470 --> 01:02:10,570
operator it runs produces some output

1540
01:02:10,570 --> 01:02:12,850
take that output now call the next

1541
01:02:12,850 --> 01:02:15,730
function on on and so forth right if

1542
01:02:15,730 --> 01:02:17,950
you're going top down it's like call

1543
01:02:17,950 --> 01:02:19,480
this call this call this I guess this

1544
01:02:19,480 --> 01:02:22,720
stuff is still the same for

1545
01:02:22,720 --> 01:02:27,630
materialization it doesn't matter for

1546
01:02:27,630 --> 01:02:53,590
vectorization yes yes he's right it's

1547
01:02:53,590 --> 01:02:55,480
its inner execution is that of a crisp

1548
01:02:55,480 --> 01:02:59,770
execution and then when you have like

1549
01:02:59,770 --> 01:03:03,340
the the as far as my understanding is

1550
01:03:03,340 --> 01:03:04,780
the best code you can have for compiler

1551
01:03:04,780 --> 01:03:08,109
if it's just a bunch of no conditionals

1552
01:03:08,109 --> 01:03:10,270
everything's sort of one instruction

1553
01:03:10,270 --> 01:03:11,859
after another because that the hot the

1554
01:03:11,859 --> 01:03:13,510
compiler can look event holistically and

1555
01:03:13,510 --> 01:03:19,900
make better decisions okay you have a

1556
01:03:19,900 --> 01:03:24,730
smaller cross leg as well yes that

1557
01:03:24,730 --> 01:03:27,990
typically is not gonna be an issue

1558
01:03:27,990 --> 01:03:33,670
query plans but it's like put like like

1559
01:03:33,670 --> 01:03:38,490
the call stack you know it's not I mean

1560
01:03:38,490 --> 01:03:41,260
yeah but I can't imagine is gonna be

1561
01:03:41,260 --> 01:03:43,270
like a million function calls okay like

1562
01:03:43,270 --> 01:03:45,490
even operators in query plan that that's

1563
01:03:45,490 --> 01:03:48,220
the onion door I can't even example like

1564
01:03:48,220 --> 01:03:51,520
that that's a the query can't be big but

1565
01:03:51,520 --> 01:03:55,330
like the theme no one's doing 1 million

1566
01:03:55,330 --> 01:03:57,850
table joints I think the the highest

1567
01:03:57,850 --> 01:04:02,170
number solve was from I saw a talk from

1568
01:04:02,170 --> 01:04:05,350
Hana people recently where they had one

1569
01:04:05,350 --> 01:04:07,120
query that was doing a join of 1,500

1570
01:04:07,120 --> 01:04:10,000
tables it's a lot but it's not a million

1571
01:04:10,000 --> 01:04:14,290
so it's it's a it's not gonna be the

1572
01:04:14,290 --> 01:04:16,140
call stack I don't think it's me bad bit

1573
01:04:16,140 --> 01:04:19,120
alright so again I I realized again I

1574
01:04:19,120 --> 01:04:20,950
keep saying oh well cover this on Monday

1575
01:04:20,950 --> 01:04:22,630
we'll cover this in one day so the

1576
01:04:22,630 --> 01:04:24,550
compilation stuff which is stuff on

1577
01:04:24,550 --> 01:04:28,600
Monday I that is is the paper meeting is

1578
01:04:28,600 --> 01:04:30,850
from the hyper guy and I'm saying god I

1579
01:04:30,850 --> 01:04:33,130
finger ler like Thomas Newman he that

1580
01:04:33,130 --> 01:04:34,750
system and wrote the paper by himself

1581
01:04:34,750 --> 01:04:37,810
which is crazy and it's it's a bit dense

1582
01:04:37,810 --> 01:04:39,940
because it shows little of all L of M

1583
01:04:39,940 --> 01:04:41,530
bar which you don't need it for yeah

1584
01:04:41,530 --> 01:04:44,070
there's any idea what they're doing

1585
01:04:44,070 --> 01:04:46,450
organize the query plan is such a way

1586
01:04:46,450 --> 01:04:48,400
that it's ideal for the compiler to then

1587
01:04:48,400 --> 01:04:51,670
generate CPU efficient code is what the

1588
01:04:51,670 --> 01:04:54,910
main takeaway should be okay question

1589
01:04:54,910 --> 01:05:01,170
yes yes yes

1590
01:05:01,980 --> 01:05:04,049
this question is should those data

1591
01:05:04,049 --> 01:05:06,240
points for Cindy instructions again will

1592
01:05:06,240 --> 01:05:08,369
cover Cindy in way more detail we'll

1593
01:05:08,369 --> 01:05:10,230
have to do two lectures on it question

1594
01:05:10,230 --> 01:05:12,359
is for Cindy instructions do the data

1595
01:05:12,359 --> 01:05:13,559
points you want to put into the same

1596
01:05:13,559 --> 01:05:15,180
instruction do they need to be

1597
01:05:15,180 --> 01:05:16,980
contiguous so the way it works is

1598
01:05:16,980 --> 01:05:18,660
there's there's a there's a vectorized

1599
01:05:18,660 --> 01:05:21,119
register and you have to do loads into

1600
01:05:21,119 --> 01:05:23,819
that the the data you're loading in

1601
01:05:23,819 --> 01:05:27,990
doesn't have to be all you have to put

1602
01:05:27,990 --> 01:05:29,819
it in application it actually continues

1603
01:05:29,819 --> 01:05:31,680
memories he PU cache and then write that

1604
01:05:31,680 --> 01:05:33,990
in I don't think you can do like load

1605
01:05:33,990 --> 01:05:35,040
multiple times from like different

1606
01:05:35,040 --> 01:05:39,619
locations so if now the data is in

1607
01:05:39,619 --> 01:05:41,940
different locations in memory you have

1608
01:05:41,940 --> 01:05:43,530
been copying into a single location then

1609
01:05:43,530 --> 01:05:45,240
copied it but I think about if I'm

1610
01:05:45,240 --> 01:05:46,770
trying to do a scan on a column and

1611
01:05:46,770 --> 01:05:48,540
apply a predicate into the vectra's

1612
01:05:48,540 --> 01:05:50,160
instruction it will all be contiguous

1613
01:05:50,160 --> 01:05:58,950
for me yes yes so he said there's a

1614
01:05:58,950 --> 01:06:00,329
scatter and gather CP instruction for

1615
01:06:00,329 --> 01:06:04,799
this I don't know one of them is

1616
01:06:04,799 --> 01:06:07,339
implement where I forget which one I

1617
01:06:07,339 --> 01:06:12,930
think it's the gather a might like as

1618
01:06:12,930 --> 01:06:15,660
I'd like to 2019 2020 maybe it is there

1619
01:06:15,660 --> 01:06:17,369
was a period where like gather had to be

1620
01:06:17,369 --> 01:06:18,089
implemented through multiple

1621
01:06:18,089 --> 01:06:19,880
instructions and it did that for you

1622
01:06:19,880 --> 01:06:22,710
yeah we'll cover all that as well yes

1623
01:06:22,710 --> 01:06:30,030
yes yes so I don't know what share

1624
01:06:30,030 --> 01:06:32,730
screens so a shared scan is if I have

1625
01:06:32,730 --> 01:06:34,559
two queries show up at the exact same

1626
01:06:34,559 --> 01:06:36,540
time and the boat accessing the same

1627
01:06:36,540 --> 01:06:39,720
table it's it's a bit more tricky and in

1628
01:06:39,720 --> 01:06:40,740
memory system this is why they want to

1629
01:06:40,740 --> 01:06:42,510
cover it like for disk based system but

1630
01:06:42,510 --> 01:06:43,950
most expensive thing is going fetching

1631
01:06:43,950 --> 01:06:46,290
the page so even if you and I are

1632
01:06:46,290 --> 01:06:47,280
running the same time but we have

1633
01:06:47,280 --> 01:06:49,079
completely different predicates if we

1634
01:06:49,079 --> 01:06:51,270
can share that disk i/o to go fetch that

1635
01:06:51,270 --> 01:06:53,520
in and then we take a copy of the table

1636
01:06:53,520 --> 01:06:55,770
or the block and do our predicates

1637
01:06:55,770 --> 01:06:58,859
separately then that's a huge win for an

1638
01:06:58,859 --> 01:07:01,260
in-memory system

1639
01:07:01,260 --> 01:07:04,050
oh I'll still get some benefit of maybe

1640
01:07:04,050 --> 01:07:06,090
having locality bring it into my CPU

1641
01:07:06,090 --> 01:07:08,430
cache like that'll still matter

1642
01:07:08,430 --> 01:07:12,450
put the overhead of coordinating the the

1643
01:07:12,450 --> 01:07:13,859
different predicate at the same time I

1644
01:07:13,859 --> 01:07:17,190
think is tricky like sometimes you can

1645
01:07:17,190 --> 01:07:19,020
do things like I think the Hana guys do

1646
01:07:19,020 --> 01:07:21,720
something look like this like if my my

1647
01:07:21,720 --> 01:07:25,950
query is like we're we're a equals

1648
01:07:25,950 --> 01:07:27,869
equals one and your queries where a

1649
01:07:27,869 --> 01:07:29,820
equals two they would then convert that

1650
01:07:29,820 --> 01:07:33,420
to where a in 1 comma 2 and apply that

1651
01:07:33,420 --> 01:07:36,300
predicate at once and then the output of

1652
01:07:36,300 --> 01:07:38,010
that we both get and then we have both

1653
01:07:38,010 --> 01:07:39,210
have to then apply our additional

1654
01:07:39,210 --> 01:07:41,400
predicate to get what we really want so

1655
01:07:41,400 --> 01:07:44,760
there's tricks like that you can do I I

1656
01:07:44,760 --> 01:07:46,050
don't think it's that common for enemy

1657
01:07:46,050 --> 01:07:49,470
systems because it really requires that

1658
01:07:49,470 --> 01:07:50,820
was like pinpoint precision of a query

1659
01:07:50,820 --> 01:07:52,260
showing up at exact same time that can

1660
01:07:52,260 --> 01:07:54,530
do this

1661
01:07:58,010 --> 01:08:27,540
right yeah yeah so his question is so in

1662
01:08:27,540 --> 01:08:30,180
a disk based system for scan sharing or

1663
01:08:30,180 --> 01:08:34,020
one technique to do is if if I need to

1664
01:08:34,020 --> 01:08:35,939
do scan on the table but I have a limit

1665
01:08:35,939 --> 01:08:38,100
clause so I only need you know maybe 10

1666
01:08:38,100 --> 01:08:40,410
to poles rather than just opening up a

1667
01:08:40,410 --> 01:08:43,080
cursor or scanning the the table

1668
01:08:43,080 --> 01:08:45,450
can I go peek in my buffer pool figure

1669
01:08:45,450 --> 01:08:47,100
out what I already have from that table

1670
01:08:47,100 --> 01:08:50,189
and if I have enough tuples then I just

1671
01:08:50,189 --> 01:08:52,259
process based on that I don't think

1672
01:08:52,259 --> 01:08:54,029
anybody actually does that I think

1673
01:08:54,029 --> 01:08:56,069
everyone always just does the scan

1674
01:08:56,069 --> 01:08:58,080
because if it's already in your buffer

1675
01:08:58,080 --> 01:08:59,759
pool when you go do the lookup on that

1676
01:08:59,759 --> 01:09:02,250
page you'll get a hit in the buffer pool

1677
01:09:02,250 --> 01:09:04,890
I don't think anybody actually does what

1678
01:09:04,890 --> 01:09:07,109
you're proposing we should do that for

1679
01:09:07,109 --> 01:09:10,160
me for a project an intro class um the

1680
01:09:10,160 --> 01:09:12,600
the other more common thing that would

1681
01:09:12,600 --> 01:09:14,430
be the covering queries for for indexes

1682
01:09:14,430 --> 01:09:15,180
where if

1683
01:09:15,180 --> 01:09:16,710
I have all the answers I need to process

1684
01:09:16,710 --> 01:09:21,300
the query in the index itself it's a

1685
01:09:21,300 --> 01:09:22,770
minimally scan sharing that's it's just

1686
01:09:22,770 --> 01:09:24,180
avoiding having to go look at look at

1687
01:09:24,180 --> 01:09:26,700
look at the actual scan on the table so

1688
01:09:26,700 --> 01:09:28,109
that that's probably more common I don't

1689
01:09:28,109 --> 01:09:29,430
know of anybody peeking into the buff

1690
01:09:29,430 --> 01:09:30,899
ball see what's around there and see if

1691
01:09:30,899 --> 01:09:34,319
there's enough for me right because you

1692
01:09:34,319 --> 01:09:35,490
would need to maintain some heuristics

1693
01:09:35,490 --> 01:09:37,380
to say like I think what you're doing

1694
01:09:37,380 --> 01:09:39,029
right it's it's a needle in the haystack

1695
01:09:39,029 --> 01:09:42,029
search so my query shows up it's on

1696
01:09:42,029 --> 01:09:43,830
table a but most of the queries are on

1697
01:09:43,830 --> 01:09:45,660
table be so but every single time I go

1698
01:09:45,660 --> 01:09:47,520
look at my brother pool and I'm all you

1699
01:09:47,520 --> 01:09:49,529
see pages from B then I'm just wasting

1700
01:09:49,529 --> 01:09:50,040
time

1701
01:09:50,040 --> 01:09:52,319
mm I'll just go scan a so you'd have the

1702
01:09:52,319 --> 01:09:54,120
maintain some heuristics and maintain

1703
01:09:54,120 --> 01:09:57,420
some kind of quick lookup to say like oh

1704
01:09:57,420 --> 01:09:59,760
if you're looking for table a like go

1705
01:09:59,760 --> 01:10:01,200
you know find some stuff you know you'll

1706
01:10:01,200 --> 01:10:02,070
find some stuff in the buffer pool

1707
01:10:02,070 --> 01:10:03,210
there's probability

1708
01:10:03,210 --> 01:10:07,970
I don't think anybody does that would

1709
01:10:08,900 --> 01:10:10,740
and I don't know if there'll be enough

1710
01:10:10,740 --> 01:10:12,750
queries acted to want to do that right

1711
01:10:12,750 --> 01:10:15,900
for ol OTP go get Andy's record I'm

1712
01:10:15,900 --> 01:10:17,310
gonna follow the index it's gonna take

1713
01:10:17,310 --> 01:10:19,080
me to the page I want to get and I fetch

1714
01:10:19,080 --> 01:10:22,440
that in for OLAP it's usually like scan

1715
01:10:22,440 --> 01:10:26,040
the entire table things with limit calls

1716
01:10:26,040 --> 01:10:27,810
would be like think of like loading a

1717
01:10:27,810 --> 01:10:30,180
web page look like of like like hacker

1718
01:10:30,180 --> 01:10:32,010
news you see the top ten posts wherever

1719
01:10:32,010 --> 01:10:33,410
they're for the most recent ten toes

1720
01:10:33,410 --> 01:10:35,250
there's a limit call as to make that

1721
01:10:35,250 --> 01:10:38,010
work right but it's sorting based on the

1722
01:10:38,010 --> 01:10:41,100
on the on the timestamp and those tuples

1723
01:10:41,100 --> 01:10:43,610
may or may not be all in the same page

1724
01:10:43,610 --> 01:10:47,400
so I I don't think it's actually I don't

1725
01:10:47,400 --> 01:10:48,510
know if queries actually make that

1726
01:10:48,510 --> 01:10:49,770
affected worthwhile and the other head

1727
01:10:49,770 --> 01:10:51,840
are doing it it would be too high yeah

1728
01:10:51,840 --> 01:10:52,910
there's a good thought experiment though

1729
01:10:52,910 --> 01:10:56,730
anything else any other random queries

1730
01:10:56,730 --> 01:11:00,450
about databases cool awesome guys all

1731
01:11:00,450 --> 01:11:04,760
right my new class will be will be a

1732
01:11:04,760 --> 01:11:07,860
compilation and then get started on the

1733
01:11:07,860 --> 01:11:10,230
on the ii project as soon as possible

1734
01:11:10,230 --> 01:11:13,290
and then we will we will announce what

1735
01:11:13,290 --> 01:11:14,670
what machine you should test your

1736
01:11:14,670 --> 01:11:16,080
concurrency stuff on okay

1737
01:11:16,080 --> 01:11:18,000
so his question early to be in the class

1738
01:11:18,000 --> 01:11:20,910
was the first check point we will not

1739
01:11:20,910 --> 01:11:23,400
check for concurrency we're only check

1740
01:11:23,400 --> 01:11:26,250
for correctness so if you wanted to you

1741
01:11:26,250 --> 01:11:27,810
could put a giant latch on the top of

1742
01:11:27,810 --> 01:11:28,309
the thing

1743
01:11:28,309 --> 01:11:31,979
and this proved that you can do inserts

1744
01:11:31,979 --> 01:11:34,489
and lookups correct correctly okay

1745
01:11:34,489 --> 01:11:36,780
because in grey scoop only gives us a

1746
01:11:36,780 --> 01:11:37,979
single thread so we really can't Hammer

1747
01:11:37,979 --> 01:11:45,349
it too much all right what is this

1748
01:11:45,510 --> 01:12:12,699
[Music]

